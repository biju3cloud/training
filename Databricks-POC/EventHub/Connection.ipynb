{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a144154a-fda8-4e63-9fa0-6685299efd6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eec1e73-2098-4e8a-9fe2-43112f36c0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f29cd28-6aa8-4562-b3c5-ae917c279f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Azure Event Hub - Python SDK (Fixed Version)\n",
    "# MAGIC \n",
    "# MAGIC **‚úÖ Works on shared clusters - No library installation needed**\n",
    "# MAGIC \n",
    "# MAGIC This version:\n",
    "# MAGIC - Handles empty Event Hubs properly\n",
    "# MAGIC - Sends test messages first\n",
    "# MAGIC - Better error handling\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Install Python SDK\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "%pip install azure-eventhub pandas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event Hub Configuration\n",
    "eh_namespace = \"evhns-gdctraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-gdctraining-001\"\n",
    "\n",
    "# PASTE YOUR KEYS HERE\n",
    "# Get these from Azure Portal ‚Üí Event Hub ‚Üí Shared access policies\n",
    "\n",
    "# READ ACCESS KEY (from evh-gdctraining-001-read secret in Key Vault)\n",
    "# Or from evhaccesspolicylisten in Event Hub\n",
    "READ_ACCESS_KEY = \"YB11jV8CfL4cFRgr2LSAMudsBdtKeMDUX+AEhLceoGI=\"\n",
    "\n",
    "# WRITE ACCESS KEY (from evh-gdctraining-001-write secret in Key Vault)\n",
    "# Or from evhaccesspolicysend in Event Hub\n",
    "WRITE_ACCESS_KEY = \"uilv4rbO5nBOr8JfHiFqpRXVmXluwx47c+AEhBoNmvQ=\"\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Build Connection Strings\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Connection String for READING\n",
    "read_connection_string = f\"Endpoint=sb://{eh_namespace}/;SharedAccessKeyName=evhaccesspolicylisten;SharedAccessKey={READ_ACCESS_KEY};EntityPath={eh_name}\"\n",
    "\n",
    "# Connection String for WRITING\n",
    "write_connection_string = f\"Endpoint=sb://{eh_namespace}/;SharedAccessKeyName=evhaccesspolicysend;SharedAccessKey={WRITE_ACCESS_KEY};EntityPath={eh_name}\"\n",
    "\n",
    "print(\"‚úì Connection strings built\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: FIRST - Send Test Messages to Event Hub\n",
    "# MAGIC \n",
    "# MAGIC Let's send some test data first so we have something to read\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Create test messages\n",
    "test_messages = [\n",
    "    {\"id\": \"test_001\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"temperature\", \"value\": 22.5, \"unit\": \"celsius\"},\n",
    "    {\"id\": \"test_002\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"humidity\", \"value\": 65.3, \"unit\": \"percent\"},\n",
    "    {\"id\": \"test_003\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"pressure\", \"value\": 1013.2, \"unit\": \"hPa\"},\n",
    "    {\"id\": \"test_004\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"temperature\", \"value\": 23.1, \"unit\": \"celsius\"},\n",
    "    {\"id\": \"test_005\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"humidity\", \"value\": 67.8, \"unit\": \"percent\"}\n",
    "]\n",
    "\n",
    "print(\"Sending test messages to Event Hub...\")\n",
    "print(f\"Messages to send: {len(test_messages)}\")\n",
    "\n",
    "try:\n",
    "    # Create producer\n",
    "    producer = EventHubProducerClient.from_connection_string(\n",
    "        conn_str=write_connection_string\n",
    "    )\n",
    "    \n",
    "    # Create and send batch\n",
    "    event_batch = producer.create_batch()\n",
    "    \n",
    "    for msg in test_messages:\n",
    "        event_batch.add(EventData(json.dumps(msg)))\n",
    "        print(f\"  Added: {msg['id']} - {msg['sensor']}: {msg['value']}\")\n",
    "    \n",
    "    # Send the batch\n",
    "    producer.send_batch(event_batch)\n",
    "    producer.close()\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully sent {len(test_messages)} messages!\")\n",
    "    print(\"‚è≥ Wait 5 seconds for messages to be available...\")\n",
    "    \n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error sending messages: {str(e)}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - WRITE_ACCESS_KEY is correct\")\n",
    "    print(\"  - You have 'Azure Event Hubs Data Sender' permission\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: Read from Event Hub (Fixed Version)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "\n",
    "# Storage for collected events\n",
    "collected_events = []\n",
    "\n",
    "def on_event(partition_context, event):\n",
    "    \"\"\"Process each event - with proper None checking\"\"\"\n",
    "    try:\n",
    "        # Check if event is None\n",
    "        if event is None:\n",
    "            print(\"Received None event (skipping)\")\n",
    "            return\n",
    "        \n",
    "        # Try to get event body\n",
    "        try:\n",
    "            body = event.body_as_str()\n",
    "        except Exception:\n",
    "            body = str(event.body) if hasattr(event, 'body') else \"No body\"\n",
    "        \n",
    "        # Parse event data\n",
    "        event_data = {\n",
    "            'body': body,\n",
    "            'enqueued_time': event.enqueued_time if hasattr(event, 'enqueued_time') else None,\n",
    "            'offset': event.offset if hasattr(event, 'offset') else None,\n",
    "            'sequence_number': event.sequence_number if hasattr(event, 'sequence_number') else None,\n",
    "            'partition_key': event.partition_key if hasattr(event, 'partition_key') else None\n",
    "        }\n",
    "        \n",
    "        collected_events.append(event_data)\n",
    "        \n",
    "        # Print progress\n",
    "        if len(collected_events) <= 10 or len(collected_events) % 10 == 0:\n",
    "            print(f\"‚úì Collected {len(collected_events)} events\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event: {str(e)}\")\n",
    "\n",
    "# Create consumer\n",
    "print(\"Connecting to Event Hub...\")\n",
    "print(\"Reading events (will read for 20 seconds)...\\n\")\n",
    "\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=read_connection_string,\n",
    "    consumer_group=\"$Default\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with client:\n",
    "        client.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",  # Start from beginning\n",
    "            max_wait_time=20  # Read for 20 seconds\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Finished reading from Event Hub\")\n",
    "    print(f\"‚úì Total events collected: {len(collected_events)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Error reading from Event Hub: {str(e)}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - READ_ACCESS_KEY is correct\")\n",
    "    print(\"  - You have 'Azure Event Hubs Data Receiver' permission\")\n",
    "    print(\"  - Event Hub namespace and name are correct\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 6: Display Collected Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    print(f\"Found {len(collected_events)} events!\\n\")\n",
    "    \n",
    "    # Show first few events\n",
    "    print(\"First 5 events:\")\n",
    "    print(\"=\"*60)\n",
    "    for i, event in enumerate(collected_events[:5], 1):\n",
    "        print(f\"\\nEvent {i}:\")\n",
    "        print(f\"  Body: {event['body']}\")\n",
    "        print(f\"  Enqueued Time: {event['enqueued_time']}\")\n",
    "        print(f\"  Offset: {event['offset']}\")\n",
    "        print(f\"  Sequence Number: {event['sequence_number']}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No events collected from Event Hub\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Event Hub is empty - Try running Step 4 to send test messages\")\n",
    "    print(\"  2. Connection issue - Check your access keys\")\n",
    "    print(\"  3. Permissions - Verify you have 'Data Receiver' role\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 7: Convert to Pandas DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if collected_events:\n",
    "    # Convert to Pandas\n",
    "    df_pandas = pd.DataFrame(collected_events)\n",
    "    \n",
    "    print(f\"Created Pandas DataFrame with {len(df_pandas)} rows\")\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    print(df_pandas.info())\n",
    "    \n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    display(df_pandas.head(10))\n",
    "else:\n",
    "    print(\"No events to convert\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 8: Convert to Spark DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    # Convert to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(collected_events)\n",
    "    \n",
    "    print(\"‚úì Created Spark DataFrame\")\n",
    "    print(\"\\nSchema:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "    print(f\"\\nTotal records: {df_spark.count()}\")\n",
    "    \n",
    "    # Display\n",
    "    display(df_spark)\n",
    "else:\n",
    "    print(\"No events to convert to Spark DataFrame\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 9: Parse JSON Message Bodies\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Sample message to understand structure\n",
    "    print(\"Sample message body:\")\n",
    "    print(collected_events[0]['body'])\n",
    "    print()\n",
    "    \n",
    "    # Define schema for your messages\n",
    "    message_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"sensor\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True),\n",
    "        StructField(\"unit\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Parse JSON\n",
    "    df_parsed = df_spark.select(\n",
    "        from_json(col(\"body\"), message_schema).alias(\"data\"),\n",
    "        col(\"enqueued_time\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"sequence_number\")\n",
    "    ).select(\"data.*\", \"enqueued_time\", \"offset\", \"sequence_number\")\n",
    "    \n",
    "    print(\"‚úì Parsed JSON messages\")\n",
    "    print(f\"Total parsed records: {df_parsed.count()}\\n\")\n",
    "    \n",
    "    display(df_parsed)\n",
    "else:\n",
    "    print(\"No events to parse\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 10: Save to Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Output path\n",
    "    output_path = \"/tmp/eventhub/data\"\n",
    "    \n",
    "    # Save to Delta\n",
    "    df_spark.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(output_path)\n",
    "    \n",
    "    print(f\"‚úì Saved {len(collected_events)} events to Delta Lake\")\n",
    "    print(f\"  Location: {output_path}\")\n",
    "else:\n",
    "    print(\"No events to save\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 11: Read from Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "output_path = \"/tmp/eventhub/data\"\n",
    "\n",
    "try:\n",
    "    # Read Delta table\n",
    "    df_delta = spark.read.format(\"delta\").load(output_path)\n",
    "    \n",
    "    print(f\"‚úì Delta table loaded\")\n",
    "    print(f\"Total records: {df_delta.count()}\\n\")\n",
    "    \n",
    "    # Display latest records\n",
    "    display(df_delta.orderBy(col(\"enqueued_time\").desc()).limit(100))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Delta table not found: {str(e)}\")\n",
    "    print(\"Run Step 10 first to save data to Delta Lake\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 12: Send More Test Messages (Anytime)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def send_test_messages(count=5, sensor_type=\"temperature\"):\n",
    "    \"\"\"Send test messages to Event Hub\"\"\"\n",
    "    from datetime import datetime, timezone\n",
    "    import json\n",
    "    \n",
    "    messages = []\n",
    "    for i in range(count):\n",
    "        msg = {\n",
    "            \"id\": f\"msg_{datetime.now().strftime('%Y%m%d%H%M%S')}_{i}\",\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"sensor\": sensor_type,\n",
    "            \"value\": 20.0 + i * 0.5,\n",
    "            \"unit\": \"celsius\" if sensor_type == \"temperature\" else \"percent\"\n",
    "        }\n",
    "        messages.append(msg)\n",
    "    \n",
    "    try:\n",
    "        producer = EventHubProducerClient.from_connection_string(\n",
    "            conn_str=write_connection_string\n",
    "        )\n",
    "        \n",
    "        event_batch = producer.create_batch()\n",
    "        for msg in messages:\n",
    "            event_batch.add(EventData(json.dumps(msg)))\n",
    "        \n",
    "        producer.send_batch(event_batch)\n",
    "        producer.close()\n",
    "        \n",
    "        print(f\"‚úì Sent {count} test messages\")\n",
    "        for msg in messages:\n",
    "            print(f\"  - {msg['id']}: {msg['sensor']} = {msg['value']}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Send 5 temperature readings\n",
    "send_test_messages(count=5, sensor_type=\"temperature\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 13: Read Latest Messages\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Clear previous events\n",
    "collected_events = []\n",
    "\n",
    "print(\"Reading latest messages from Event Hub...\")\n",
    "print(\"(Reading for 15 seconds)\\n\")\n",
    "\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=read_connection_string,\n",
    "    consumer_group=\"$Default\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with client:\n",
    "        client.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",\n",
    "            max_wait_time=15\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úì Collected {len(collected_events)} events\")\n",
    "    \n",
    "    if collected_events:\n",
    "        # Show latest events\n",
    "        print(\"\\nLatest events:\")\n",
    "        for i, event in enumerate(collected_events[-5:], 1):\n",
    "            print(f\"{i}. {event['body']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {str(e)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Troubleshooting Guide\n",
    "# MAGIC \n",
    "# MAGIC ### Error: \"NoneType object has no attribute\"\n",
    "# MAGIC **Fixed!** The code now handles None events properly.\n",
    "# MAGIC \n",
    "# MAGIC ### No events collected:\n",
    "# MAGIC 1. **First, send test messages:** Run Step 4\n",
    "# MAGIC 2. **Wait a few seconds:** Give Event Hub time to process\n",
    "# MAGIC 3. **Then read:** Run Step 5\n",
    "# MAGIC \n",
    "# MAGIC ### \"Unauthorized\" error:\n",
    "# MAGIC - Check access keys are correct (no extra spaces)\n",
    "# MAGIC - Verify you have proper permissions:\n",
    "# MAGIC   - \"Azure Event Hubs Data Sender\" for writing\n",
    "# MAGIC   - \"Azure Event Hubs Data Receiver\" for reading\n",
    "# MAGIC \n",
    "# MAGIC ### Connection timeout:\n",
    "# MAGIC - Event Hub might be empty - send test messages first\n",
    "# MAGIC - Check namespace and Event Hub name are correct\n",
    "# MAGIC - Verify network connectivity from Databricks to Azure\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Quick Reference\n",
    "# MAGIC \n",
    "# MAGIC ### Typical Workflow:\n",
    "# MAGIC \n",
    "# MAGIC 1. **Send test messages:** Run Step 4\n",
    "# MAGIC 2. **Wait 5 seconds**\n",
    "# MAGIC 3. **Read messages:** Run Step 5\n",
    "# MAGIC 4. **View data:** Run Steps 6-8\n",
    "# MAGIC 5. **Parse JSON:** Run Step 9\n",
    "# MAGIC 6. **Save to Delta:** Run Step 10\n",
    "# MAGIC \n",
    "# MAGIC ### Key Functions:\n",
    "# MAGIC \n",
    "# MAGIC ```python\n",
    "# MAGIC # Send messages\n",
    "# MAGIC send_test_messages(count=10, sensor_type=\"humidity\")\n",
    "# MAGIC \n",
    "# MAGIC # Read and convert to Spark\n",
    "# MAGIC # (Run Step 5, then Step 8)\n",
    "# MAGIC \n",
    "# MAGIC # Save to Delta\n",
    "# MAGIC df_spark.write.format(\"delta\").mode(\"append\").save(\"/path\")\n",
    "# MAGIC ```\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Next Steps\n",
    "# MAGIC \n",
    "# MAGIC ### ‚úÖ What's Working:\n",
    "# MAGIC - Sending messages to Event Hub\n",
    "# MAGIC - Reading messages from Event Hub\n",
    "# MAGIC - Converting to Spark DataFrames\n",
    "# MAGIC - Saving to Delta Lake\n",
    "# MAGIC - Parsing JSON messages\n",
    "# MAGIC \n",
    "# MAGIC ### üöÄ For Production:\n",
    "# MAGIC 1. **Schedule this notebook as a job** (Workflows ‚Üí Create Job)\n",
    "# MAGIC 2. **Set up Delta tables** with proper schemas\n",
    "# MAGIC 3. **Add data validation** and error handling\n",
    "# MAGIC 4. **Monitor with Databricks monitoring tools**\n",
    "# MAGIC \n",
    "# MAGIC ### üí° Performance Tips:\n",
    "# MAGIC - For high-volume data: Ask admin to add Spark connector to allowlist\n",
    "# MAGIC - Use Delta Lake for efficient storage and queries\n",
    "# MAGIC - Schedule jobs during off-peak hours\n",
    "# MAGIC - Monitor Event Hub throughput units"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Connection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
