{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e6f45a-1cf5-4eb8-bc03-ed99fe8d9637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c815be14-4eb8-46e9-ab74-ca49c38b4c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7071259f-2ccc-4daa-97f6-b04cb14b6f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Option A\n",
    "secret_value = dbutils.secrets.get(\n",
    "    scope=\"dbx-ss-kv-natraining-2\", key=\"evh-natraining-read-write\"\n",
    ")\n",
    "send_conn_str = (\n",
    "    \"Endpoint=sb://evhns-natraining.servicebus.windows.net/;\"\n",
    "    \"SharedAccessKeyName=SharedAccessKeyToSendAndListen;\"\n",
    "    f\"SharedAccessKey={secret_value};\"\n",
    "    \"EntityPath=evh-natraining-001\"\n",
    ")\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-001\"\n",
    "dbutils.widgets.text(\"eventhub_name\", \"evh-natraining-001\")\n",
    "dbutils.widgets.text(\n",
    "    \"test_message\", \"Test Biju on Databricks!\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dae6df6-46bd-4fac-975d-2a8eac26e8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "\n",
    "\n",
    "# Event Hub Configuration\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "\n",
    "# Key Vault Secret Scope\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "\n",
    "# Secret names in Key Vault\n",
    "read_secret_name = \"evh-natraining-read-write\"\n",
    "write_secret_name = \"evh-natraining-read-write\"\n",
    "\n",
    "print(\"‚úì Configuration loaded successfully\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Retrieve Access Keys from Key Vault\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Retrieve the access keys from Key Vault via Databricks Secrets\n",
    "try:\n",
    "    read_access_key = dbutils.secrets.get(scope=keyvault_scope, key=read_secret_name)\n",
    "    write_access_key = dbutils.secrets.get(scope=keyvault_scope, key=write_secret_name)\n",
    "    print(\"‚úì Successfully retrieved secrets from Key Vault\")\n",
    "    print(f\"  - Read secret: {read_secret_name}\")\n",
    "    print(f\"  - Write secret: {write_secret_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error retrieving secrets: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify secret scope exists:\")\n",
    "    print(\"   Available scopes:\", [s.name for s in dbutils.secrets.listScopes()])\n",
    "    print(\"\\n2. Verify secrets exist in scope:\")\n",
    "    try:\n",
    "        secrets = dbutils.secrets.list(keyvault_scope)\n",
    "        print(f\"   Secrets in '{keyvault_scope}':\", [s.key for s in secrets])\n",
    "    except:\n",
    "        print(f\"   Cannot list secrets in scope '{keyvault_scope}'\")\n",
    "    print(\"\\n3. Check Key Vault access permissions\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Build Connection Strings\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Connection String for READING\n",
    "read_connection_string = f\"Endpoint=sb://{eh_namespace}/;SharedAccessKeyName=evhaccesspolicylisten;SharedAccessKey={read_access_key};EntityPath={eh_name}\"\n",
    "\n",
    "# Connection String for WRITING\n",
    "write_connection_string = f\"Endpoint=sb://{eh_namespace}/;SharedAccessKeyName=evhaccesspolicysend;SharedAccessKey={write_access_key};EntityPath={eh_name}\"\n",
    "\n",
    "print(\"‚úì Connection strings built successfully\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31589740-2ad9-403b-b253-8f3e74e99b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: FIRST - Send Test Messages to Event Hub\n",
    "# MAGIC \n",
    "# MAGIC Let's send some test data first so we have something to read\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Create test messages\n",
    "test_messages = [\n",
    "    {\"id\": \"test_00661\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"temperature\", \"value\": 22.5, \"unit\": \"celsius\"},\n",
    "    {\"id\": \"test_06602\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"humidity\", \"value\": 65.3, \"unit\": \"percent\"},\n",
    "    {\"id\": \"test_06603\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"pressure\", \"value\": 1013.2, \"unit\": \"hPa\"},\n",
    "    {\"id\": \"test_00664\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"temperature\", \"value\": 23.1, \"unit\": \"celsius\"},\n",
    "    {\"id\": \"test_00665\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"humidity\", \"value\": 67.8, \"unit\": \"percent\"}\n",
    "]\n",
    "\n",
    "print(\"Sending test messages to Event Hub...\")\n",
    "print(f\"Messages to send: {len(test_messages)}\")\n",
    "\n",
    "try:\n",
    "    # Create producer\n",
    "    producer = EventHubProducerClient.from_connection_string(\n",
    "        conn_str=write_connection_string\n",
    "    )\n",
    "    \n",
    "    # Create and send batch\n",
    "    event_batch = producer.create_batch()\n",
    "    \n",
    "    for msg in test_messages:\n",
    "        event_batch.add(EventData(json.dumps(msg)))\n",
    "        print(f\"  Added: {msg['id']} - {msg['sensor']}: {msg['value']}\")\n",
    "    \n",
    "    # Send the batch\n",
    "    producer.send_batch(event_batch)\n",
    "    producer.close()\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully sent {len(test_messages)} messages!\")\n",
    "    print(\"‚è≥ Wait 5 seconds for messages to be available...\")\n",
    "    \n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error sending messages: {str(e)}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - Write access key from Key Vault is correct\")\n",
    "    print(\"  - You have 'Azure Event Hubs Data Sender' permission\")\n",
    "    print(\"  - Event Hub namespace and name are correct\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 6: Read from Event Hub\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "\n",
    "# Storage for collected events\n",
    "collected_events = []\n",
    "\n",
    "def on_event(partition_context, event):\n",
    "    \"\"\"Process each event - with proper None checking\"\"\"\n",
    "    try:\n",
    "        # Check if event is None\n",
    "        if event is None:\n",
    "            print(\"Received None event (skipping)\")\n",
    "            return\n",
    "        \n",
    "        # Try to get event body\n",
    "        try:\n",
    "            body = event.body_as_str()\n",
    "        except Exception:\n",
    "            body = str(event.body) if hasattr(event, 'body') else \"No body\"\n",
    "        \n",
    "        # Parse event data\n",
    "        event_data = {\n",
    "            'body': body,\n",
    "            'enqueued_time': event.enqueued_time if hasattr(event, 'enqueued_time') else None,\n",
    "            'offset': event.offset if hasattr(event, 'offset') else None,\n",
    "            'sequence_number': event.sequence_number if hasattr(event, 'sequence_number') else None,\n",
    "            'partition_key': event.partition_key if hasattr(event, 'partition_key') else None\n",
    "        }\n",
    "        \n",
    "        collected_events.append(event_data)\n",
    "        \n",
    "        # Print progress\n",
    "        if len(collected_events) <= 10 or len(collected_events) % 10 == 0:\n",
    "            print(f\"‚úì Collected {len(collected_events)} events\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event: {str(e)}\")\n",
    "\n",
    "# Create consumer\n",
    "print(\"Connecting to Event Hub...\")\n",
    "print(\"Reading events (will read for 20 seconds)...\\n\")\n",
    "\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=read_connection_string,\n",
    "    consumer_group=\"$Default\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with client:\n",
    "        client.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",  # Start from beginning\n",
    "            max_wait_time=20  # Read for 20 seconds\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Finished reading from Event Hub\")\n",
    "    print(f\"‚úì Total events collected: {len(collected_events)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Error reading from Event Hub: {str(e)}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - Read access key from Key Vault is correct\")\n",
    "    print(\"  - You have 'Azure Event Hubs Data Receiver' permission\")\n",
    "    print(\"  - Event Hub namespace and name are correct\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 7: Display Collected Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    print(f\"Found {len(collected_events)} events!\\n\")\n",
    "    \n",
    "    # Show first few events\n",
    "    print(\"First 5 events:\")\n",
    "    print(\"=\"*60)\n",
    "    for i, event in enumerate(collected_events[:5], 1):\n",
    "        print(f\"\\nEvent {i}:\")\n",
    "        print(f\"  Body: {event['body']}\")\n",
    "        print(f\"  Enqueued Time: {event['enqueued_time']}\")\n",
    "        print(f\"  Offset: {event['offset']}\")\n",
    "        print(f\"  Sequence Number: {event['sequence_number']}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No events collected from Event Hub\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Event Hub is empty - Try running Step 5 to send test messages\")\n",
    "    print(\"  2. Connection issue - Check your Key Vault secrets\")\n",
    "    print(\"  3. Permissions - Verify you have 'Data Receiver' role\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 8: Convert to Pandas DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if collected_events:\n",
    "    # Convert to Pandas\n",
    "    df_pandas = pd.DataFrame(collected_events)\n",
    "    \n",
    "    print(f\"Created Pandas DataFrame with {len(df_pandas)} rows\")\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    print(df_pandas.info())\n",
    "    \n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    display(df_pandas.head(10))\n",
    "else:\n",
    "    print(\"No events to convert\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 9: Convert to Spark DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    # Convert to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(collected_events)\n",
    "    \n",
    "    print(\"‚úì Created Spark DataFrame\")\n",
    "    print(\"\\nSchema:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "    print(f\"\\nTotal records: {df_spark.count()}\")\n",
    "    \n",
    "    # Display\n",
    "    display(df_spark)\n",
    "else:\n",
    "    print(\"No events to convert to Spark DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0247b02a-e251-4cce-950b-337fcf6fee1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: FIRST - Send Test Messages to Event Hub\n",
    "# MAGIC \n",
    "# MAGIC Let's send some test data first so we have something to read\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Create test messages\n",
    "test_messages = [\n",
    "    {\"id\": \"bj_001\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"temperature\", \"value\": 212.5, \"unit\": \"celsius\"},\n",
    "    {\"id\": \"bj_002\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"humidity\", \"value\": 65.3, \"unit\": \"percent\"},\n",
    "    {\"id\": \"bj_003\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"pressure\", \"value\": 1013.2, \"unit\": \"hPa\"},\n",
    "    {\"id\": \"bj_004\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"temperature\", \"value\": 23.1, \"unit\": \"celsius\"},\n",
    "    {\"id\": \"bj_005\", \"timestamp\": datetime.now(timezone.utc).isoformat(), \"sensor\": \"humidity\", \"value\": 67.8, \"unit\": \"percent\"}\n",
    "]\n",
    "\n",
    "print(\"Sending test messages to Event Hub...\")\n",
    "print(f\"Messages to send: {len(test_messages)}\")\n",
    "\n",
    "try:\n",
    "    # Create producer\n",
    "    producer = EventHubProducerClient.from_connection_string(\n",
    "        conn_str=write_connection_string\n",
    "    )\n",
    "    \n",
    "    # Create and send batch\n",
    "    event_batch = producer.create_batch()\n",
    "    \n",
    "    for msg in test_messages:\n",
    "        event_batch.add(EventData(json.dumps(msg)))\n",
    "        print(f\"  Added: {msg['id']} - {msg['sensor']}: {msg['value']}\")\n",
    "    \n",
    "    # Send the batch\n",
    "    producer.send_batch(event_batch)\n",
    "    producer.close()\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully sent {len(test_messages)} messages!\")\n",
    "    print(\"‚è≥ Wait 5 seconds for messages to be available...\")\n",
    "    \n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error sending messages: {str(e)}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - Write access key from Key Vault is correct\")\n",
    "    print(\"  - You have 'Azure Event Hubs Data Sender' permission\")\n",
    "    print(\"  - Event Hub namespace and name are correct\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 6: Read from Event Hub\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "\n",
    "# Storage for collected events\n",
    "collected_events = []\n",
    "\n",
    "def on_event(partition_context, event):\n",
    "    \"\"\"Process each event - with proper None checking\"\"\"\n",
    "    try:\n",
    "        # Check if event is None\n",
    "        if event is None:\n",
    "            print(\"Received None event (skipping)\")\n",
    "            return\n",
    "        \n",
    "        # Try to get event body\n",
    "        try:\n",
    "            body = event.body_as_str()\n",
    "        except Exception:\n",
    "            body = str(event.body) if hasattr(event, 'body') else \"No body\"\n",
    "        \n",
    "        # Parse event data\n",
    "        event_data = {\n",
    "            'body': body,\n",
    "            'enqueued_time': event.enqueued_time if hasattr(event, 'enqueued_time') else None,\n",
    "            'offset': event.offset if hasattr(event, 'offset') else None,\n",
    "            'sequence_number': event.sequence_number if hasattr(event, 'sequence_number') else None,\n",
    "            'partition_key': event.partition_key if hasattr(event, 'partition_key') else None\n",
    "        }\n",
    "        \n",
    "        collected_events.append(event_data)\n",
    "        \n",
    "        # Print progress\n",
    "        if len(collected_events) <= 10 or len(collected_events) % 10 == 0:\n",
    "            print(f\"‚úì Collected {len(collected_events)} events\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event: {str(e)}\")\n",
    "\n",
    "# Create consumer\n",
    "print(\"Connecting to Event Hub...\")\n",
    "print(\"Reading events (will read for 20 seconds)...\\n\")\n",
    "\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=read_connection_string,\n",
    "    consumer_group=\"$Default\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with client:\n",
    "        client.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",  # Start from beginning\n",
    "            max_wait_time=20  # Read for 20 seconds\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Finished reading from Event Hub\")\n",
    "    print(f\"‚úì Total events collected: {len(collected_events)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Error reading from Event Hub: {str(e)}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - Read access key from Key Vault is correct\")\n",
    "    print(\"  - You have 'Azure Event Hubs Data Receiver' permission\")\n",
    "    print(\"  - Event Hub namespace and name are correct\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 7: Display Collected Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    print(f\"Found {len(collected_events)} events!\\n\")\n",
    "    \n",
    "    # Show first few events\n",
    "    print(\"First 5 events:\")\n",
    "    print(\"=\"*60)\n",
    "    for i, event in enumerate(collected_events[:5], 1):\n",
    "        print(f\"\\nEvent {i}:\")\n",
    "        print(f\"  Body: {event['body']}\")\n",
    "        print(f\"  Enqueued Time: {event['enqueued_time']}\")\n",
    "        print(f\"  Offset: {event['offset']}\")\n",
    "        print(f\"  Sequence Number: {event['sequence_number']}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No events collected from Event Hub\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Event Hub is empty - Try running Step 5 to send test messages\")\n",
    "    print(\"  2. Connection issue - Check your Key Vault secrets\")\n",
    "    print(\"  3. Permissions - Verify you have 'Data Receiver' role\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 8: Convert to Pandas DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if collected_events:\n",
    "    # Convert to Pandas\n",
    "    df_pandas = pd.DataFrame(collected_events)\n",
    "    \n",
    "    print(f\"Created Pandas DataFrame with {len(df_pandas)} rows\")\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    print(df_pandas.info())\n",
    "    \n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    display(df_pandas.head(10))\n",
    "else:\n",
    "    print(\"No events to convert\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 9: Convert to Spark DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    # Convert to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(collected_events)\n",
    "    \n",
    "    print(\"‚úì Created Spark DataFrame\")\n",
    "    print(\"\\nSchema:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "    print(f\"\\nTotal records: {df_spark.count()}\")\n",
    "    \n",
    "    # Display\n",
    "    display(df_spark)\n",
    "else:\n",
    "    print(\"No events to convert to Spark DataFrame\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 10: Parse JSON Message Bodies\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Sample message to understand structure\n",
    "    print(\"Sample message body:\")\n",
    "    print(collected_events[0]['body'])\n",
    "    print()\n",
    "    \n",
    "    # Define schema for your messages\n",
    "    message_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"sensor\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True),\n",
    "        StructField(\"unit\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Parse JSON\n",
    "    df_parsed = df_spark.select(\n",
    "        from_json(col(\"body\"), message_schema).alias(\"data\"),\n",
    "        col(\"enqueued_time\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"sequence_number\")\n",
    "    ).select(\"data.*\", \"enqueued_time\", \"offset\", \"sequence_number\")\n",
    "    \n",
    "    print(\"‚úì Parsed JSON messages\")\n",
    "    print(f\"Total parsed records: {df_parsed.count()}\\n\")\n",
    "    \n",
    "    display(df_parsed)\n",
    "else:\n",
    "    print(\"No events to parse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56895cfa-1616-46d4-88ca-8a7432634b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema  gdc_dbxtraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eea3d6b0-eb39-4c33-ba66-590dd0cb2693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 11: Save to Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Output path\n",
    "    output_path = \"/tmp/eventhub/data\"\n",
    "    \n",
    "    # Save to Delta\n",
    "    df_spark.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(output_path)\n",
    "    \n",
    "    print(f\"‚úì Saved {len(collected_events)} events to Delta Lake\")\n",
    "    print(f\"  Location: {output_path}\")\n",
    "else:\n",
    "    print(\"No events to save\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 12: Read from Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "output_path = \"/tmp/eventhub/data\"\n",
    "\n",
    "try:\n",
    "    # Read Delta table\n",
    "    df_delta = spark.read.format(\"delta\").load(output_path)\n",
    "    \n",
    "    print(f\"‚úì Delta table loaded\")\n",
    "    print(f\"Total records: {df_delta.count()}\\n\")\n",
    "    \n",
    "    # Display latest records\n",
    "    display(df_delta.orderBy(col(\"enqueued_time\").desc()).limit(100))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Delta table not found: {str(e)}\")\n",
    "    print(\"Run Step 11 first to save data to Delta Lake\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 13: Send More Test Messages (Anytime)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def send_test_messages(count=5, sensor_type=\"temperature\"):\n",
    "    \"\"\"Send test messages to Event Hub\"\"\"\n",
    "    from datetime import datetime, timezone\n",
    "    import json\n",
    "    \n",
    "    messages = []\n",
    "    for i in range(count):\n",
    "        msg = {\n",
    "            \"id\": f\"msg_{datetime.now().strftime('%Y%m%d%H%M%S')}_{i}\",\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"sensor\": sensor_type,\n",
    "            \"value\": 20.0 + i * 0.5,\n",
    "            \"unit\": \"celsius\" if sensor_type == \"temperature\" else \"percent\"\n",
    "        }\n",
    "        messages.append(msg)\n",
    "    \n",
    "    try:\n",
    "        producer = EventHubProducerClient.from_connection_string(\n",
    "            conn_str=write_connection_string\n",
    "        )\n",
    "        \n",
    "        event_batch = producer.create_batch()\n",
    "        for msg in messages:\n",
    "            event_batch.add(EventData(json.dumps(msg)))\n",
    "        \n",
    "        producer.send_batch(event_batch)\n",
    "        producer.close()\n",
    "        \n",
    "        print(f\"‚úì Sent {count} test messages\")\n",
    "        for msg in messages:\n",
    "            print(f\"  - {msg['id']}: {msg['sensor']} = {msg['value']}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Send 5 temperature readings\n",
    "send_test_messages(count=5, sensor_type=\"temperature\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 14: Read Latest Messages\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Clear previous events\n",
    "collected_events = []\n",
    "\n",
    "print(\"Reading latest messages from Event Hub...\")\n",
    "print(\"(Reading for 15 seconds)\\n\")\n",
    "\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=read_connection_string,\n",
    "    consumer_group=\"$Default\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with client:\n",
    "        client.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",\n",
    "            max_wait_time=15\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úì Collected {len(collected_events)} events\")\n",
    "    \n",
    "    if collected_events:\n",
    "        # Show latest events\n",
    "        print(\"\\nLatest events:\")\n",
    "        for i, event in enumerate(collected_events[-5:], 1):\n",
    "            print(f\"{i}. {event['body']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {str(e)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 15: Verify Key Vault Connection\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Verify secrets are accessible (without displaying the actual values)\n",
    "try:\n",
    "    # List available scopes\n",
    "    scopes = dbutils.secrets.listScopes()\n",
    "    print(\"Available Secret Scopes:\")\n",
    "    for scope in scopes:\n",
    "        print(f\"  - {scope.name}\")\n",
    "    \n",
    "    # List secrets in your scope (keys only, not values)\n",
    "    print(f\"\\nSecrets in '{keyvault_scope}' scope:\")\n",
    "    secrets = dbutils.secrets.list(keyvault_scope)\n",
    "    for secret in secrets:\n",
    "        print(f\"  - {secret.key}\")\n",
    "    \n",
    "    # Verify the specific secrets exist\n",
    "    secret_keys = [s.key for s in secrets]\n",
    "    \n",
    "    if read_secret_name in secret_keys:\n",
    "        print(f\"\\n‚úì Read secret '{read_secret_name}' found\")\n",
    "    else:\n",
    "        print(f\"\\n‚úó Read secret '{read_secret_name}' NOT found\")\n",
    "    \n",
    "    if write_secret_name in secret_keys:\n",
    "        print(f\"‚úì Write secret '{write_secret_name}' found\")\n",
    "    else:\n",
    "        print(f\"‚úó Write secret '{write_secret_name}' NOT found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"1. Secret scope 'dbx-ss-kv-gdctraining' is created and linked to Key Vault\")\n",
    "    print(\"2. Databricks has access to Key Vault (Managed Identity or Service Principal)\")\n",
    "    print(\"3. Secrets exist in Key Vault with the correct names\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Troubleshooting Guide\n",
    "# MAGIC \n",
    "# MAGIC ### Error: \"Secret does not exist with scope\"\n",
    "# MAGIC **Cause:** Key Vault secret scope is not set up or secrets don't exist\n",
    "# MAGIC **Solution:**\n",
    "# MAGIC 1. Verify secret scope exists: Run Step 15\n",
    "# MAGIC 2. Check scope name is correct: `dbx-ss-kv-gdctraining`\n",
    "# MAGIC 3. Verify secrets exist in Key Vault:\n",
    "# MAGIC    - `evh-gdctraining-001-read`\n",
    "# MAGIC    - `evh-gdctraining-001-write`\n",
    "# MAGIC \n",
    "# MAGIC ### Error: \"PERMISSION_DENIED\" from Key Vault\n",
    "# MAGIC **Cause:** Databricks doesn't have permission to access Key Vault\n",
    "# MAGIC **Solution:**\n",
    "# MAGIC 1. Grant Databricks Managed Identity access to Key Vault\n",
    "# MAGIC 2. Add \"Get\" and \"List\" permissions for secrets\n",
    "# MAGIC 3. Wait 5-10 minutes for permissions to propagate\n",
    "# MAGIC \n",
    "# MAGIC ### Error: \"NoneType object has no attribute\"\n",
    "# MAGIC **Fixed!** The code now handles None events properly.\n",
    "# MAGIC \n",
    "# MAGIC ### No events collected:\n",
    "# MAGIC 1. **First, send test messages:** Run Step 5\n",
    "# MAGIC 2. **Wait a few seconds:** Give Event Hub time to process\n",
    "# MAGIC 3. **Then read:** Run Step 6\n",
    "# MAGIC \n",
    "# MAGIC ### \"Unauthorized\" error when reading/writing:\n",
    "# MAGIC - Check Key Vault secrets contain valid access keys\n",
    "# MAGIC - Verify you have proper Event Hub permissions:\n",
    "# MAGIC   - \"Azure Event Hubs Data Sender\" for writing\n",
    "# MAGIC   - \"Azure Event Hubs Data Receiver\" for reading\n",
    "# MAGIC \n",
    "# MAGIC ### Connection timeout:\n",
    "# MAGIC - Event Hub might be empty - send test messages first\n",
    "# MAGIC - Check namespace: `evhns-gdctraining.servicebus.windows.net`\n",
    "# MAGIC - Verify network connectivity from Databricks to Azure\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Quick Reference\n",
    "# MAGIC \n",
    "# MAGIC ### Configuration:\n",
    "# MAGIC - **Event Hub Namespace:** evhns-gdctraining.servicebus.windows.net\n",
    "# MAGIC - **Event Hub Name:** evh-gdctraining-001\n",
    "# MAGIC - **Key Vault Scope:** dbx-ss-kv-gdctraining\n",
    "# MAGIC - **Read Secret:** evh-gdctraining-001-read\n",
    "# MAGIC - **Write Secret:** evh-gdctraining-001-write\n",
    "# MAGIC \n",
    "# MAGIC ### Typical Workflow:\n",
    "# MAGIC \n",
    "# MAGIC 1. **Setup:** Run Steps 1-4 (install, config, retrieve secrets, build connection strings)\n",
    "# MAGIC 2. **Send test messages:** Run Step 5\n",
    "# MAGIC 3. **Wait 5 seconds**\n",
    "# MAGIC 4. **Read messages:** Run Step 6\n",
    "# MAGIC 5. **View data:** Run Steps 7-9\n",
    "# MAGIC 6. **Parse JSON:** Run Step 10\n",
    "# MAGIC 7. **Save to Delta:** Run Step 11\n",
    "# MAGIC \n",
    "# MAGIC ### Key Functions:\n",
    "# MAGIC \n",
    "# MAGIC ```python\n",
    "# MAGIC # Send messages\n",
    "# MAGIC send_test_messages(count=10, sensor_type=\"humidity\")\n",
    "# MAGIC \n",
    "# MAGIC # Verify Key Vault\n",
    "# MAGIC dbutils.secrets.listScopes()\n",
    "# MAGIC dbutils.secrets.list(keyvault_scope)\n",
    "# MAGIC \n",
    "# MAGIC # Save to Delta\n",
    "# MAGIC df_spark.write.format(\"delta\").mode(\"append\").save(\"/path\")\n",
    "# MAGIC ```\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Next Steps\n",
    "# MAGIC \n",
    "# MAGIC ### ‚úÖ What's Working:\n",
    "# MAGIC - Secure credential management with Key Vault\n",
    "# MAGIC - Sending messages to Event Hub\n",
    "# MAGIC - Reading messages from Event Hub\n",
    "# MAGIC - Converting to Spark DataFrames\n",
    "# MAGIC - Saving to Delta Lake\n",
    "# MAGIC - Parsing JSON messages\n",
    "# MAGIC \n",
    "# MAGIC ### üöÄ For Production:\n",
    "# MAGIC 1. **Schedule this notebook as a job** (Workflows ‚Üí Create Job)\n",
    "# MAGIC 2. **Set up Delta tables** with proper schemas\n",
    "# MAGIC 3. **Add data validation** and error handling\n",
    "# MAGIC 4. **Monitor with Databricks monitoring tools**\n",
    "# MAGIC 5. **Set up alerts** for failed jobs\n",
    "# MAGIC \n",
    "# MAGIC ### üí° Security Best Practices:\n",
    "# MAGIC - ‚úÖ Using Key Vault for secrets (not hardcoded)\n",
    "# MAGIC - ‚úÖ Using Databricks secret scopes\n",
    "# MAGIC - ‚úÖ Secrets never displayed in output\n",
    "# MAGIC - Consider: Rotate access keys regularly\n",
    "# MAGIC - Consider: Use Managed Identity for authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05045285-1cab-47c9-a695-7d0abdf0e9a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7b151f-2232-4938-9222-21f70049e4a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Azure Event Hub - Python SDK with Key Vault\n",
    "# MAGIC \n",
    "# MAGIC **‚úÖ Works on shared clusters - No library installation needed**\n",
    "# MAGIC **‚úÖ Uses single shared access key for read and write**\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Install Python SDK\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "%pip install azure-eventhub pandas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event Hub Configuration\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "\n",
    "# Key Vault Secret Scope\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "\n",
    "# Secret name in Key Vault (same for both read and write)\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "\n",
    "# Shared Access Key Name (policy that has both send and listen permissions)\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "# Create widgets for configuration\n",
    "#dbutils.widgets.text(\"eventhub_name\", \"evh-natraining-001\")\n",
    "#dbutils.widgets.text(\"test_message\", \"Test Biju on Databricks!\")\n",
    "\n",
    "print(\"‚úì Configuration loaded successfully\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Retrieve Access Key from Key Vault\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Retrieve the access key from Key Vault via Databricks Secrets\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "    print(\"‚úì Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error retrieving secret: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify secret scope exists:\")\n",
    "    try:\n",
    "        scopes = dbutils.secrets.listScopes()\n",
    "        print(\"   Available scopes:\", [s.name for s in scopes])\n",
    "    except:\n",
    "        print(\"   Cannot list scopes\")\n",
    "    \n",
    "    print(\"\\n2. Verify secret exists in scope:\")\n",
    "    try:\n",
    "        secrets = dbutils.secrets.list(keyvault_scope)\n",
    "        print(f\"   Secrets in '{keyvault_scope}':\", [s.key for s in secrets])\n",
    "    except:\n",
    "        print(f\"   Cannot list secrets in scope '{keyvault_scope}'\")\n",
    "    \n",
    "    print(\"\\n3. Check Key Vault access permissions\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Build Connection String\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Build connection string using the shared access key\n",
    "# This key has both Send and Listen permissions\n",
    "send_conn_str = (\n",
    "    f\"Endpoint=sb://{eh_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={secret_value};\"\n",
    "    f\"EntityPath={eh_name}\"\n",
    ")\n",
    "\n",
    "# Same connection string for both read and write since we have a shared key\n",
    "read_connection_string = send_conn_str\n",
    "write_connection_string = send_conn_str\n",
    "\n",
    "print(\"‚úì Connection string built successfully\")\n",
    "print(f\"  - Using shared access key: {shared_access_key_name}\")\n",
    "print(f\"  - Event Hub: {eh_name}\")\n",
    "print(f\"  - Namespace: {eh_namespace}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: Send Test Messages to Event Hub\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Get test message from widget\n",
    "test_message_text = dbutils.widgets.get(\"test_message\")\n",
    "\n",
    "# Create test messages\n",
    "test_messages = [\n",
    "    {\n",
    "        \"id\": \"test_056757\",\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"message\": test_message_text,\n",
    "        \"sensor\": \"temperature\",\n",
    "        \"value\": 22.5,\n",
    "        \"unit\": \"celsius\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"test_57657\",\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"message\": \"Temperature reading 2\",\n",
    "        \"sensor\": \"temperature\",\n",
    "        \"value\": 23.1,\n",
    "        \"unit\": \"celsius\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"test_57573\",\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"message\": \"Humidity reading\",\n",
    "        \"sensor\": \"humidity\",\n",
    "        \"value\": 65.3,\n",
    "        \"unit\": \"percent\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"test_088812\",\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"message\": \"Pressure reading\",\n",
    "        \"sensor\": \"pressure\",\n",
    "        \"value\": 1013.2,\n",
    "        \"unit\": \"hPa\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"test_0575708\",\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"message\": \"Final test message\",\n",
    "        \"sensor\": \"humidity\",\n",
    "        \"value\": 67.8,\n",
    "        \"unit\": \"percent\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Sending test messages to Event Hub...\")\n",
    "print(f\"Event Hub: {eh_name}\")\n",
    "print(f\"Messages to send: {len(test_messages)}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create producer\n",
    "    producer = EventHubProducerClient.from_connection_string(\n",
    "        conn_str=write_connection_string\n",
    "    )\n",
    "    \n",
    "    # Create and send batch\n",
    "    event_batch = producer.create_batch()\n",
    "    \n",
    "    for msg in test_messages:\n",
    "        event_batch.add(EventData(json.dumps(msg)))\n",
    "        print(f\"  ‚úì Added: {msg['id']} - {msg['message'][:50]}\")\n",
    "    \n",
    "    # Send the batch\n",
    "    producer.send_batch(event_batch)\n",
    "    producer.close()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úì Successfully sent {len(test_messages)} messages to Event Hub!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\n‚è≥ Wait 5 seconds for messages to be available...\")\n",
    "    \n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úó Error sending messages: {str(e)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\nDebugging Information:\")\n",
    "    print(f\"  - Event Hub Namespace: {eh_namespace}\")\n",
    "    print(f\"  - Event Hub Name: {eh_name}\")\n",
    "    print(f\"  - Shared Access Key Name: {shared_access_key_name}\")\n",
    "    print(\"\\nPossible Issues:\")\n",
    "    print(\"  1. Access key is invalid or expired\")\n",
    "    print(\"  2. Shared access policy doesn't have Send permission\")\n",
    "    print(\"  3. Network connectivity issue\")\n",
    "    print(\"  4. Event Hub namespace or name is incorrect\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec517a7-3129-475e-b478-57743130fd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## Step 6: Read from Event Hub\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "\n",
    "# Storage for collected events\n",
    "collected_events = []\n",
    "\n",
    "def on_event(partition_context, event):\n",
    "    \"\"\"Process each event - with proper None checking\"\"\"\n",
    "    try:\n",
    "        # Check if event is None\n",
    "        if event is None:\n",
    "            return\n",
    "        \n",
    "        # Try to get event body\n",
    "        try:\n",
    "            body = event.body_as_str()\n",
    "        except Exception:\n",
    "            body = str(event.body) if hasattr(event, 'body') else \"No body\"\n",
    "        \n",
    "        # Parse event data\n",
    "        event_data = {\n",
    "            'body': body,\n",
    "            'enqueued_time': event.enqueued_time if hasattr(event, 'enqueued_time') else None,\n",
    "            'offset': event.offset if hasattr(event, 'offset') else None,\n",
    "            'sequence_number': event.sequence_number if hasattr(event, 'sequence_number') else None,\n",
    "            'partition_key': event.partition_key if hasattr(event, 'partition_key') else None\n",
    "        }\n",
    "        \n",
    "        collected_events.append(event_data)\n",
    "        \n",
    "        # Print progress\n",
    "        if len(collected_events) <= 10 or len(collected_events) % 10 == 0:\n",
    "            print(f\"  ‚úì Collected {len(collected_events)} events\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing event: {str(e)}\")\n",
    "\n",
    "# Create consumer\n",
    "print(\"Connecting to Event Hub for reading...\")\n",
    "print(f\"Event Hub: {eh_name}\")\n",
    "print(\"Reading events (will read for 20 seconds)...\\n\")\n",
    "\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=read_connection_string,\n",
    "    consumer_group=\"$Default\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with client:\n",
    "        client.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",  # Start from beginning\n",
    "            max_wait_time=20  # Read for 20 seconds\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úì Finished reading from Event Hub\")\n",
    "    print(f\"‚úì Total events collected: {len(collected_events)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úó Error reading from Event Hub: {str(e)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\nDebugging Information:\")\n",
    "    print(f\"  - Event Hub Namespace: {eh_namespace}\")\n",
    "    print(f\"  - Event Hub Name: {eh_name}\")\n",
    "    print(f\"  - Shared Access Key Name: {shared_access_key_name}\")\n",
    "    print(\"\\nPossible Issues:\")\n",
    "    print(\"  1. Access key is invalid or expired\")\n",
    "    print(\"  2. Shared access policy doesn't have Listen permission\")\n",
    "    print(\"  3. Network connectivity issue\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 7: Display Collected Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Found {len(collected_events)} events!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Show first few events\n",
    "    print(\"First 5 events:\")\n",
    "    print(\"-\"*70)\n",
    "    for i, event in enumerate(collected_events[:5], 1):\n",
    "        print(f\"\\nEvent {i}:\")\n",
    "        print(f\"  Body: {event['body'][:100]}...\")  # Truncate long messages\n",
    "        print(f\"  Enqueued Time: {event['enqueued_time']}\")\n",
    "        print(f\"  Offset: {event['offset']}\")\n",
    "        print(f\"  Sequence Number: {event['sequence_number']}\")\n",
    "    print(\"-\"*70)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No events collected from Event Hub\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Event Hub is empty - Try running Step 5 to send test messages\")\n",
    "    print(\"  2. Connection issue - Check your Key Vault secret\")\n",
    "    print(\"  3. Permissions - Verify the shared access policy has Listen permission\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 8: Convert to Spark DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events:\n",
    "    # Convert to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(collected_events)\n",
    "    \n",
    "    print(\"‚úì Created Spark DataFrame\")\n",
    "    print(\"\\nSchema:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "    print(f\"\\nTotal records: {df_spark.count()}\\n\")\n",
    "    \n",
    "    # Display\n",
    "    display(df_spark)\n",
    "else:\n",
    "    print(\"No events to convert to Spark DataFrame\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 9: Parse JSON Message Bodies\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Sample message to understand structure\n",
    "    print(\"Sample message body:\")\n",
    "    print(collected_events[0]['body'])\n",
    "    print()\n",
    "    \n",
    "    # Define schema for your messages\n",
    "    message_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True),\n",
    "        StructField(\"sensor\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True),\n",
    "        StructField(\"unit\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Parse JSON\n",
    "    df_parsed = df_spark.select(\n",
    "        from_json(col(\"body\"), message_schema).alias(\"data\"),\n",
    "        col(\"enqueued_time\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"sequence_number\")\n",
    "    ).select(\"data.*\", \"enqueued_time\", \"offset\", \"sequence_number\")\n",
    "    \n",
    "    print(\"‚úì Parsed JSON messages\")\n",
    "    print(f\"Total parsed records: {df_parsed.count()}\\n\")\n",
    "    \n",
    "    display(df_parsed)\n",
    "else:\n",
    "    print(\"No events to parse\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 10: Save to Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Output path\n",
    "    output_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/eventhub/natraining/data\"\n",
    "    \n",
    "    # Save to Delta\n",
    "    df_spark.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(output_path)\n",
    "    \n",
    "    print(f\"‚úì Saved {len(collected_events)} events to Delta Lake\")\n",
    "    print(f\"  Location: {output_path}\")\n",
    "else:\n",
    "    print(\"No events to save\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 11: Read from Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "output_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/eventhub/natraining/data\"\n",
    "\n",
    "try:\n",
    "    # Read Delta table\n",
    "    df_delta = spark.read.format(\"delta\").load(output_path)\n",
    "    \n",
    "    print(f\"‚úì Delta table loaded\")\n",
    "    print(f\"Total records: {df_delta.count()}\\n\")\n",
    "    \n",
    "    # Display latest records\n",
    "    display(df_delta.orderBy(col(\"enqueued_time\").desc()).limit(100))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Delta table not found: {str(e)}\")\n",
    "    print(\"Run Step 10 first to save data to Delta Lake\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a38da7e-5f33-44d6-af40-5a65ea6bd4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 9: Parse JSON Message Bodies\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Sample message to understand structure\n",
    "    print(\"Sample message body:\")\n",
    "    print(collected_events[0]['body'])\n",
    "    print()\n",
    "    \n",
    "    # Define schema for your messages\n",
    "    message_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True),\n",
    "        StructField(\"sensor\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True),\n",
    "        StructField(\"unit\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Parse JSON\n",
    "    df_parsed = df_spark.select(\n",
    "        from_json(col(\"body\"), message_schema).alias(\"data\"),\n",
    "        col(\"enqueued_time\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"sequence_number\")\n",
    "    ).select(\"data.*\", \"enqueued_time\", \"offset\", \"sequence_number\")\n",
    "    \n",
    "    print(\"‚úì Parsed JSON messages\")\n",
    "    print(f\"Total parsed records: {df_parsed.count()}\\n\")\n",
    "    \n",
    "    display(df_parsed)\n",
    "else:\n",
    "    print(\"No events to parse\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 10: Save to Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if collected_events and len(collected_events) > 0:\n",
    "    # Output path\n",
    "    output_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/eventhub/natraining/data\"\n",
    "    \n",
    "    # Save to Delta\n",
    "    df_spark.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(output_path)\n",
    "    \n",
    "    print(f\"‚úì Saved {len(collected_events)} events to Delta Lake\")\n",
    "    print(f\"  Location: {output_path}\")\n",
    "else:\n",
    "    print(\"No events to save\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 11: Read from Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "output_path = \"/tmp/eventhub/natraining/data\"\n",
    "\n",
    "try:\n",
    "    # Read Delta table\n",
    "    df_delta = spark.read.format(\"delta\").load(output_path)\n",
    "    \n",
    "    print(f\"‚úì Delta table loaded\")\n",
    "    print(f\"Total records: {df_delta.count()}\\n\")\n",
    "    \n",
    "    # Display latest records\n",
    "    display(df_delta.orderBy(col(\"enqueued_time\").desc()).limit(100))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Delta table not found: {str(e)}\")\n",
    "    print(\"Run Step 10 first to save data to Delta Lake\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Eventhub-connection updated",
   "widgets": {
    "eventhub_name": {
     "currentValue": "evh-natraining-biju",
     "nuid": "1c7f699b-e0a3-4e03-a854-675df5f6a023",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "evh-natraining-001",
      "label": null,
      "name": "eventhub_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "evh-natraining-001",
      "label": null,
      "name": "eventhub_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "test_message": {
     "currentValue": "Test Biju on Databricks!",
     "nuid": "0514100d-ee16-4760-bc16-9a6a48fee1c1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Test Biju on Databricks!",
      "label": null,
      "name": "test_message",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Test Biju on Databricks!",
      "label": null,
      "name": "test_message",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
