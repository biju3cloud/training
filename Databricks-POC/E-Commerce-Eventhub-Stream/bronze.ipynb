{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3243d18c-677c-4dee-abe6-b70d6a8bc4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f87a72f-e55e-4542-9b83-259b53465cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeebcc6-d4f9-4df7-99ce-5f34119177a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Define Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event payload schema\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úì Event schema defined\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 1: Read from Event Hub and Parse Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING EVENT HUB STREAMING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read streaming data from Event Hub\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON payload and add metadata\n",
    "parsed_stream = (raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"timestamp\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "    .withColumn(\"order_timestamp_parsed\", to_timestamp(col(\"order_timestamp\")))\n",
    ")\n",
    "\n",
    "print(\"‚úì Stream parsing configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 2: Write Orders to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ORDERS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select orders columns\n",
    "orders_stream = parsed_stream.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"location\"),\n",
    "    col(\"product_id\"),  # FK to products\n",
    "    col(\"order_status\"),\n",
    "    col(\"payment_method\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"discount_pct\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"order_timestamp\"),\n",
    "    col(\"order_timestamp_parsed\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"kafka_offset\"),\n",
    "    col(\"partition_id\"),\n",
    "    col(\"bronze_timestamp\")\n",
    ")\n",
    "\n",
    "# Write orders stream to Delta table\n",
    "orders_query = (orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")  # Micro-batch every 10 seconds\n",
    "    .toTable(bronze_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Orders stream started\")\n",
    "print(f\"  Query ID: {orders_query.id}\")\n",
    "print(f\"  Target: {bronze_orders_table}\")\n",
    "print(f\"  Checkpoint: {orders_checkpoint}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 3: Deduplicate and Write Products to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PRODUCTS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select and deduplicate products\n",
    "products_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"base_price\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"bronze_timestamp\")\n",
    "    )\n",
    "    .dropDuplicates([\"product_id\"])  # Keep only unique products per batch\n",
    ")\n",
    "\n",
    "# Write products stream to Delta table with merge logic\n",
    "products_query = (products_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", products_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .foreachBatch(lambda batch_df, batch_id: upsert_products(batch_df, batch_id))\n",
    "    .start()\n",
    ")\n",
    "\n",
    "def upsert_products(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Upsert products to avoid duplicates across batches\n",
    "    \"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    # Create temp view\n",
    "    batch_df.createOrReplaceTempView(\"products_batch\")\n",
    "    \n",
    "    # Merge logic\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {bronze_products_table} target\n",
    "    USING products_batch source\n",
    "    ON target.product_id = source.product_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.product_name = source.product_name,\n",
    "            target.category = source.category,\n",
    "            target.brand = source.brand,\n",
    "            target.base_price = source.base_price,\n",
    "            target.unit_price = source.unit_price,\n",
    "            target.bronze_timestamp = source.bronze_timestamp\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (product_id, product_name, category, brand, base_price, unit_price, bronze_timestamp)\n",
    "        VALUES (source.product_id, source.product_name, source.category, source.brand, \n",
    "                source.base_price, source.unit_price, source.bronze_timestamp)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute merge\n",
    "    try:\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"  Batch {batch_id}: Upserted {batch_df.count()} products\")\n",
    "    except Exception as e:\n",
    "        # If table doesn't exist, create it\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "            batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_products_table)\n",
    "            print(f\"  Batch {batch_id}: Created table and inserted {batch_df.count()} products\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"‚úì Products stream started\")\n",
    "print(f\"  Query ID: {products_query.id}\")\n",
    "print(f\"  Target: {bronze_products_table}\")\n",
    "print(f\"  Checkpoint: {products_checkpoint}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Streaming Queries\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVE STREAMING QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nQuery ID: {stream.id}\")\n",
    "    print(f\"  Name: {stream.name if stream.name else 'unnamed'}\")\n",
    "    print(f\"  Status: {stream.status['message']}\")\n",
    "    print(f\"  Is Active: {stream.isActive}\")\n",
    "    \n",
    "    if stream.recentProgress:\n",
    "        latest = stream.recentProgress[-1]\n",
    "        print(f\"  Recent Progress:\")\n",
    "        print(f\"    - Batch: {latest.get('batchId', 'N/A')}\")\n",
    "        print(f\"    - Input Rows: {latest.get('numInputRows', 0)}\")\n",
    "        print(f\"    - Processing Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Wait and Monitor (Run for 30 seconds)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONITORING STREAMS FOR 30 SECONDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Check orders table\n",
    "    try:\n",
    "        orders_count = spark.table(bronze_orders_table).count()\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Orders: {orders_count} records\")\n",
    "    except:\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Orders table not created yet\")\n",
    "    \n",
    "    # Check products table\n",
    "    try:\n",
    "        products_count = spark.table(bronze_products_table).count()\n",
    "        print(f\"    Products: {products_count} records\")\n",
    "    except:\n",
    "        print(f\"    Products table not created yet\")\n",
    "\n",
    "print(\"\\n‚úì Monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Bronze Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRONZE LAYER VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Orders table\n",
    "try:\n",
    "    orders_df = spark.table(bronze_orders_table)\n",
    "    orders_count = orders_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Orders Table: {bronze_orders_table}\")\n",
    "    print(f\"   Total Records: {orders_count}\")\n",
    "    \n",
    "    if orders_count > 0:\n",
    "        print(\"\\n   Latest 10 orders:\")\n",
    "        display(orders_df.orderBy(desc(\"bronze_timestamp\")).limit(10))\n",
    "        \n",
    "        print(\"\\n   Orders by Location:\")\n",
    "        display(\n",
    "            orders_df.groupBy(\"location\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"order_count\"),\n",
    "                sum(\"total_amount\").alias(\"total_revenue\")\n",
    "            )\n",
    "            .orderBy(desc(\"order_count\"))\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orders table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Products table\n",
    "try:\n",
    "    products_df = spark.table(bronze_products_table)\n",
    "    products_count = products_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Products Table: {bronze_products_table}\")\n",
    "    print(f\"   Total Records: {products_count}\")\n",
    "    \n",
    "    if products_count > 0:\n",
    "        print(\"\\n   All products:\")\n",
    "        display(products_df.orderBy(\"product_id\"))\n",
    "        \n",
    "        print(\"\\n   Products by Category:\")\n",
    "        display(\n",
    "            products_df.groupBy(\"category\", \"brand\")\n",
    "            .count()\n",
    "            .orderBy(\"category\", \"brand\")\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Products table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream Management\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Option 1: Keep streams running\n",
    "print(\"\\nüí° Streams are running continuously\")\n",
    "print(\"   They will process new events as they arrive in Event Hub\")\n",
    "print(\"\\n   To stop streams, run the next cell\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop All Streams (Run when done)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop all streams\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"STOPPING ALL STREAMING QUERIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nStopping: {stream.id}\")\n",
    "    stream.stop()\n",
    "    print(f\"  ‚úì Stopped\")\n",
    "\n",
    "print(\"\\n‚úì All streams stopped\")\n",
    "print(\"\\nNote: You can restart streams by re-running the stream cells above\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì BRONZE LAYER STREAMING SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    orders_count = spark.table(bronze_orders_table).count()\n",
    "    products_count = spark.table(bronze_products_table).count()\n",
    "    \n",
    "    print(f\"\\nCurrent State:\")\n",
    "    print(f\"  Orders: {orders_count} records\")\n",
    "    print(f\"  Products: {products_count} records\")\n",
    "    print(f\"\\nStreaming Status:\")\n",
    "    print(f\"  Active Queries: {len(spark.streams.active)}\")\n",
    "    \n",
    "    for stream in spark.streams.active:\n",
    "        print(f\"    - {stream.id}: {stream.status['message']}\")\n",
    "    \n",
    "    print(f\"\\nData Flow:\")\n",
    "    print(f\"  Event Hub ({eh_name})\")\n",
    "    print(f\"    ‚Üì Kafka Streaming\")\n",
    "    print(f\"  Parsed Events\")\n",
    "    print(f\"    ‚Üì Split & Transform\")\n",
    "    print(f\"  Bronze Tables\")\n",
    "    print(f\"    ‚Ä¢ Orders (with product_id FK)\")\n",
    "    print(f\"    ‚Ä¢ Products (deduplicated)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Next Steps:\")\n",
    "    print(\"  1. Monitor streams using 'Monitor Streaming Queries' cell\")\n",
    "    print(\"  2. Run Silver Layer notebook to join Orders with Products\")\n",
    "    print(\"  3. Stop streams when done using 'Stop All Streams' cell\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Tables not created yet. Wait for streams to process data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113831fa-4b81-4fa1-b18b-5976c14c6739",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765835060382}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from   `na-dbxtraining`.biju_bronze.productsnew"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6048721265021639,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
