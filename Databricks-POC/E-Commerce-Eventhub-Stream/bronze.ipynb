{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3243d18c-677c-4dee-abe6-b70d6a8bc4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f87a72f-e55e-4542-9b83-259b53465cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeebcc6-d4f9-4df7-99ce-5f34119177a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Define Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event payload schema\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úì Event schema defined\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 1: Read from Event Hub and Parse Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING EVENT HUB STREAMING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read streaming data from Event Hub\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON payload and add metadata\n",
    "parsed_stream = (raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"timestamp\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "    .withColumn(\"order_timestamp_parsed\", to_timestamp(col(\"order_timestamp\")))\n",
    ")\n",
    "\n",
    "print(\"‚úì Stream parsing configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 2: Write Orders to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ORDERS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select orders columns\n",
    "orders_stream = parsed_stream.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"location\"),\n",
    "    col(\"product_id\"),  # FK to products\n",
    "    col(\"order_status\"),\n",
    "    col(\"payment_method\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"discount_pct\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"order_timestamp\"),\n",
    "    col(\"order_timestamp_parsed\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"kafka_offset\"),\n",
    "    col(\"partition_id\"),\n",
    "    col(\"bronze_timestamp\")\n",
    ")\n",
    "\n",
    "# Write orders stream to Delta table\n",
    "orders_query = (orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")  # Micro-batch every 10 seconds\n",
    "    .toTable(bronze_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Orders stream started\")\n",
    "print(f\"  Query ID: {orders_query.id}\")\n",
    "print(f\"  Target: {bronze_orders_table}\")\n",
    "print(f\"  Checkpoint: {orders_checkpoint}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 3: Deduplicate and Write Products to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PRODUCTS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select and deduplicate products\n",
    "products_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"base_price\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"bronze_timestamp\")\n",
    "    )\n",
    "    .dropDuplicates([\"product_id\"])  # Keep only unique products per batch\n",
    ")\n",
    "\n",
    "# Write products stream to Delta table with merge logic\n",
    "products_query = (products_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", products_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .foreachBatch(lambda batch_df, batch_id: upsert_products(batch_df, batch_id))\n",
    "    .start()\n",
    ")\n",
    "\n",
    "def upsert_products(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Upsert products to avoid duplicates across batches\n",
    "    \"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    # Create temp view\n",
    "    batch_df.createOrReplaceTempView(\"products_batch\")\n",
    "    \n",
    "    # Merge logic\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {bronze_products_table} target\n",
    "    USING products_batch source\n",
    "    ON target.product_id = source.product_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.product_name = source.product_name,\n",
    "            target.category = source.category,\n",
    "            target.brand = source.brand,\n",
    "            target.base_price = source.base_price,\n",
    "            target.unit_price = source.unit_price,\n",
    "            target.bronze_timestamp = source.bronze_timestamp\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (product_id, product_name, category, brand, base_price, unit_price, bronze_timestamp)\n",
    "        VALUES (source.product_id, source.product_name, source.category, source.brand, \n",
    "                source.base_price, source.unit_price, source.bronze_timestamp)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute merge\n",
    "    try:\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"  Batch {batch_id}: Upserted {batch_df.count()} products\")\n",
    "    except Exception as e:\n",
    "        # If table doesn't exist, create it\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "            batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_products_table)\n",
    "            print(f\"  Batch {batch_id}: Created table and inserted {batch_df.count()} products\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"‚úì Products stream started\")\n",
    "print(f\"  Query ID: {products_query.id}\")\n",
    "print(f\"  Target: {bronze_products_table}\")\n",
    "print(f\"  Checkpoint: {products_checkpoint}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Streaming Queries\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVE STREAMING QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nQuery ID: {stream.id}\")\n",
    "    print(f\"  Name: {stream.name if stream.name else 'unnamed'}\")\n",
    "    print(f\"  Status: {stream.status['message']}\")\n",
    "    print(f\"  Is Active: {stream.isActive}\")\n",
    "    \n",
    "    if stream.recentProgress:\n",
    "        latest = stream.recentProgress[-1]\n",
    "        print(f\"  Recent Progress:\")\n",
    "        print(f\"    - Batch: {latest.get('batchId', 'N/A')}\")\n",
    "        print(f\"    - Input Rows: {latest.get('numInputRows', 0)}\")\n",
    "        print(f\"    - Processing Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Wait and Monitor (Run for 30 seconds)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONITORING STREAMS FOR 30 SECONDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Check orders table\n",
    "    try:\n",
    "        orders_count = spark.table(bronze_orders_table).count()\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Orders: {orders_count} records\")\n",
    "    except:\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Orders table not created yet\")\n",
    "    \n",
    "    # Check products table\n",
    "    try:\n",
    "        products_count = spark.table(bronze_products_table).count()\n",
    "        print(f\"    Products: {products_count} records\")\n",
    "    except:\n",
    "        print(f\"    Products table not created yet\")\n",
    "\n",
    "print(\"\\n‚úì Monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Bronze Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRONZE LAYER VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Orders table\n",
    "try:\n",
    "    orders_df = spark.table(bronze_orders_table)\n",
    "    orders_count = orders_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Orders Table: {bronze_orders_table}\")\n",
    "    print(f\"   Total Records: {orders_count}\")\n",
    "    \n",
    "    if orders_count > 0:\n",
    "        print(\"\\n   Latest 10 orders:\")\n",
    "        display(orders_df.orderBy(desc(\"bronze_timestamp\")).limit(10))\n",
    "        \n",
    "        print(\"\\n   Orders by Location:\")\n",
    "        display(\n",
    "            orders_df.groupBy(\"location\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"order_count\"),\n",
    "                sum(\"total_amount\").alias(\"total_revenue\")\n",
    "            )\n",
    "            .orderBy(desc(\"order_count\"))\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orders table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Products table\n",
    "try:\n",
    "    products_df = spark.table(bronze_products_table)\n",
    "    products_count = products_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Products Table: {bronze_products_table}\")\n",
    "    print(f\"   Total Records: {products_count}\")\n",
    "    \n",
    "    if products_count > 0:\n",
    "        print(\"\\n   All products:\")\n",
    "        display(products_df.orderBy(\"product_id\"))\n",
    "        \n",
    "        print(\"\\n   Products by Category:\")\n",
    "        display(\n",
    "            products_df.groupBy(\"category\", \"brand\")\n",
    "            .count()\n",
    "            .orderBy(\"category\", \"brand\")\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Products table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream Management\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Option 1: Keep streams running\n",
    "print(\"\\nüí° Streams are running continuously\")\n",
    "print(\"   They will process new events as they arrive in Event Hub\")\n",
    "print(\"\\n   To stop streams, run the next cell\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop All Streams (Run when done)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop all streams\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"STOPPING ALL STREAMING QUERIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nStopping: {stream.id}\")\n",
    "    stream.stop()\n",
    "    print(f\"  ‚úì Stopped\")\n",
    "\n",
    "print(\"\\n‚úì All streams stopped\")\n",
    "print(\"\\nNote: You can restart streams by re-running the stream cells above\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì BRONZE LAYER STREAMING SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    orders_count = spark.table(bronze_orders_table).count()\n",
    "    products_count = spark.table(bronze_products_table).count()\n",
    "    \n",
    "    print(f\"\\nCurrent State:\")\n",
    "    print(f\"  Orders: {orders_count} records\")\n",
    "    print(f\"  Products: {products_count} records\")\n",
    "    print(f\"\\nStreaming Status:\")\n",
    "    print(f\"  Active Queries: {len(spark.streams.active)}\")\n",
    "    \n",
    "    for stream in spark.streams.active:\n",
    "        print(f\"    - {stream.id}: {stream.status['message']}\")\n",
    "    \n",
    "    print(f\"\\nData Flow:\")\n",
    "    print(f\"  Event Hub ({eh_name})\")\n",
    "    print(f\"    ‚Üì Kafka Streaming\")\n",
    "    print(f\"  Parsed Events\")\n",
    "    print(f\"    ‚Üì Split & Transform\")\n",
    "    print(f\"  Bronze Tables\")\n",
    "    print(f\"    ‚Ä¢ Orders (with product_id FK)\")\n",
    "    print(f\"    ‚Ä¢ Products (deduplicated)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Next Steps:\")\n",
    "    print(\"  1. Monitor streams using 'Monitor Streaming Queries' cell\")\n",
    "    print(\"  2. Run Silver Layer notebook to join Orders with Products\")\n",
    "    print(\"  3. Stop streams when done using 'Stop All Streams' cell\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Tables not created yet. Wait for streams to process data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113831fa-4b81-4fa1-b18b-5976c14c6739",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765835060382}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from   `na-dbxtraining`.biju_bronze.products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffa7642-0244-42ef-a3fe-b6a4fc19c572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table Names (with backticks for catalog)\n",
    "bronze_orders_table = f\"`{catalog}`.{schema_bronze}.orders\"\n",
    "bronze_products_table = f\"`{catalog}`.{schema_bronze}.products\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0bb77ee-ab7d-416e-bd77-a4582703e3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer - Orders Stream\n",
    "# MAGIC Reads events from Event Hub and creates Orders table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event Hub Configuration\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "catalog = \"na-dbxtraining\"\n",
    "schema_bronze = \"biju_bronze\"\n",
    "\n",
    "# Table Names (with backticks for catalog)\n",
    "bronze_orders_table = f\"`{catalog}`.{schema_bronze}.orders\"\n",
    "\n",
    "# Checkpoint location\n",
    "checkpoint_base = f\"/Volumes/na-dbxtraining/biju_raw/biju_vol/checkpoints/{catalog.replace('-', '_')}\"\n",
    "orders_checkpoint = f\"{checkpoint_base}/bronze_orders\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ORDERS STREAM CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Event Hub: {eh_name}\")\n",
    "print(f\"Orders Table: {bronze_orders_table}\")\n",
    "print(f\"Checkpoint: {orders_checkpoint}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Get Secret from Key Vault\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "    print(\"‚úì Successfully retrieved secret from Key Vault\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Build Connection String and Kafka Options\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Connection string\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eh_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={secret_value}\"\n",
    ")\n",
    "\n",
    "# Kafka options for streaming\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}:9093\",\n",
    "    \"subscribe\": eh_name,\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";',\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"30000\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"maxOffsetsPerTrigger\": \"10000\"\n",
    "}\n",
    "\n",
    "print(\"‚úì Kafka options configured for streaming\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Define Orders Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event payload schema\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úì Event schema defined\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read from Event Hub and Parse Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING EVENT HUB STREAMING FOR ORDERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read streaming data from Event Hub\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON payload and add metadata\n",
    "parsed_stream = (raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"timestamp\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "    .withColumn(\"order_timestamp_parsed\", to_timestamp(col(\"order_timestamp\")))\n",
    ")\n",
    "\n",
    "print(\"‚úì Stream parsing configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Select Orders Columns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURING ORDERS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select orders columns\n",
    "orders_stream = parsed_stream.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"location\"),\n",
    "    col(\"product_id\"),  # FK to products\n",
    "    col(\"order_status\"),\n",
    "    col(\"payment_method\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"discount_pct\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"order_timestamp\"),\n",
    "    col(\"order_timestamp_parsed\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"kafka_offset\"),\n",
    "    col(\"partition_id\"),\n",
    "    col(\"bronze_timestamp\")\n",
    ")\n",
    "\n",
    "print(\"‚úì Orders columns selected\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write Orders Stream to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ORDERS STREAM WRITE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Write orders stream to Delta table\n",
    "orders_query = (orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "  #  .trigger(processingTime=\"10 seconds\")  # Micro-batch every 10 seconds\n",
    "    .trigger(once=True) # for testing \n",
    "    .toTable(bronze_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Orders stream started\")\n",
    "print(f\"  Query ID: {orders_query.id}\")\n",
    "print(f\"  Target: {bronze_orders_table}\")\n",
    "print(f\"  Checkpoint: {orders_checkpoint}\")\n",
    "print(f\"  Trigger: 10 seconds\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Orders Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVE ORDERS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    if stream.id == orders_query.id:\n",
    "        print(f\"\\nQuery ID: {stream.id}\")\n",
    "        print(f\"  Name: {stream.name if stream.name else 'unnamed'}\")\n",
    "        print(f\"  Status: {stream.status['message']}\")\n",
    "        print(f\"  Is Active: {stream.isActive}\")\n",
    "        \n",
    "        if stream.recentProgress:\n",
    "            latest = stream.recentProgress[-1]\n",
    "            print(f\"  Recent Progress:\")\n",
    "            print(f\"    - Batch: {latest.get('batchId', 'N/A')}\")\n",
    "            print(f\"    - Input Rows: {latest.get('numInputRows', 0)}\")\n",
    "            print(f\"    - Processing Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Wait and Monitor (30 seconds)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONITORING ORDERS STREAM FOR 30 SECONDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        orders_count = spark.table(bronze_orders_table).count()\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Orders: {orders_count} records\")\n",
    "    except:\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Orders table not created yet\")\n",
    "\n",
    "print(\"\\n‚úì Monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Orders Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ORDERS TABLE VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    orders_df = spark.table(bronze_orders_table)\n",
    "    orders_count = orders_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Orders Table: {bronze_orders_table}\")\n",
    "    print(f\"   Total Records: {orders_count}\")\n",
    "    \n",
    "    if orders_count > 0:\n",
    "        print(\"\\n   Schema:\")\n",
    "        orders_df.printSchema()\n",
    "        \n",
    "        print(\"\\n   Latest 10 orders:\")\n",
    "        display(orders_df.orderBy(desc(\"bronze_timestamp\")).limit(10))\n",
    "        \n",
    "        print(\"\\n   Orders by Location:\")\n",
    "        display(\n",
    "            orders_df.groupBy(\"location\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"order_count\"),\n",
    "                sum(\"total_amount\").alias(\"total_revenue\")\n",
    "            )\n",
    "            .orderBy(desc(\"order_count\"))\n",
    "        )\n",
    "        \n",
    "        print(\"\\n   Orders by Status:\")\n",
    "        display(\n",
    "            orders_df.groupBy(\"order_status\")\n",
    "            .count()\n",
    "            .orderBy(desc(\"count\"))\n",
    "        )\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orders table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop Orders Stream (Run when done)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop the orders stream\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"STOPPING ORDERS STREAM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if orders_query.isActive:\n",
    "    print(f\"\\nStopping: {orders_query.id}\")\n",
    "    orders_query.stop()\n",
    "    print(f\"  ‚úì Stopped\")\n",
    "else:\n",
    "    print(\"\\nStream is not active\")\n",
    "\n",
    "print(\"\\n‚úì Orders stream stopped\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì ORDERS STREAM SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    orders_count = spark.table(bronze_orders_table).count()\n",
    "    \n",
    "    print(f\"\\nCurrent State:\")\n",
    "    print(f\"  Orders: {orders_count} records\")\n",
    "    \n",
    "    print(f\"\\nStreaming Status:\")\n",
    "    print(f\"  Query Active: {orders_query.isActive}\")\n",
    "    print(f\"  Query ID: {orders_query.id}\")\n",
    "    \n",
    "    print(f\"\\nData Flow:\")\n",
    "    print(f\"  Event Hub ({eh_name})\")\n",
    "    print(f\"    ‚Üì Kafka Protocol\")\n",
    "    print(f\"  Raw Kafka Stream\")\n",
    "    print(f\"    ‚Üì Parse JSON\")\n",
    "    print(f\"  Parsed Events\")\n",
    "    print(f\"    ‚Üì Select Orders Columns\")\n",
    "    print(f\"  Bronze Orders Table\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Next Steps:\")\n",
    "    print(\"  1. Run Bronze-Products notebook to create products table\")\n",
    "    print(\"  2. Run Silver Layer notebook to join Orders with Products\")\n",
    "    print(\"  3. Stop stream when done using 'Stop Orders Stream' cell\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orders table not created yet. Wait for stream to process data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5622ae0a-dc23-4acf-84c6-5f35baacaf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer - Products Stream\n",
    "# MAGIC Reads events from Event Hub and creates deduplicated Products table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event Hub Configuration\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "catalog = \"na-dbxtraining\"\n",
    "schema_bronze = \"biju_bronze\"\n",
    "\n",
    "# Table Names (with backticks for catalog)\n",
    "bronze_products_table = f\"`{catalog}`.{schema_bronze}.products\"\n",
    "\n",
    "# Checkpoint location\n",
    "checkpoint_base = f\"/Volumes/na-dbxtraining/biju_raw/biju_vol/checkpoints/{catalog.replace('-', '_')}\"\n",
    "products_checkpoint = f\"{checkpoint_base}/bronze_products\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRODUCTS STREAM CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Event Hub: {eh_name}\")\n",
    "print(f\"Products Table: {bronze_products_table}\")\n",
    "print(f\"Checkpoint: {products_checkpoint}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Get Secret from Key Vault\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "    print(\"‚úì Successfully retrieved secret from Key Vault\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Build Connection String and Kafka Options\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Connection string\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eh_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={secret_value}\"\n",
    ")\n",
    "\n",
    "# Kafka options for streaming\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}:9093\",\n",
    "    \"subscribe\": eh_name,\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";',\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"30000\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"maxOffsetsPerTrigger\": \"10000\"\n",
    "}\n",
    "\n",
    "print(\"‚úì Kafka options configured for streaming\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Define Products Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event payload schema (we'll extract product fields)\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úì Event schema defined\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read from Event Hub and Parse Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING EVENT HUB STREAMING FOR PRODUCTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read streaming data from Event Hub\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON payload and add metadata\n",
    "parsed_stream = (raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"timestamp\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    ")\n",
    "\n",
    "print(\"‚úì Stream parsing configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Select and Deduplicate Products\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURING PRODUCTS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select and deduplicate products\n",
    "products_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"base_price\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"bronze_timestamp\")\n",
    "    )\n",
    "    .dropDuplicates([\"product_id\"])  # Keep only unique products per batch\n",
    ")\n",
    "\n",
    "print(\"‚úì Products columns selected with deduplication\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Define Upsert Function\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def upsert_products(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Upsert products to avoid duplicates across batches\n",
    "    Updates existing products and inserts new ones\n",
    "    \"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        print(f\"  Batch {batch_id}: No products to process\")\n",
    "        return\n",
    "    \n",
    "    # Create temp view\n",
    "    batch_df.createOrReplaceTempView(\"products_batch\")\n",
    "    \n",
    "    # Merge logic\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {bronze_products_table} target\n",
    "    USING products_batch source\n",
    "    ON target.product_id = source.product_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.product_name = source.product_name,\n",
    "            target.category = source.category,\n",
    "            target.brand = source.brand,\n",
    "            target.base_price = source.base_price,\n",
    "            target.unit_price = source.unit_price,\n",
    "            target.bronze_timestamp = source.bronze_timestamp\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (product_id, product_name, category, brand, base_price, unit_price, bronze_timestamp)\n",
    "        VALUES (source.product_id, source.product_name, source.category, source.brand, \n",
    "                source.base_price, source.unit_price, source.bronze_timestamp)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute merge\n",
    "    try:\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"  Batch {batch_id}: Upserted {batch_df.count()} products\")\n",
    "    except Exception as e:\n",
    "        # If table doesn't exist, create it\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "            batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_products_table)\n",
    "            print(f\"  Batch {batch_id}: Created table and inserted {batch_df.count()} products\")\n",
    "        else:\n",
    "            print(f\"  Batch {batch_id}: Error - {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "print(\"‚úì Upsert function defined\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write Products Stream to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PRODUCTS STREAM WRITE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Write products stream to Delta table with merge logic\n",
    "products_query = (products_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(upsert_products)\n",
    "    .option(\"checkpointLocation\", products_checkpoint)\n",
    "    #.trigger(processingTime=\"10 seconds\")\n",
    "    .trigger(once=True) # for testing\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"‚úì Products stream started\")\n",
    "print(f\"  Query ID: {products_query.id}\")\n",
    "print(f\"  Target: {bronze_products_table}\")\n",
    "print(f\"  Checkpoint: {products_checkpoint}\")\n",
    "print(f\"  Trigger: 10 seconds\")\n",
    "print(f\"  Method: foreachBatch with MERGE (upsert)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Products Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVE PRODUCTS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    if stream.id == products_query.id:\n",
    "        print(f\"\\nQuery ID: {stream.id}\")\n",
    "        print(f\"  Name: {stream.name if stream.name else 'unnamed'}\")\n",
    "        print(f\"  Status: {stream.status['message']}\")\n",
    "        print(f\"  Is Active: {stream.isActive}\")\n",
    "        \n",
    "        if stream.recentProgress:\n",
    "            latest = stream.recentProgress[-1]\n",
    "            print(f\"  Recent Progress:\")\n",
    "            print(f\"    - Batch: {latest.get('batchId', 'N/A')}\")\n",
    "            print(f\"    - Input Rows: {latest.get('numInputRows', 0)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Wait and Monitor (30 seconds)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONITORING PRODUCTS STREAM FOR 30 SECONDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        products_count = spark.table(bronze_products_table).count()\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Products: {products_count} unique products\")\n",
    "    except:\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Products table not created yet\")\n",
    "\n",
    "print(\"\\n‚úì Monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Products Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRODUCTS TABLE VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    products_df = spark.table(bronze_products_table)\n",
    "    products_count = products_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Products Table: {bronze_products_table}\")\n",
    "    print(f\"   Total Unique Products: {products_count}\")\n",
    "    \n",
    "    if products_count > 0:\n",
    "        print(\"\\n   Schema:\")\n",
    "        products_df.printSchema()\n",
    "        \n",
    "        print(\"\\n   All products:\")\n",
    "        display(products_df.orderBy(\"product_id\"))\n",
    "        \n",
    "        print(\"\\n   Products by Category:\")\n",
    "        display(\n",
    "            products_df.groupBy(\"category\", \"brand\")\n",
    "            .count()\n",
    "            .orderBy(\"category\", \"brand\")\n",
    "        )\n",
    "        \n",
    "        print(\"\\n   Price Range by Category:\")\n",
    "        display(\n",
    "            products_df.groupBy(\"category\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"product_count\"),\n",
    "                min(\"base_price\").alias(\"min_price\"),\n",
    "                max(\"base_price\").alias(\"max_price\"),\n",
    "                avg(\"base_price\").alias(\"avg_price\")\n",
    "            )\n",
    "            .orderBy(\"category\")\n",
    "        )\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Products table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop Products Stream (Run when done)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop the products stream\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"STOPPING PRODUCTS STREAM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if products_query.isActive:\n",
    "    print(f\"\\nStopping: {products_query.id}\")\n",
    "    products_query.stop()\n",
    "    print(f\"  ‚úì Stopped\")\n",
    "else:\n",
    "    print(\"\\nStream is not active\")\n",
    "\n",
    "print(\"\\n‚úì Products stream stopped\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì PRODUCTS STREAM SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    products_count = spark.table(bronze_products_table).count()\n",
    "    \n",
    "    print(f\"\\nCurrent State:\")\n",
    "    print(f\"  Unique Products: {products_count} records\")\n",
    "    \n",
    "    print(f\"\\nStreaming Status:\")\n",
    "    print(f\"  Query Active: {products_query.isActive}\")\n",
    "    print(f\"  Query ID: {products_query.id}\")\n",
    "    \n",
    "    print(f\"\\nData Flow:\")\n",
    "    print(f\"  Event Hub ({eh_name})\")\n",
    "    print(f\"    ‚Üì Kafka Protocol\")\n",
    "    print(f\"  Raw Kafka Stream\")\n",
    "    print(f\"    ‚Üì Parse JSON\")\n",
    "    print(f\"  Parsed Events\")\n",
    "    print(f\"    ‚Üì Extract Product Fields\")\n",
    "    print(f\"  Deduplicated Products\")\n",
    "    print(f\"    ‚Üì MERGE (Upsert)\")\n",
    "    print(f\"  Bronze Products Table (Unique)\")\n",
    "    \n",
    "    print(f\"\\nKey Features:\")\n",
    "    print(f\"  ‚Ä¢ Deduplication within each batch\")\n",
    "    print(f\"  ‚Ä¢ MERGE operation across batches\")\n",
    "    print(f\"  ‚Ä¢ Updates existing products\")\n",
    "    print(f\"  ‚Ä¢ Inserts new products\")\n",
    "    print(f\"  ‚Ä¢ Maintains unique product_id constraint\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Next Steps:\")\n",
    "    print(\"  1. Ensure Bronze-Orders notebook is also running\")\n",
    "    print(\"  2. Run Silver Layer notebook to join Orders with Products\")\n",
    "    print(\"  3. Stop stream when done using 'Stop Products Stream' cell\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Products table not created yet. Wait for stream to process data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a8c4c9-0632-4d76-88ac-aa2792380221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from `na-dbxtraining`.biju_bronze.products"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5534691258377251,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
