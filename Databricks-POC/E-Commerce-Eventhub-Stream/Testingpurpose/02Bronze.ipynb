{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5455a6d-a24a-48de-b4b8-aa2afa6a6e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00eventhubconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "017132c3-cee9-4e76-a098-cfea6551a235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16ff752-92c5-4792-8c50-280cefef696c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47bcc03-3686-40c0-bcbe-ffbcca25fe0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Replace 2.12 and 2.3.22 with your cluster's correct versions\n",
    "# This downloads the JAR and puts it in the Spark classpath upon cluster restart\n",
    "wget https://repo1.maven.org/maven2/com/microsoft/azure/azure-eventhubs-spark_2.12/2.3.22/azure-eventhubs-spark_2.12-2.3.22.jar -P /databricks/jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9272fe-30be-4da6-8648-c5f192c33fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "eventhub_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eventhub_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "#shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(\n",
    "        scope=keyvault_scope,\n",
    "        key=secret_name\n",
    "    )\n",
    "    print(\"✓ Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "shared_access_key=secret_value\n",
    "shared_access_key_name = \"RootManageSharedAccessKey\"\n",
    "\n",
    "# Build connection string\n",
    "eventhub_connection_string = f\"Endpoint=sb://{eventhub_namespace}.servicebus.windows.net/;SharedAccessKeyName={shared_access_key_name};SharedAccessKey={shared_access_key}\"\n",
    "\n",
    "# Kafka options dictionary\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eventhub_namespace}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": eventhub_name,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{eventhub_connection_string}\";',\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"60000\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"failOnDataLoss\": \"false\"\n",
    "}\n",
    "\n",
    "# Storage paths\n",
    "bronze_orders_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/bronze/orders\"\n",
    "bronze_products_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/bronze/products\"\n",
    "silver_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol//silver/order_details\"\n",
    "gold_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol//gold/aggregations\"\n",
    "\n",
    "# Checkpoint locations\n",
    "checkpoint_bronze_orders = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/bronze_orders\"\n",
    "checkpoint_bronze_products = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/bronze_products\"\n",
    "checkpoint_silver = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/silver\"\n",
    "checkpoint_gold = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fadc120b-b8b0-425f-a008-bd6ce4402fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Schema for enriched event\n",
    "enriched_schema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"customer_name\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"order_status\", StringType()),\n",
    "    StructField(\"payment_method\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"discount_pct\", DoubleType()),\n",
    "    StructField(\"total_amount\", DoubleType()),\n",
    "    StructField(\"order_timestamp\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"brand\", StringType()),\n",
    "    StructField(\"base_price\", DoubleType()),\n",
    "    StructField(\"unit_price\", DoubleType())\n",
    "])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read from Event Hub via Kafka\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read stream using Kafka connector\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"✓ Connected to Event Hub\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Parse JSON from Event Hub body\n",
    "parsed_stream = (\n",
    "    raw_stream\n",
    "    .withColumn(\"body_string\", col(\"value\").cast(\"string\"))\n",
    "    .withColumn(\"data\", from_json(col(\"body_string\"), enriched_schema))\n",
    "    .select(\"data.*\", col(\"timestamp\").alias(\"event_time\"))\n",
    ")\n",
    "\n",
    "print(\"✓ Parsed event stream\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Split into Orders Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Extract orders\n",
    "orders_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"customer_name\"),\n",
    "        col(\"location\"),\n",
    "        col(\"product_id\"),\n",
    "        col(\"order_status\"),\n",
    "        col(\"payment_method\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"discount_pct\"),\n",
    "        col(\"total_amount\"),\n",
    "        col(\"order_timestamp\"),\n",
    "        col(\"event_time\"),\n",
    "        current_timestamp().alias(\"bronze_timestamp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to Bronze Orders\n",
    "orders_query = (orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_bronze_orders)\n",
    "    .trigger(processingTime='5 seconds')\n",
    "    .start(bronze_orders_path)\n",
    ")\n",
    "\n",
    "print(\"✓ Orders stream started\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Split into Products Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Extract products (deduplicated)\n",
    "products_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"base_price\"),\n",
    "        col(\"unit_price\"),\n",
    "        current_timestamp().alias(\"bronze_timestamp\")\n",
    "    )\n",
    "    .dropDuplicates([\"product_id\"])\n",
    ")\n",
    "\n",
    "# Write to Bronze Products\n",
    "products_query = (products_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_bronze_products)\n",
    "    .trigger(processingTime='5 seconds')\n",
    "    .start(bronze_orders_path)\n",
    ")\n",
    "\n",
    "print(\"✓ Products stream started\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Progress\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\nMonitoring Bronze tables (60 seconds)...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(12):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        orders_count = spark.read.format(\"delta\").load(bronze_orders_path).count()\n",
    "    except:\n",
    "        orders_count = 0\n",
    "    \n",
    "    try:\n",
    "        products_count = spark.read.format(\"delta\").load(bronze_products_path).count()\n",
    "    except:\n",
    "        products_count = 0\n",
    "    \n",
    "    print(f\"[{i*5:3d}s] Orders: {orders_count:4d} | Products: {products_count:3d}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Display Bronze Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== BRONZE ORDERS ===\")\n",
    "display(spark.read.format(\"delta\").load(bronze_orders_path).orderBy(desc(\"bronze_timestamp\")).limit(20))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== BRONZE PRODUCTS ===\")\n",
    "display(spark.read.format(\"delta\").load(bronze_products_path).orderBy(\"product_id\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Summary\n",
    "orders_final = spark.read.format(\"delta\").load(bronze_orders_path).count()\n",
    "products_final = spark.read.format(\"delta\").load(bronze_products_path).count()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BRONZE LAYER COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Orders:   {orders_final:4d} records\")\n",
    "print(f\"Products: {products_final:3d} records\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n✓ Using Kafka connector - no compatibility issues!\")\n",
    "print(\"Next: Run 03_Silver_Layer.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d0b9d0c-1153-455a-8a3c-c044ae49f94e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "eventhub_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eventhub_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "#shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(\n",
    "        scope=keyvault_scope,\n",
    "        key=secret_name\n",
    "    )\n",
    "    print(\"✓ Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "shared_access_key=secret_value\n",
    "shared_access_key_name = \"RootManageSharedAccessKey\"\n",
    "\n",
    "# Build connection string\n",
    "eventhub_connection_string = f\"Endpoint=sb://{eventhub_namespace}.servicebus.windows.net/;SharedAccessKeyName={shared_access_key_name};SharedAccessKey={shared_access_key}\"\n",
    "\n",
    "# Kafka options dictionary\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eventhub_namespace}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": eventhub_name,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{eventhub_connection_string}\";',\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"60000\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"failOnDataLoss\": \"false\"\n",
    "}\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 3: Test Kafka Connection Directly\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"Testing Kafka connection...\")\n",
    "\n",
    "# Try to read just 10 messages\n",
    "test_df = (spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"endingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "message_count = test_df.count()\n",
    "print(f\"\\n✓ Successfully connected!\")\n",
    "print(f\"✓ Found {message_count} messages in Event Hub\")\n",
    "\n",
    "if message_count == 0:\n",
    "    print(\"\\n⚠️  WARNING: Event Hub is EMPTY!\")\n",
    "    print(\"   Run 01_Data_Generator.py to send data\")\n",
    "else:\n",
    "    print(\"\\n✓ Event Hub has data - streaming should work\")\n",
    "    print(\"\\nSample messages:\")\n",
    "    display(test_df.select(col(\"value\").cast(\"string\")).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 4: Verify Streaming Queries are Running\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check active streams\n",
    "active = spark.streams.active\n",
    "\n",
    "print(f\"Active Streams: {len(active)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for stream in active:\n",
    "    print(f\"Stream ID: {stream.id}\")\n",
    "    print(f\"  Status: {stream.status}\")\n",
    "    print(f\"  Recent Progress: {stream.lastProgress}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "if len(active) == 0:\n",
    "    print(\"\\n⚠️  No active streams!\")\n",
    "    print(\"   Run 02_Bronze_Layer.py to start streams\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 5: Check Checkpoint Locations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Checkpoint locations:\")\n",
    "print(f\"Orders:   {checkpoint_bronze_orders}\")\n",
    "print(f\"Products: {checkpoint_bronze_products}\")\n",
    "print(\"\")\n",
    "\n",
    "# Check if checkpoints exist\n",
    "try:\n",
    "    orders_files = dbutils.fs.ls(checkpoint_bronze_orders)\n",
    "    print(f\"✓ Orders checkpoint exists with {len(orders_files)} files\")\n",
    "except:\n",
    "    print(\"✗ Orders checkpoint doesn't exist yet (this is OK if first run)\")\n",
    "\n",
    "try:\n",
    "    products_files = dbutils.fs.ls(checkpoint_bronze_products)\n",
    "    print(f\"✓ Products checkpoint exists with {len(products_files)} files\")\n",
    "except:\n",
    "    print(\"✗ Products checkpoint doesn't exist yet (this is OK if first run)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 6: Try Reading with Different Starting Offset\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Testing with 'latest' offset (only NEW messages)...\")\n",
    "\n",
    "# Update kafka_options for this test\n",
    "test_options = kafka_options.copy()\n",
    "test_options[\"startingOffsets\"] = \"latest\"\n",
    "\n",
    "test_stream = (spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**test_options)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"✓ Stream created with 'latest' offset\")\n",
    "print(\"If your data was sent BEFORE the stream started, it won't appear\")\n",
    "print(\"Solution: Use 'earliest' offset (which is already in config)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 7: Manual Bronze Write Test\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Testing manual batch write to Bronze...\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Read in batch mode\n",
    "batch_df = (spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"endingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "count = batch_df.count()\n",
    "print(f\"Messages in Event Hub: {count}\")\n",
    "\n",
    "if count > 0:\n",
    "    # Parse one message\n",
    "    sample = batch_df.limit(1).select(col(\"value\").cast(\"string\")).collect()[0][0]\n",
    "    print(\"\\nSample message:\")\n",
    "    print(sample)\n",
    "    \n",
    "    # Try to write to Bronze\n",
    "    try:\n",
    "        test_path = \"/tmp/test_bronze_orders\"\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(test_path)\n",
    "        print(f\"\\n✓ Successfully wrote to Delta: {test_path}\")\n",
    "        print(\"The issue is likely with streaming, not with Event Hub or Delta\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error writing to Delta: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Solution: Clear Checkpoints and Restart\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"If streams are stuck, clear checkpoints and restart:\")\n",
    "print(\"\")\n",
    "print(\"# Stop all streams\")\n",
    "print(\"for stream in spark.streams.active:\")\n",
    "print(\"    stream.stop()\")\n",
    "print(\"\")\n",
    "print(\"# Clear checkpoints\")\n",
    "print(\"dbutils.fs.rm('/mnt/delta/checkpoints', True)\")\n",
    "print(\"\")\n",
    "print(\"# Re-run 02_Bronze_Layer.py\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Quick Fix Commands\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment and run if needed:\n",
    "\n",
    "# # Stop all streams\n",
    "# for stream in spark.streams.active:\n",
    "#     print(f\"Stopping: {stream.id}\")\n",
    "#     stream.stop()\n",
    "\n",
    "# # Clear checkpoints\n",
    "# dbutils.fs.rm(\"/mnt/delta/checkpoints\", True)\n",
    "# print(\"✓ Checkpoints cleared\")\n",
    "\n",
    "# # Clear Bronze tables\n",
    "# dbutils.fs.rm(\"/mnt/delta/bronze\", True)\n",
    "# print(\"✓ Bronze tables cleared\")\n",
    "\n",
    "# Now re-run 02_Bronze_Layer.py\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5639601691273116,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
