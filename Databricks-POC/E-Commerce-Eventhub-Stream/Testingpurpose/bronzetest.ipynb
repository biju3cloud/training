{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136c4280-d650-472e-9954-cd7ebd80f88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bdfb3b-f2e8-490b-8ba0-ace9f3b3663a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Event Hub Configuration\n",
    "\n",
    "eventhub_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eventhub_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(\n",
    "        scope=keyvault_scope,\n",
    "        key=secret_name\n",
    "    )\n",
    "    print(\"âœ“ Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "shared_access_key=secret_value\n",
    "\n",
    "# Build connection string\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eventhub_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={shared_access_key};\"\n",
    "    f\"EntityPath={eventhub_name}\"\n",
    ")\n",
    "\n",
    "# Kafka-style connection for Event Hub\n",
    "kafka_bootstrap_servers = f\"{eventhub_namespace}.servicebus.windows.net:9093\"\n",
    "kafka_topic = eventhub_name\n",
    "\n",
    "# SASL connection string for Kafka\n",
    "jaas_config = f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://{eventhub_namespace}.servicebus.windows.net/;SharedAccessKeyName={shared_access_key_name};SharedAccessKey={shared_access_key}\";'\n",
    "\n",
    "\n",
    "# Storage paths\n",
    "bronze_orders_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/bronze/orders\"\n",
    "silver_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol//silver/order_details\"\n",
    "gold_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol//gold/aggregations\"\n",
    "\n",
    "# Checkpoint locations\n",
    "checkpoint_bronze_orders = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/bronze_orders\"\n",
    "checkpoint_bronze_products = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/bronze_products\"\n",
    "checkpoint_silver = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/silver\"\n",
    "checkpoint_gold = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/gold\"\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"âœ“ Event Hub Namespace: {eventhub_namespace}\")\n",
    "print(f\"âœ“ Event Hub (Kafka Topic): {eventhub_name}\")\n",
    "print(f\"âœ“ Kafka Bootstrap: {kafka_bootstrap_servers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048b2c2a-91fc-45e0-a607-f4c50cb53c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d66651-2d7b-4e95-905e-06971abcd909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bbfc27e-668b-4e69-ad96-86a667a5c6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ðŸ¥‰ BRONZE Notebook: Streaming Ingestion (Event Hub to Delta using KAFKA)\n",
    "# MAGIC \n",
    "# MAGIC Ingests raw event data from Azure Event Hubs using the Kafka connector (`readStream.format(\"kafka\")`) and writes to the Bronze Delta table.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- Configuration Section ---\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-bijunew\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\" # NOTE: Not directly used in the Kafka config, but good to retain if needed\n",
    "\n",
    "# Table configuration\n",
    "catalog = \"na-dbxtraining\"\n",
    "bronze_schema = \"biju_bronze\"\n",
    "bronze_table_name = \"eventhubbronzeorderdata\"\n",
    "full_bronze_table_name = f\"`{catalog}`.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "# Checkpoint path (MANDATORY for Structured Streaming)\n",
    "checkpoint_path = f\"/Volumes/na-dbxtraining/biju_raw/biju_vol/streaming_checkpoints/{bronze_table_name}_kafka/\" \n",
    "\n",
    "# Get secret value\n",
    "secret_value = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "\n",
    "print(f\"Target Bronze table: {full_bronze_table_name}\")\n",
    "print(f\"Checkpoint location: {checkpoint_path}\")\n",
    "print(\"âœ“ Configuration Ready\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Read Stream from Event Hub using Kafka Connector\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Set Kafka options for Azure Event Hubs\n",
    "kafka_options = {\n",
    "    # Event Hubs uses the Kafka protocol on port 9093\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}:9093\",\n",
    "    # The Kafka topic is the Event Hub name\n",
    "    \"subscribe\": eh_name,\n",
    "    # Authentication settings required by Azure Event Hubs Kafka endpoint\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    # SASL PLAIN authentication uses \"$ConnectionString\" as the username and the primary key as the password\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{secret_value}\";',\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"60000\",\n",
    "    \"maxOffsetsPerTrigger\": \"10000\" # Throttle the input rate for micro-batch\n",
    "}\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# Read stream from Event Hub using Kafka connector\n",
    "df_bronze_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    # Use 'latest' to start reading only new events from when the stream starts\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Kafka connector output columns: \n",
    "# key (binary), value (binary), topic (string), partition (int), offset (long), timestamp (long), timestampType (int)\n",
    "df_bronze_stream.printSchema()\n",
    "\n",
    "# Apply minimal transformation (Add ingestion time and rename 'value' to 'body')\n",
    "df_bronze_stream = (\n",
    "    df_bronze_stream\n",
    "    .withColumn(\"ingestion_time\", current_timestamp())\n",
    "    # Rename 'value' (the payload) to 'body' for consistency with the previous structure\n",
    "    .withColumnRenamed(\"value\", \"body\")\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Write Stream to Bronze Delta Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Ensure the schema/catalog exist before writing\n",
    "#park.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog}.{bronze_schema}\")\n",
    "\n",
    "# Select columns to write. 'body' contains the raw JSON data.\n",
    "df_write = df_bronze_stream.select(\"body\", \"topic\", \"partition\", \"offset\", \"timestamp\", \"ingestion_time\")\n",
    "\n",
    "# Start the stream write\n",
    "query = (\n",
    "  df_write.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime='10 seconds') # Run a micro-batch every 10 seconds\n",
    "    .toTable(full_bronze_table_name)\n",
    ")\n",
    "\n",
    "print(f\"Starting Bronze stream (KAFKA)... (Query ID: {query.id})\")\n",
    "# NOTE: This cell will run indefinitely until manually stopped or the cluster shuts down.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Stream Verification (Optional)\n",
    "# MAGIC *Run this in a separate cell after the stream is started.*\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# # Example of how to stop the stream (run in a separate cell if needed)\n",
    "# # for stream in spark.streams.active:\n",
    "# #     if stream.name == query.name: # Replace 'query.name' if you didn't define it\n",
    "# #         print(f\"Stopping stream: {stream.id}\")\n",
    "# #         stream.stop()\n",
    "\n",
    "# # Display status of the stream\n",
    "# # display(spark.table(full_bronze_table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6141441d-8d76-4a06-95fd-f831a3db96f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists `na-dbxtraining`.biju_bronze.eventhubbronzeorderdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c79d46b-b3e4-4213-bef4-d5532c62023c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient\n",
    "from kafka.errors import KafkaConfigurationError\n",
    "\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(\n",
    "        scope=keyvault_scope,\n",
    "        key=secret_name\n",
    "    )\n",
    "    print(\"âœ“ Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "shared_access_key=secret_value\n",
    "\n",
    "# Build connection string\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eventhub_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={shared_access_key};\"\n",
    "    f\"EntityPath={eventhub_name}\"\n",
    ")\n",
    "\n",
    "# Kafka-style connection for Event Hub\n",
    "kafka_bootstrap_servers = f\"{eventhub_namespace}.servicebus.windows.net:9093\"\n",
    "kafka_topic = eventhub_name\n",
    "\n",
    "# SASL connection string for Kafka\n",
    "jaas_config = f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://{eventhub_namespace}.servicebus.windows.net/;SharedAccessKeyName={shared_access_key_name};SharedAccessKey={shared_access_key}\";'\n",
    "# Replicate your SASL configuration\n",
    "sasl_username = \"$jaas_config\"\n",
    "# Ensure secret_value is the actual key obtained from dbutils.secrets.get(...)\n",
    "\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "bootstrap_servers=f\"{eh_namespace}:9093\",\n",
    "security_protocol=\"SASL_SSL\",\n",
    "sasl_mechanism=\"PLAIN\",\n",
    "sasl_plain_username=sasl_username,\n",
    "sasl_plain_password=secret_value,\n",
    "request_timeout_ms=60000,\n",
    "api_version=(0, 10, 2) # Use a slightly older API version for compatibility if needed\n",
    ")\n",
    "    # Attempt to list topics to confirm successful connection and authentication\n",
    "topics = admin_client.list_topics()\n",
    "print(\"âœ… Connection Successful! Topics found:\", topics)\n",
    "admin_client.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ae8844-8440-446c-a0db-267736213a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve secret from Key Vault\n",
    "secret_value = dbutils.secrets.get(\n",
    "    scope=keyvault_scope,\n",
    "    key=secret_name\n",
    ")\n",
    "\n",
    "# Kafka connection options for Azure Event Hubs\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eventhub_namespace}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": eventhub_name,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": (\n",
    "        f'org.apache.kafka.common.security.plain.PlainLoginModule required '\n",
    "        f'username=\"$ConnectionString\" '\n",
    "        f'password=\"Endpoint=sb://{eventhub_namespace}.servicebus.windows.net/;'\n",
    "        f'SharedAccessKeyName={shared_access_key_name};'\n",
    "        f'SharedAccessKey={secret_value}\";'\n",
    "    ),\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"60000\"\n",
    "}\n",
    "\n",
    "# Read stream from Event Hub using Kafka connector\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a35eef6-a211-46ef-839a-46aa32c157eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ðŸ¥‰ BRONZE Notebook: Streaming Ingestion (Event Hub to Delta using KAFKA)\n",
    "# MAGIC \n",
    "# MAGIC Ingests raw event data from Azure Event Hubs using the Kafka connector and writes to the Bronze Delta table.\n",
    "# MAGIC \n",
    "# MAGIC **NOTE:** This version uses the Shared Access Policy (Key Name + Key) components in the SASL password.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- Configuration Section ---\n",
    "# Use the naming conventions from your previous notebooks for consistency\n",
    "eh_namespace = \"evhns-natraining\" # Just the namespace name\n",
    "eh_name = \"evh-natraining-bijunew\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\" # The name of your SAS Policy\n",
    "\n",
    "# Table configuration\n",
    "catalog = \"na-dbxtraining\"\n",
    "bronze_schema = \"biju_bronze\"\n",
    "bronze_table_name = \"eventhubbronzeorderdata\"\n",
    "full_bronze_table_name = f\"`{catalog}`.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "# Checkpoint path (MANDATORY for Structured Streaming)\n",
    "checkpoint_path = f\"/Volumes/na-dbxtraining/biju_raw/biju_vol/streaming_checkpoints/{bronze_table_name}_sas/\" \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Prepare Credentials and Kafka Options\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Retrieve secret (the Shared Access Key value) from Key Vault\n",
    "secret_value = dbutils.secrets.get(\n",
    "    scope=keyvault_scope,\n",
    "    key=secret_name\n",
    ")\n",
    "\n",
    "# Construct the SASL password string: KeyName + KeyValue\n",
    "# Format: <SharedAccessKeyName>:<SharedAccessKey>\n",
    "sasl_password = f\"{shared_access_key_name}:{secret_value}\"\n",
    "\n",
    "\n",
    "# Kafka connection options for Azure Event Hubs\n",
    "kafka_options = {\n",
    "    # Broker address\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}.servicebus.windows.net:9093\",\n",
    "    # Topic to subscribe to (Event Hub name)\n",
    "    \"subscribe\": eh_name,\n",
    "    # Authentication settings\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": (\n",
    "        f'org.apache.kafka.common.security.plain.PlainLoginModule required '\n",
    "        # Use the literal username identifier\n",
    "        f'username=\"$ConnectionString\" '\n",
    "        # Use the constructed SASL password (KeyName:KeyValue)\n",
    "        f'password=\"{sasl_password}\";' \n",
    "    ),\n",
    "    # Connection timeout settings\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"60000\",\n",
    "    \"maxOffsetsPerTrigger\": \"10000\"\n",
    "}\n",
    "\n",
    "print(f\"Target Bronze table: {full_bronze_table_name}\")\n",
    "print(f\"Checkpoint location: {checkpoint_path}\")\n",
    "print(\"âœ“ Kafka Options Configured using Shared Access Policy\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Read Stream from Event Hub and Transform\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# Read stream from Event Hub using Kafka connector\n",
    "df_bronze_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    # Start reading from the latest available events\n",
    "    .option(\"startingOffsets\", \"latest\") \n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Transform the stream for Bronze layer storage\n",
    "df_bronze_stream = (\n",
    "    df_bronze_stream\n",
    "    # Rename 'value' (the payload) to 'body' for clarity\n",
    "    .withColumnRenamed(\"value\", \"body\") \n",
    "    # Add pipeline metadata: ingestion timestamp\n",
    "    .withColumn(\"ingestion_time\", current_timestamp())\n",
    "    # Cast the binary 'body' value to string \n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\")) \n",
    ")\n",
    "\n",
    "print(\"Bronze Stream Schema:\")\n",
    "df_bronze_stream.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Write Stream to Bronze Delta Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Ensure the catalog and schema exist\n",
    "#spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{bronze_schema}\")\n",
    "\n",
    "# Select the required columns for the Bronze table\n",
    "df_write = df_bronze_stream.select(\n",
    "    \"body\", # The raw data payload\n",
    "    \"topic\", \n",
    "    \"partition\", \n",
    "    \"offset\", \n",
    "    \"timestamp\", # The event enqueue time\n",
    "    \"ingestion_time\" # The processing time\n",
    ")\n",
    "\n",
    "# Start the stream write\n",
    "query = (\n",
    "  df_write.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime='10 seconds') # Define micro-batch interval\n",
    "    .toTable(full_bronze_table_name)\n",
    ")\n",
    "\n",
    "print(f\"Starting Bronze stream... (Query ID: {query.id})\")\n",
    "# NOTE: This cell will run indefinitely until manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d75a91-890d-43ac-b637-f566e6ec70cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Use the same variables as in your config cell\n",
    "namespace = \"evhns-natraining\"\n",
    "eventhub_name = \"evh-natraining-bijunew\"\n",
    "shared_access_key_name = \"bijuaccesspolicy\"\n",
    "shared_access_key = \"0wRvFTfbTJz1daTi6zgmAOBs8sTd9qEAQ+AEhPjdO/I=\"\n",
    "\n",
    "# SASL password should be <KeyName>:<Key>\n",
    "sasl_password = f\"{shared_access_key_name}:{shared_access_key}\"\n",
    "\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": f\"{namespace}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": eventhub_name,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": (\n",
    "        f'org.apache.kafka.common.security.plain.PlainLoginModule required '\n",
    "        f'username=\"$ConnectionString\" password=\"{sasl_password}\";'\n",
    "    ),\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.session.timeout.ms\": \"60000\"\n",
    "}\n",
    "\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df_parsed = df.withColumn(\"body\", col(\"value\").cast(\"string\"))\n",
    "display(df_parsed)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5639601691273214,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronzetest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
