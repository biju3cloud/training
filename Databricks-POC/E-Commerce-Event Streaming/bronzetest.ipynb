{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136c4280-d650-472e-9954-cd7ebd80f88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d66651-2d7b-4e95-905e-06971abcd909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bbfc27e-668b-4e69-ad96-86a667a5c6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ü•â BRONZE Notebook: Event Hub to Raw Delta Table\n",
    "# MAGIC \n",
    "# MAGIC Ingests raw event data from Azure Event Hubs directly into the Bronze Delta table.\n",
    "# MAGIC \n",
    "# MAGIC **Goal:** Minimal transformation, retain raw structure for historical replay.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- Configuration Section ---\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-bijunew\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "# Table configuration\n",
    "catalog = \"na-dbxtraining\"\n",
    "bronze_schema = \"biju_bronze\"\n",
    "bronze_table_name = \"eventhubbronzeorderdata\"\n",
    "full_bronze_table_name = f\"`{catalog}`.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "# Get connection string\n",
    "secret_value = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eh_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={secret_value};\"\n",
    "    f\"EntityPath={eh_name}\"\n",
    ")\n",
    "\n",
    "print(f\"Target Bronze table: {full_bronze_table_name}\")\n",
    "print(\"‚úì Configuration Ready\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Data Generation (For Demo/Testing)\n",
    "# MAGIC *In a production environment, this would be an external process.*\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def send_test_data(count=50):\n",
    "    \"\"\"Send test events to Event Hubs\"\"\"\n",
    "    print(f\"Sending {count} test events...\")\n",
    "    \n",
    "    producer = EventHubProducerClient.from_connection_string(connection_string)\n",
    "    batch = producer.create_batch()\n",
    "    \n",
    "    for i in range(count):\n",
    "        # The raw data payload\n",
    "        event = {\n",
    "            \"id\": f\"test_{datetime.now().strftime('%H%M%S%f')}_{i}\",\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"message\": f\"Test message {i}\",\n",
    "            \"sensor\": [\"temperature\", \"humidity\", \"pressure\"][i % 3],\n",
    "            \"value\": 20.0 + i % 10,\n",
    "            \"unit\": [\"celsius\", \"percent\", \"hPa\"][i % 3]\n",
    "        }\n",
    "        \n",
    "        # Add to batch\n",
    "        try:\n",
    "            batch.add(EventData(json.dumps(event)))\n",
    "        except ValueError: # Batch is full, send and start a new one\n",
    "            producer.send_batch(batch)\n",
    "            batch = producer.create_batch()\n",
    "            batch.add(EventData(json.dumps(event)))\n",
    "    \n",
    "    producer.send_batch(batch)\n",
    "    producer.close()\n",
    "    \n",
    "    print(f\"‚úì Sent {count} events. Waiting for availability...\")\n",
    "    time.sleep(3)\n",
    "\n",
    "# Send test data for the pipeline run\n",
    "#send_test_data(50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Read Events from Event Hub (Simulated Micro-batch)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "events = []\n",
    "stop_flag = threading.Event()\n",
    "\n",
    "def collect_events(max_events=1000):\n",
    "    \"\"\"Collect events with thread-based timeout\"\"\"\n",
    "    from azure.eventhub import EventHubConsumerClient # Import locally for thread\n",
    "    \n",
    "    def on_event(partition_context, event):\n",
    "        if stop_flag.is_set() or len(events) >= max_events:\n",
    "            return\n",
    "        \n",
    "        if event:\n",
    "            events.append({\n",
    "                # Raw data from Event Hub\n",
    "                'body': event.body_as_str(),\n",
    "                'event_time': event.enqueued_time.isoformat() if event.enqueued_time else None,\n",
    "                'offset': event.offset,\n",
    "                'sequence_number': event.sequence_number,\n",
    "                'partition_id': partition_context.partition_id\n",
    "            })\n",
    "            \n",
    "            if len(events) % 10 == 0:\n",
    "                print(f\"  Collected {len(events)} events...\")\n",
    "    \n",
    "    # NOTE: Using consumer group $Default and starting_position=\"-1\" to read recent events\n",
    "    client = EventHubConsumerClient.from_connection_string(\n",
    "        conn_str=connection_string,\n",
    "        consumer_group=\"$Default\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with client:\n",
    "            client.receive(\n",
    "                on_event=on_event,\n",
    "                starting_position=\"-1\", # Read from the latest events\n",
    "                max_wait_time=2,\n",
    "                # NOTE: Only read from one partition for this simple example\n",
    "                # If you need to read from ALL partitions, remove the partition_id argument below\n",
    "                partition_id='0' \n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during event collection: {str(e)}\")\n",
    "        pass\n",
    "\n",
    "# Start collection\n",
    "print(\"Reading events for max 10 seconds...\")\n",
    "thread = threading.Thread(target=lambda: collect_events(max_events=1000))\n",
    "thread.daemon = True\n",
    "thread.start()\n",
    "\n",
    "# Wait 5 seconds\n",
    "time.sleep(5) \n",
    "\n",
    "# Stop collection\n",
    "stop_flag.set()\n",
    "thread.join(timeout=2)\n",
    "\n",
    "print(f\"\\n‚úì Collected {len(events)} events\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Write to Bronze Delta Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "if events:\n",
    "    print(f\"Creating DataFrame from {len(events)} events...\")\n",
    "    \n",
    "    # 1. Create DataFrame from raw events\n",
    "    df = spark.createDataFrame(events)\n",
    "    \n",
    "    # 2. Add pipeline metadata\n",
    "    df = df.withColumn(\"ingestion_time\", current_timestamp())\n",
    "    \n",
    "    # 3. Write directly to Delta table (append mode)\n",
    "    print(f\"\\nWriting {df.count()} rows to Bronze table: {full_bronze_table_name}\")\n",
    "    \n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(full_bronze_table_name)\n",
    "    \n",
    "    print(f\"‚úì Saved {len(events)} events to table {full_bronze_table_name}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No events collected. Skipping write to Bronze.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Verification\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(full_bronze_table_name)\n",
    "    total = df_bronze.count()\n",
    "    print(f\"Total records in Bronze: {total}\")\n",
    "    display(df_bronze.orderBy(\"ingestion_time\", ascending=False).limit(5))\n",
    "except Exception as e:\n",
    "    print(f\"Error reading table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6141441d-8d76-4a06-95fd-f831a3db96f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from  `na-dbxtraining`.biju_bronze.eventhubbronzeorderdata"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5639601691273214,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronzetest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
