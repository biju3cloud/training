{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9b83fd-d5fd-4ce1-a7fa-3e7eff75e177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Event Hub Configuration\n",
    "\n",
    "eventhub_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eventhub_name = \"evh-natraining-bijunew\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(\n",
    "        scope=keyvault_scope,\n",
    "        key=secret_name\n",
    "    )\n",
    "    print(\"✓ Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "try:\n",
    "    secret_value = dbutils.secrets.get(\n",
    "        scope=keyvault_scope,\n",
    "        key=secret_name\n",
    "    )\n",
    "    print(\"✓ Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "shared_access_key=secret_value\n",
    "\n",
    "\n",
    "# Build connection string\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eventhub_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={shared_access_key};\"\n",
    "    f\"EntityPath={eventhub_name}\"\n",
    ")\n",
    "\n",
    "# Event Hub configuration for Spark\n",
    "eh_conf = {\n",
    "    'eventhubs.connectionString': connection_string,\n",
    "    'eventhubs.consumerGroup': '$Default'\n",
    "}\n",
    "\n",
    "# Storage paths\n",
    "bronze_orders_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/bronze/orders\"\n",
    "silver_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol//silver/order_details\"\n",
    "gold_path = \"/Volumes/na-dbxtraining/biju_raw/biju_vol//gold/aggregations\"\n",
    "\n",
    "# Checkpoint locations\n",
    "checkpoint_bronze_orders = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/bronze_orders\"\n",
    "checkpoint_bronze_products = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/bronze_products\"\n",
    "checkpoint_silver = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/silver\"\n",
    "checkpoint_gold = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/mnt/delta/checkpoints/gold\"\n",
    "\n",
    "print(\"✓ Event Hub configuration loaded\")\n",
    "print(f\"✓ Event Hub: {eventhub_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467679aa-4aac-434c-b15e-17fd211dd7bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b9ee14-fc3c-4d81-9520-502d42e54646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Event Hub Configuration\n",
    "eventhub_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eventhub_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "try:\n",
    "    # The actual Shared Access Key value is retrieved and stored in 'secret_value'\n",
    "    secret_value = dbutils.secrets.get(\n",
    "        scope=keyvault_scope,\n",
    "        key=secret_name\n",
    "    )\n",
    "    print(\"✓ Successfully retrieved secret from Key Vault\")\n",
    "    print(f\"  - Secret name: {secret_name}\")\n",
    "    print(f\"  - Scope: {keyvault_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error retrieving secret: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Build connection string\n",
    "# FIX: Replaced 'shared_access_key' with the actual secret value variable: 'secret_value'\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eventhub_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={secret_value};\" # <--- FIXED HERE!\n",
    "    f\"EntityPath={eventhub_name}\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Generated Connection String ---\")\n",
    "# Use 'dbutils.secrets.mask()' to safely print the connection string without exposing the key\n",
    "print(connection_string.replace(secret_value, '***'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9ad48a-3b54-4fb5-9ddf-14cd0ff831a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "import random\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Product catalog\n",
    "products = [\n",
    "    {\"product_id\": \"P001\", \"name\": \"Laptop\", \"category\": \"Electronics\", \"brand\": \"Dell\", \"base_price\": 1200},\n",
    "    {\"product_id\": \"P002\", \"name\": \"Smartphone\", \"category\": \"Electronics\", \"brand\": \"Samsung\", \"base_price\": 800},\n",
    "    {\"product_id\": \"P003\", \"name\": \"Headphones\", \"category\": \"Accessories\", \"brand\": \"Sony\", \"base_price\": 150},\n",
    "    {\"product_id\": \"P004\", \"name\": \"Keyboard\", \"category\": \"Accessories\", \"brand\": \"Logitech\", \"base_price\": 80},\n",
    "    {\"product_id\": \"P005\", \"name\": \"Monitor\", \"category\": \"Electronics\", \"brand\": \"LG\", \"base_price\": 400},\n",
    "    {\"product_id\": \"P006\", \"name\": \"Mouse\", \"category\": \"Accessories\", \"brand\": \"Logitech\", \"base_price\": 50},\n",
    "    {\"product_id\": \"P007\", \"name\": \"Tablet\", \"category\": \"Electronics\", \"brand\": \"Apple\", \"base_price\": 600},\n",
    "    {\"product_id\": \"P008\", \"name\": \"Webcam\", \"category\": \"Accessories\", \"brand\": \"Logitech\", \"base_price\": 100},\n",
    "    {\"product_id\": \"P009\", \"name\": \"Smartwatch\", \"category\": \"Electronics\", \"brand\": \"Apple\", \"base_price\": 350},\n",
    "    {\"product_id\": \"P010\", \"name\": \"USB Cable\", \"category\": \"Accessories\", \"brand\": \"Generic\", \"base_price\": 15}\n",
    "]\n",
    "\n",
    "customer_names = [\"John Smith\", \"Jane Doe\", \"Bob Johnson\", \"Alice Williams\", \"Charlie Brown\", \n",
    "                  \"Diana Davis\", \"Eve Martinez\", \"Frank Wilson\", \"Grace Lee\", \"Henry Taylor\"]\n",
    "\n",
    "locations = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \n",
    "             \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\"]\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def generate_enriched_order(order_id):\n",
    "    \"\"\"Generate enriched order event with product details embedded\"\"\"\n",
    "    \n",
    "    customer_id = f\"C{random.randint(1000, 1050):04d}\"\n",
    "    product = random.choice(products)\n",
    "    quantity = random.randint(1, 5)\n",
    "    discount = random.choice([0, 0.05, 0.10, 0.15])\n",
    "    price = product[\"base_price\"] * (1 - discount)\n",
    "    \n",
    "    event = {\n",
    "        # Order Information\n",
    "        \"order_id\": f\"ORD{order_id:06d}\",\n",
    "        \"customer_id\": customer_id,\n",
    "        \"customer_name\": random.choice(customer_names),\n",
    "        \"location\": random.choice(locations),\n",
    "        \"order_status\": random.choice([\"pending\", \"confirmed\", \"shipped\"]),\n",
    "        \"payment_method\": random.choice([\"credit_card\", \"debit_card\", \"paypal\"]),\n",
    "        \"quantity\": quantity,\n",
    "        \"discount_pct\": discount,\n",
    "        \"total_amount\": round(price * quantity, 2),\n",
    "        \"order_timestamp\": datetime.utcnow().isoformat(),\n",
    "        \n",
    "        # Product Information (embedded)\n",
    "        \"product_id\": product[\"product_id\"],\n",
    "        \"product_name\": product[\"name\"],\n",
    "        \"category\": product[\"category\"],\n",
    "        \"brand\": product[\"brand\"],\n",
    "        \"base_price\": product[\"base_price\"],\n",
    "        \"unit_price\": round(price, 2)\n",
    "    }\n",
    "    \n",
    "    return event\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def send_events():\n",
    "    \"\"\"Send enriched order events to Event Hub\"\"\"\n",
    "    \n",
    "    producer = EventHubProducerClient.from_connection_string(\n",
    "        conn_str=connection_string.replace(f\";EntityPath={eventhub_name}\", \"\"),\n",
    "        eventhub_name=eventhub_name\n",
    "    )\n",
    "    \n",
    "    print(\"Starting event generation for 60 seconds...\")\n",
    "    print(\"Each event contains BOTH order and product information\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    order_count = 0\n",
    "    \n",
    "    try:\n",
    "        while time.time() - start_time < 60:\n",
    "            # Send 5 orders per second\n",
    "            batch = producer.create_batch()\n",
    "            for i in range(5):\n",
    "                event = generate_enriched_order(order_count)\n",
    "                batch.add(EventData(json.dumps(event)))\n",
    "                order_count += 1\n",
    "            \n",
    "            producer.send_batch(batch)\n",
    "            \n",
    "            # Progress update\n",
    "            if order_count % 50 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"[{elapsed:.0f}s] Generated: {order_count} enriched orders\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    finally:\n",
    "        producer.close()\n",
    "    \n",
    "    print(f\"\\n✓ Generation complete!\")\n",
    "    print(f\"✓ Total enriched orders: {order_count}\")\n",
    "    print(f\"✓ Each order contains order details + product details\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Run the generator\n",
    "send_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c98143aa-35fc-4681-b222-71068aae2ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Check if 01_Data_Generator completed successfully\n",
    "# You should see output like: \"✓ Total orders sent: 300\"\n",
    "print(\"Did you run 01_Data_Generator.py and see '✓ Total orders sent: 300'?\")\n",
    "print(\"If NO, run 01_Data_Generator.py first!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 2: Verify Event Hub Has Data (Azure Portal)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Check Azure Portal:\")\n",
    "print(\"1. Go to Azure Portal → Your Event Hub Namespace\")\n",
    "print(\"2. Click on 'orders-events' Event Hub\")\n",
    "print(\"3. Go to 'Overview' or 'Metrics'\")\n",
    "print(\"4. Look at 'Incoming Messages' metric\")\n",
    "print(\"\")\n",
    "print(\"You should see ~300 incoming messages in the last hour\")\n",
    "print(\"If you see 0 messages, the data generator didn't send data properly\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 3: Test Kafka Connection Directly\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"Testing Kafka connection...\")\n",
    "\n",
    "# Try to read just 10 messages\n",
    "test_df = (spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"endingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "message_count = test_df.count()\n",
    "print(f\"\\n✓ Successfully connected!\")\n",
    "print(f\"✓ Found {message_count} messages in Event Hub\")\n",
    "\n",
    "if message_count == 0:\n",
    "    print(\"\\n⚠️  WARNING: Event Hub is EMPTY!\")\n",
    "    print(\"   Run 01_Data_Generator.py to send data\")\n",
    "else:\n",
    "    print(\"\\n✓ Event Hub has data - streaming should work\")\n",
    "    print(\"\\nSample messages:\")\n",
    "    display(test_df.select(col(\"value\").cast(\"string\")).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 4: Verify Streaming Queries are Running\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check active streams\n",
    "active = spark.streams.active\n",
    "\n",
    "print(f\"Active Streams: {len(active)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for stream in active:\n",
    "    print(f\"Stream ID: {stream.id}\")\n",
    "    print(f\"  Status: {stream.status}\")\n",
    "    print(f\"  Recent Progress: {stream.lastProgress}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "if len(active) == 0:\n",
    "    print(\"\\n⚠️  No active streams!\")\n",
    "    print(\"   Run 02_Bronze_Layer.py to start streams\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 5: Check Checkpoint Locations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Checkpoint locations:\")\n",
    "print(f\"Orders:   {checkpoint_bronze_orders}\")\n",
    "print(f\"Products: {checkpoint_bronze_products}\")\n",
    "print(\"\")\n",
    "\n",
    "# Check if checkpoints exist\n",
    "try:\n",
    "    orders_files = dbutils.fs.ls(checkpoint_bronze_orders)\n",
    "    print(f\"✓ Orders checkpoint exists with {len(orders_files)} files\")\n",
    "except:\n",
    "    print(\"✗ Orders checkpoint doesn't exist yet (this is OK if first run)\")\n",
    "\n",
    "try:\n",
    "    products_files = dbutils.fs.ls(checkpoint_bronze_products)\n",
    "    print(f\"✓ Products checkpoint exists with {len(products_files)} files\")\n",
    "except:\n",
    "    print(\"✗ Products checkpoint doesn't exist yet (this is OK if first run)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 6: Try Reading with Different Starting Offset\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Testing with 'latest' offset (only NEW messages)...\")\n",
    "\n",
    "# Update kafka_options for this test\n",
    "test_options = kafka_options.copy()\n",
    "test_options[\"startingOffsets\"] = \"latest\"\n",
    "\n",
    "test_stream = (spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**test_options)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"✓ Stream created with 'latest' offset\")\n",
    "print(\"If your data was sent BEFORE the stream started, it won't appear\")\n",
    "print(\"Solution: Use 'earliest' offset (which is already in config)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Check 7: Manual Bronze Write Test\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Testing manual batch write to Bronze...\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Read in batch mode\n",
    "batch_df = (spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_options)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"endingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "count = batch_df.count()\n",
    "print(f\"Messages in Event Hub: {count}\")\n",
    "\n",
    "if count > 0:\n",
    "    # Parse one message\n",
    "    sample = batch_df.limit(1).select(col(\"value\").cast(\"string\")).collect()[0][0]\n",
    "    print(\"\\nSample message:\")\n",
    "    print(sample)\n",
    "    \n",
    "    # Try to write to Bronze\n",
    "    try:\n",
    "        test_path = \"/tmp/test_bronze_orders\"\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(test_path)\n",
    "        print(f\"\\n✓ Successfully wrote to Delta: {test_path}\")\n",
    "        print(\"The issue is likely with streaming, not with Event Hub or Delta\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error writing to Delta: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Solution: Clear Checkpoints and Restart\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Quick Fix Commands\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment and run if needed:\n",
    "\n",
    "# # Stop all streams\n",
    "# for stream in spark.streams.active:\n",
    "#     print(f\"Stopping: {stream.id}\")\n",
    "#     stream.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01DataGenerator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
