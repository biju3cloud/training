{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3243d18c-677c-4dee-abe6-b70d6a8bc4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f87a72f-e55e-4542-9b83-259b53465cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeebcc6-d4f9-4df7-99ce-5f34119177a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer - Direct Table Creation\n",
    "# MAGIC Reads enriched events from Event Hub and splits into Orders & Products tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import (\n",
    "    StringType, LongType, DoubleType, IntegerType, TimestampType, StructType, StructField\n",
    ")\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Read Events from Event Hub\n",
    "\n",
    "# COMMAND ----------\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),         # or LongType if needed\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True),   # or TimestampType if parsed\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"kafka_offset\", StringType(), True),\n",
    "    StructField(\"partition_id\", StringType(), True)\n",
    "])\n",
    "events = []\n",
    "stop_flag = threading.Event()\n",
    "\n",
    "def collect_events(max_events=1000):\n",
    "    \"\"\"Collect events from Event Hub with thread-based timeout\"\"\"\n",
    "    \n",
    "    def on_event(partition_context, event):\n",
    "        if stop_flag.is_set() or len(events) >= max_events:\n",
    "            return\n",
    "        \n",
    "        if event:\n",
    "            events.append({\n",
    "                'body': event.body_as_str(),\n",
    "                'event_time': event.enqueued_time.isoformat() if event.enqueued_time else None,\n",
    "                'offset': event.offset,\n",
    "                'sequence_number': event.sequence_number,\n",
    "                'partition_id': partition_context.partition_id\n",
    "            })\n",
    "            \n",
    "            if len(events) % 50 == 0:\n",
    "                print(f\"  Collected {len(events)} events...\")\n",
    "    \n",
    "    client = EventHubConsumerClient.from_connection_string(\n",
    "        conn_str=connection_string,\n",
    "        consumer_group=\"$Default\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with client:\n",
    "            client.receive(\n",
    "                on_event=on_event,\n",
    "                starting_position=\"-1\",  # Read from beginning\n",
    "                max_wait_time=3\n",
    "            )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Start collection\n",
    "print(\"=\"*70)\n",
    "print(\"READING EVENTS FROM EVENT HUB\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Event Hub: {eh_name}\")\n",
    "print(f\"Reading all available events...\\n\")\n",
    "\n",
    "thread = threading.Thread(target=lambda: collect_events(max_events=1000))\n",
    "thread.daemon = True\n",
    "thread.start()\n",
    "\n",
    "# Wait for events (15 seconds max)\n",
    "for i in range(15):\n",
    "    time.sleep(1)\n",
    "    if len(events) >= 300:  # Got all expected events\n",
    "        print(f\"  Reached target: {len(events)} events\")\n",
    "        break\n",
    "    if len(events) > 0 and i >= 5:  # Got some events, waited 5 seconds\n",
    "        break\n",
    "\n",
    "stop_flag.set()\n",
    "thread.join(timeout=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"✓ Collected {len(events)} events from Event Hub\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Parse Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if len(events) == 0:\n",
    "    print(\"\\n⚠️  NO EVENTS COLLECTED!\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Data Generator (01_Data_Generator) hasn't run yet\")\n",
    "    print(\"  2. Event Hub is empty\")\n",
    "    print(\"  3. Connection issue\")\n",
    "    print(\"\\nAction: Run 01_Data_Generator first, then re-run this notebook\")\n",
    "    dbutils.notebook.exit(\"No events to process\")\n",
    "\n",
    "print(f\"\\nParsing {len(events)} events...\")\n",
    "\n",
    "# Parse JSON bodies\n",
    "parsed_events = []\n",
    "parse_errors = 0\n",
    "\n",
    "for evt in events:\n",
    "    try:\n",
    "        data = json.loads(evt['body'])\n",
    "        data['event_time'] = evt['event_time']\n",
    "        data['kafka_offset'] = evt['offset']\n",
    "        data['partition_id'] = evt['partition_id']\n",
    "        data['sequence_number'] = evt['sequence_number']\n",
    "        parsed_events.append(data)\n",
    "    except Exception as e:\n",
    "        parse_errors += 1\n",
    "        print(f\"  Error parsing event: {e}\")\n",
    "\n",
    "print(f\"✓ Successfully parsed {len(parsed_events)} events\")\n",
    "if parse_errors > 0:\n",
    "    print(f\"⚠️  Failed to parse {parse_errors} events\")\n",
    "\n",
    "# Show sample event\n",
    "if parsed_events:\n",
    "    print(\"\\nSample parsed event:\")\n",
    "    print(json.dumps(parsed_events[0], indent=2))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Create Orders DataFrame and Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING ORDERS TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract orders data\n",
    "orders_data = []\n",
    "for evt in parsed_events:\n",
    "    orders_data.append((\n",
    "        evt['order_id'],\n",
    "        evt['customer_id'],\n",
    "        evt['customer_name'],\n",
    "        evt['location'],\n",
    "        evt['product_id'],  # Foreign key to products\n",
    "        evt['order_status'],\n",
    "        evt['payment_method'],\n",
    "        int(evt['quantity']) if evt['quantity'] is not None else None,\n",
    "        float(evt['discount_pct']) if evt['discount_pct'] is not None else None,\n",
    "        float(evt['total_amount']) if evt['total_amount'] is not None else None,\n",
    "        evt['order_timestamp'],\n",
    "        evt.get('event_time'),\n",
    "        evt.get('kafka_offset'),\n",
    "        evt.get('partition_id')\n",
    "    ))\n",
    "\n",
    "# Create DataFrame\n",
    "orders_df = spark.createDataFrame(\n",
    "    orders_data,\n",
    "     schema=orders_schema\n",
    ")\n",
    "\n",
    "# Add metadata\n",
    "orders_df = orders_df.withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "\n",
    "print(f\"✓ Created orders DataFrame with {orders_df.count()} records\")\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nOrders DataFrame schema:\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "# Write to Unity Catalog table\n",
    "print(f\"\\nWriting to table: {bronze_orders_table}\")\n",
    "\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(bronze_orders_table)\n",
    "\n",
    "orders_count = orders_df.count()\n",
    "print(f\"✓ Saved {orders_count} orders to {bronze_orders_table}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Create Products DataFrame and Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING PRODUCTS TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract unique products (deduplicate by product_id)\n",
    "products_dict = {}\n",
    "for evt in parsed_events:\n",
    "    product_id = evt['product_id']\n",
    "    if product_id not in products_dict:\n",
    "        products_dict[product_id] = (\n",
    "            evt['product_id'],\n",
    "            evt['product_name'],\n",
    "            evt['category'],\n",
    "            evt['brand'],\n",
    "            float(evt['base_price']) if evt['base_price'] is not None else None,\n",
    "            float(evt.get('unit_price', evt['base_price'])) if evt.get('unit_price', evt['base_price']) is not None else None\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "products_data = list(products_dict.values())\n",
    "\n",
    "# Create DataFrame\n",
    "products_df = spark.createDataFrame(\n",
    "    products_data,\n",
    "    [\"product_id\", \"product_name\", \"category\", \"brand\", \"base_price\", \"unit_price\"]\n",
    ")\n",
    "\n",
    "# Add metadata\n",
    "products_df = products_df.withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "\n",
    "print(f\"✓ Created products DataFrame with {products_df.count()} unique products\")\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nProducts DataFrame schema:\")\n",
    "products_df.printSchema()\n",
    "\n",
    "# Write to Unity Catalog table\n",
    "print(f\"\\nWriting to table: {bronze_products_table}\")\n",
    "\n",
    "products_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(bronze_products_table)\n",
    "\n",
    "products_count = products_df.count()\n",
    "print(f\"✓ Saved {products_count} products to {bronze_products_table}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: Verify Bronze Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRONZE LAYER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Orders table\n",
    "orders_table_df = spark.table(bronze_orders_table)\n",
    "total_orders = orders_table_df.count()\n",
    "\n",
    "print(f\"\\nOrders Table: {bronze_orders_table}\")\n",
    "print(f\"  Records: {total_orders}\")\n",
    "print(f\"  New records this run: {orders_count}\")\n",
    "\n",
    "print(\"\\nSample orders (latest 10):\")\n",
    "display(orders_table_df.orderBy(desc(\"bronze_timestamp\")).limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Products table\n",
    "products_table_df = spark.table(bronze_products_table)\n",
    "total_products = products_table_df.count()\n",
    "\n",
    "print(f\"\\nProducts Table: {bronze_products_table}\")\n",
    "print(f\"  Records: {total_products}\")\n",
    "print(f\"  New records this run: {products_count}\")\n",
    "\n",
    "print(\"\\nAll products:\")\n",
    "display(products_table_df.orderBy(\"product_id\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Quick analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOrders by product:\")\n",
    "display(\n",
    "    orders_table_df\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\")\n",
    "    )\n",
    "    .orderBy(desc(\"order_count\"))\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ BRONZE LAYER COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Orders: {total_orders} records\")\n",
    "print(f\"Products: {total_products} records\")\n",
    "print(f\"\\nData split successfully:\")\n",
    "print(f\"  - Enriched events → Orders table (with product_id FK)\")\n",
    "print(f\"  - Unique products → Products table (with product_id PK)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext: Run 03_Silver_Layer to join Orders with Products\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
