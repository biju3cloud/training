{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c60131-d107-4aea-98d3-196b56615eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n",
    "\n",
    "# Restart Python so the newly installed package is available in this notebook\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abf8f68-8510-4aef-9a79-e134e4156107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19dfb19c-5b83-4d21-af8c-beab5f3e07be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Inputs (update as needed)\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"  # SAS key with Send/Listen\n",
    "shared_access_key_name = \"RootManageSharedAccessKey\"  # <-- FIXED: Use actual policy name\n",
    "\n",
    "# Fetch SAS key from secret scope\n",
    "sas_key = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "\n",
    "# Build connection string\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eh_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={sas_key};\"\n",
    "    f\"EntityPath={eh_name}\"\n",
    ")\n",
    "\n",
    "# Event Hubs config\n",
    "eh_conf = {\n",
    "    \"eventhubs.connectionString\": connection_string,\n",
    "}\n",
    "\n",
    "# Read stream\n",
    "raw_df = (\n",
    "    spark.readStream\n",
    "         .format(\"eventhubs\")\n",
    "         .options(**eh_conf)\n",
    "         .load()\n",
    ")\n",
    "\n",
    "# Body is binary; convert to string for inspection\n",
    "df = raw_df.withColumn(\"body\", col(\"body\").cast(\"string\"))\n",
    "\n",
    "display(df.select(\"enqueuedTime\", \"offset\", \"sequenceNumber\", \"body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ada900d-4bc1-4a7f-ba25-051eace93daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EVENTHUB_NAME = dbutils.widgets.get(\"eventhub_name\")\n",
    "\n",
    "EVENTHUB_CONN_STR = (\n",
    "    \"Endpoint=sb://evhns-natraining.servicebus.windows.net/;\"\n",
    "    \"SharedAccessKeyName=SharedAccessKeyToSendAndListen;\"\n",
    "    f\"SharedAccessKey={secret_value};\"\n",
    "    f\"EntityPath={EVENTHUB_NAME}\"\n",
    ").strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "\n",
    "encrypted_conn_str = spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(EVENTHUB_CONN_STR)\n",
    "\n",
    "event_hubs_conf = {\n",
    "    \"eventhubs.connectionString\": encrypted_conn_str,\n",
    "    \"eventhubs.consumerGroup\": \"$Default\",\n",
    "    \"eventhubs.startingPosition\": \"\"\"{\n",
    "      \"offset\":\"-1\",\n",
    "      \"seqNo\":-1,\n",
    "      \"enqueuedTime\":\"1970-01-01T00:00:00.000Z\",\n",
    "      \"isInclusive\":false\n",
    "    }\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Event Hub read configuration ready:\", EVENTHUB_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70bfcb9-8ed6-4a1a-87e7-e9df164cffa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, schema_of_json\n",
    "\n",
    "# Inputs (update as needed)\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"  # SAS key with Send/Listen\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "# Fetch SAS key from secret scope\n",
    "sas_key = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "\n",
    "# Build connection string\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eh_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={sas_key};\"\n",
    "    f\"EntityPath={eh_name}\"\n",
    ")\n",
    "\n",
    "# Event Hubs config\n",
    "eh_conf = {\n",
    "    \"eventhubs.connectionString\": connection_string,\n",
    "    # Optional consumer group\n",
    "    # \"eventhubs.consumerGroup\": \"$Default\",\n",
    "    # Start positions (optional)\n",
    "    # \"eventhubs.startingPosition\": \"{\\\"offset\\\":\\\"-1\\\"}\",  # from beginning\n",
    "    # \"eventhubs.startingPosition\": \"{\\\"offset\\\":\\\"@latest\\\"}\",  # latest\n",
    "}\n",
    "\n",
    "# Read stream\n",
    "raw_df = (\n",
    "    spark.readStream\n",
    "         .format(\"eventhubs\")\n",
    "         .options(**eh_conf)\n",
    "         .load()\n",
    ")\n",
    "\n",
    "# Body is binary; convert to string for inspection\n",
    "df = raw_df.withColumn(\"body\", col(\"body\").cast(\"string\"))\n",
    "\n",
    "# If you know the JSON schema of body, you can parse it:\n",
    "# sample_json = '{\"foo\":\"bar\",\"value\":1}'\n",
    "# body_schema = schema_of_json(sample_json)\n",
    "# df = df.withColumn(\"body_parsed\", from_json(col(\"body\"), body_schema))\n",
    "\n",
    "display(df.select(\"enqueuedTime\", \"offset\", \"sequenceNumber\", \"body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70ecb6ee-cb53-486b-b627-db7f70174587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"evhns-natraining.servicebus.windows.net\"https://evhns-natraining.servicebus.windows.net:443/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7476e3-d207-451d-8679-1446dc22a79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  %sh nc -vz evhns-natraining.servicebus.windows.net 9093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3e1c4e-09ab-4dc4-b9b2-9603dc8628db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh nslookup evhns-natraining.servicebus.windows.net\n",
    "%sh nc -vz evhns-natraining.servicebus.windows.net 9093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10915196-c16a-481d-b2ae-a4448d201269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "nc -vz evhns-natraining.servicebus.windows.net 9093\n",
    "echo \"nc exit code: $?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff10a220-6179-4bf9-ac13-497a0e0f62a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "\n",
    "connection_string = (\n",
    "    f\"Endpoint=sb://{eh_namespace}/;\"\n",
    "    f\"SharedAccessKeyName={shared_access_key_name};\"\n",
    "    f\"SharedAccessKey={secret_value};\"\n",
    "    f\"EntityPath={eh_name}\"\n",
    ")\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}:9093\",\n",
    "    \"subscribe\": eh_name,\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.jaas.config\": (\n",
    "        f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule '\n",
    "        f'required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "    ),\n",
    "    \"kafka.request.timeout.ms\": \"120000\",\n",
    "    \"kafka.session.timeout.ms\": \"30000\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"maxOffsetsPerTrigger\": \"10000\",\n",
    "}\n",
    "\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "         .format(\"kafka\")\n",
    "         .options(**KAFKA_OPTIONS)\n",
    "         .load()\n",
    ")\n",
    "\n",
    "display(df_stream.select(col(\"offset\"), col(\"timestamp\"), col(\"value\").cast(\"string\")))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8946049318739263,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "eventhubtest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
