{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f87a72f-e55e-4542-9b83-259b53465cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./02config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97fc72d1-fe8c-498a-8de7-703f5bf92411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer - Event Hub Streaming\n",
    "# MAGIC Reads events from Event Hub and writes to Orders & Products tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./02config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Event Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úì Event schema defined\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read and Parse Event Hub Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING EVENT HUB STREAMING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read from Event Hub\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"eventhubs\")\n",
    "    .options(**event_hubs_conf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"‚úì Raw stream created\")\n",
    "\n",
    "# Parse messages\n",
    "parsed_stream = (raw_stream\n",
    "    .withColumn(\"json_value\", col(\"body\").cast(\"string\"))\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"enqueuedTime\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"eventhub_offset\"),\n",
    "        col(\"sequenceNumber\").alias(\"sequence_number\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "    .withColumn(\"order_timestamp_parsed\", to_timestamp(col(\"order_timestamp\")))\n",
    ")\n",
    "\n",
    "print(\"‚úì Stream parsing configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Orders Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ORDERS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "orders_stream = parsed_stream.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"location\"),\n",
    "    col(\"product_id\"),\n",
    "    col(\"order_status\"),\n",
    "    col(\"payment_method\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"discount_pct\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"order_timestamp\"),\n",
    "    col(\"order_timestamp_parsed\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"eventhub_offset\"),\n",
    "    col(\"sequence_number\"),\n",
    "    col(\"partition_id\"),\n",
    "    col(\"bronze_timestamp\")\n",
    ")\n",
    "\n",
    "orders_query = (orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    #.trigger(processingTime=\"10 seconds\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(bronze_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Orders stream started: {orders_query.id}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Products Stream with UPSERT\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PRODUCTS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "products_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"base_price\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"bronze_timestamp\")\n",
    "    )\n",
    "    .dropDuplicates([\"product_id\"])\n",
    ")\n",
    "\n",
    "def upsert_products(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    batch_df.createOrReplaceTempView(\"products_batch\")\n",
    "    \n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {bronze_products_table} target\n",
    "    USING products_batch source\n",
    "    ON target.product_id = source.product_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.product_name = source.product_name,\n",
    "            target.category = source.category,\n",
    "            target.brand = source.brand,\n",
    "            target.base_price = source.base_price,\n",
    "            target.unit_price = source.unit_price,\n",
    "            target.bronze_timestamp = source.bronze_timestamp\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (product_id, product_name, category, brand, base_price, unit_price, bronze_timestamp)\n",
    "        VALUES (source.product_id, source.product_name, source.category, source.brand, \n",
    "                source.base_price, source.unit_price, source.bronze_timestamp)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"  Batch {batch_id}: ‚úì Upserted {batch_df.count()} products\")\n",
    "    except Exception as e:\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e) or \"does not exist\" in str(e).lower():\n",
    "            batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_products_table)\n",
    "            print(f\"  Batch {batch_id}: ‚úì Created table with {batch_df.count()} products\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "products_query = (products_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", products_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    #.trigger(processingTime=\"10 seconds\")\n",
    "    .trigger(availableNow=True)\n",
    "    .foreachBatch(upsert_products)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"‚úì Products stream started: {products_query.id}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Streams\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVE STREAMING QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nQuery ID: {stream.id}\")\n",
    "    print(f\"  Status: {stream.status['message']}\")\n",
    "    print(f\"  Active: {stream.isActive}\")\n",
    "    \n",
    "    if stream.recentProgress:\n",
    "        latest = stream.recentProgress[-1]\n",
    "        print(f\"  Batch: {latest.get('batchId', 'N/A')}\")\n",
    "        print(f\"  Input Rows: {latest.get('numInputRows', 0)}\")\n",
    "        print(f\"  Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONITORING (30 seconds)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        orders_count = spark.table(bronze_orders_table).count()\n",
    "        print(f\"‚è±Ô∏è  {(i+1)*5}s - Orders: {orders_count:,}\", end=\"\")\n",
    "    except:\n",
    "        print(f\"‚è±Ô∏è  {(i+1)*5}s - Orders: waiting...\", end=\"\")\n",
    "    \n",
    "    try:\n",
    "        products_count = spark.table(bronze_products_table).count()\n",
    "        print(f\" | Products: {products_count:,}\")\n",
    "    except:\n",
    "        print(\" | Products: waiting...\")\n",
    "\n",
    "print(\"\\n‚úì Monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Orders Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    orders_df = spark.table(bronze_orders_table)\n",
    "    orders_count = orders_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Orders: {orders_count:,} records\")\n",
    "    \n",
    "    if orders_count > 0:\n",
    "        display(orders_df.orderBy(desc(\"bronze_timestamp\")).limit(10))\n",
    "        \n",
    "        display(\n",
    "            orders_df.groupBy(\"location\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"orders\"),\n",
    "                sum(\"total_amount\").alias(\"revenue\")\n",
    "            )\n",
    "            .orderBy(desc(\"orders\"))\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Orders table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Products Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    products_df = spark.table(bronze_products_table)\n",
    "    products_count = products_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Products: {products_count:,} records\")\n",
    "    \n",
    "    if products_count > 0:\n",
    "        display(products_df.orderBy(\"product_id\"))\n",
    "        \n",
    "        display(\n",
    "            products_df.groupBy(\"category\", \"brand\")\n",
    "            .count()\n",
    "            .orderBy(\"category\", \"brand\")\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Products table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop Streams\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop\n",
    "\"\"\"\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping: {stream.id}\")\n",
    "    stream.stop()\n",
    "print(\"‚úì All streams stopped\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRONZE LAYER COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    orders_count = spark.table(bronze_orders_table).count()\n",
    "    products_count = spark.table(bronze_products_table).count()\n",
    "    \n",
    "    print(f\"\\nRecords:\")\n",
    "    print(f\"  Orders: {orders_count:,}\")\n",
    "    print(f\"  Products: {products_count:,}\")\n",
    "    print(f\"\\nActive Streams: {len(spark.streams.active)}\")\n",
    "    \n",
    "    for stream in spark.streams.active:\n",
    "        print(f\"  ‚Ä¢ {stream.id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Tables not ready yet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5534691258377251,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
