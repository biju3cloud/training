{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc1cc6e-988e-443c-ab5d-735b21c5991f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %run /Workspace/Users/biju.thottathil@3cloudsolutions.com/training/databricksinternaldemo/greentaxiautoloader/00setupconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f593810-5639-461a-8e23-15c33a688470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # 01 - Bronze Layer: Autoloader Ingestion\n",
    "# MAGIC \n",
    "# MAGIC This notebook implements the Bronze layer using Databricks Autoloader for incremental data ingestion.\n",
    "# MAGIC \n",
    "# MAGIC **Features:**\n",
    "# MAGIC - Incremental file processing with Autoloader\n",
    "# MAGIC - Schema inference and evolution\n",
    "# MAGIC - Checkpoint management for exactly-once semantics\n",
    "# MAGIC - Error handling and data quality checks\n",
    "# MAGIC \n",
    "# MAGIC **Author:** Data Engineering Team  \n",
    "# MAGIC **Last Updated:** December 2024\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Load Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Access configuration from temp view\n",
    "config_df = spark.table(\"pipeline_config\")\n",
    "config = config_df.first().asDict()\n",
    "\n",
    "CATALOG = config[\"catalog\"]\n",
    "SCHEMA = config[\"schema\"]\n",
    "BRONZE_TABLE = config[\"bronze_table\"]\n",
    "SOURCE_DATA_PATH = config[\"source_data_path\"]\n",
    "CHECKPOINT_LOCATION = config[\"checkpoint_location\"]\n",
    "\n",
    "print(\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   Source: {SOURCE_DATA_PATH}\")\n",
    "print(f\"   Target: {CATALOG}.{SCHEMA}.{BRONZE_TABLE}\")\n",
    "print(f\"   Checkpoint: {CHECKPOINT_LOCATION}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Define Bronze Table Schema\n",
    "# MAGIC \n",
    "# MAGIC Define the expected schema for Green Taxi data to ensure data quality.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"lpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"RatecodeID\", IntegerType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"ehail_fee\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"trip_type\", IntegerType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Bronze schema defined\")\n",
    "print(f\"   Total fields: {len(bronze_schema.fields)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Create Autoloader Stream\n",
    "# MAGIC \n",
    "# MAGIC Use Autoloader to incrementally ingest new files from the source location.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configure Autoloader\n",
    "autoloader_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_LOCATION}schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaHints\", \"VendorID Integer, lpep_pickup_datetime Timestamp, lpep_dropoff_datetime Timestamp, store_and_fwd_flag String, RatecodeID Integer, PULocationID Integer, DOLocationID Integer, passenger_count Integer, trip_distance Double, fare_amount Double, extra Double, mta_tax Double, tip_amount Double, tolls_amount Double, ehail_fee Double, improvement_surcharge Double, total_amount Double, payment_type Integer, trip_type Integer, congestion_surcharge Double\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 100)  # Process up to 100 files per trigger\n",
    "    .schema(bronze_schema)  # Provide schema hint\n",
    "    .load(SOURCE_DATA_PATH)\n",
    ")\n",
    "\n",
    "# Add audit columns\n",
    "bronze_stream = (autoloader_df\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"source_file\", F.input_file_name())\n",
    "    .withColumn(\"processing_date\", F.current_date())\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Autoloader stream configured\")\n",
    "display(bronze_stream.limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Write to Bronze Delta Table\n",
    "# MAGIC \n",
    "# MAGIC Write the streaming data to a Bronze Delta table with checkpoint management.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define checkpoint location for this stream\n",
    "bronze_checkpoint = f\"{CHECKPOINT_LOCATION}bronze_stream/\"\n",
    "\n",
    "# Write stream to Delta table\n",
    "bronze_stream_query = (\n",
    "    bronze_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", bronze_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")  # Enable schema evolution\n",
    "    #.trigger(processingTime=\"1 minute\")  # Process every minute\n",
    "    .trigger(once=True)\n",
    "    .table(f\"`{CATALOG}`.`{SCHEMA}`.`{BRONZE_TABLE}`\")\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Bronze stream started\")\n",
    "print(f\"   Stream ID: {bronze_stream_query.id}\")\n",
    "print(f\"   Status: {bronze_stream_query.status}\")\n",
    "print(f\"   Checkpoint: {bronze_checkpoint}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Monitor Stream Progress\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "# Monitor for 2 minutes\n",
    "print(\"üìä Monitoring stream progress...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(4):  # Check 4 times (30 seconds each)\n",
    "    try:\n",
    "        # Get latest progress\n",
    "        progress = bronze_stream_query.lastProgress\n",
    "        \n",
    "        if progress:\n",
    "            print(f\"\\nüîÑ Progress Update {i+1}:\")\n",
    "            print(f\"   Timestamp: {progress['timestamp']}\")\n",
    "            print(f\"   Input Rows: {progress.get('numInputRows', 0):,}\")\n",
    "            print(f\"   Processed Rows: {progress.get('processedRowsPerSecond', 0):.2f}/sec\")\n",
    "            print(f\"   Batch ID: {progress.get('batchId', 0)}\")\n",
    "            \n",
    "            # Check sources\n",
    "            if 'sources' in progress:\n",
    "                for source in progress['sources']:\n",
    "                    print(f\"   Files Processed: {source.get('numInputRows', 0):,}\")\n",
    "        else:\n",
    "            print(f\"\\n‚è≥ Waiting for first batch... ({i+1}/4)\")\n",
    "        \n",
    "        time.sleep(30)  # Wait 30 seconds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error checking progress: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Initial monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Verify Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read bronze table\n",
    "bronze_df = spark.table(f\"{CATALOG}.{SCHEMA}.{BRONZE_TABLE}\")\n",
    "\n",
    "print(f\"‚úÖ Bronze Table: {CATALOG}.{SCHEMA}.{BRONZE_TABLE}\")\n",
    "print(f\"   Total Records: {bronze_df.count():,}\")\n",
    "print(f\"   Schema:\")\n",
    "bronze_df.printSchema()\n",
    "\n",
    "# Show sample records\n",
    "print(\"\\nüìä Sample Records:\")\n",
    "display(bronze_df.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Perform data quality checks\n",
    "print(\"üîç Running Data Quality Checks...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: Null values in critical columns\n",
    "null_checks = bronze_df.select(\n",
    "    F.sum(F.when(F.col(\"VendorID\").isNull(), 1).otherwise(0)).alias(\"null_vendor\"),\n",
    "    F.sum(F.when(F.col(\"lpep_pickup_datetime\").isNull(), 1).otherwise(0)).alias(\"null_pickup\"),\n",
    "    F.sum(F.when(F.col(\"lpep_dropoff_datetime\").isNull(), 1).otherwise(0)).alias(\"null_dropoff\"),\n",
    "    F.sum(F.when(F.col(\"total_amount\").isNull(), 1).otherwise(0)).alias(\"null_amount\")\n",
    ").first()\n",
    "\n",
    "print(\"1Ô∏è‚É£ Null Value Check:\")\n",
    "for field, count in null_checks.asDict().items():\n",
    "    print(f\"   {field}: {count:,} nulls\")\n",
    "\n",
    "# Check 2: Invalid values\n",
    "invalid_checks = bronze_df.select(\n",
    "    F.sum(F.when(F.col(\"trip_distance\") < 0, 1).otherwise(0)).alias(\"negative_distance\"),\n",
    "    F.sum(F.when(F.col(\"fare_amount\") < 0, 1).otherwise(0)).alias(\"negative_fare\"),\n",
    "    F.sum(F.when(F.col(\"passenger_count\") <= 0, 1).otherwise(0)).alias(\"invalid_passengers\"),\n",
    "    F.sum(F.when(F.col(\"total_amount\") < 0, 1).otherwise(0)).alias(\"negative_total\")\n",
    ").first()\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Invalid Value Check:\")\n",
    "for field, count in invalid_checks.asDict().items():\n",
    "    print(f\"   {field}: {count:,} invalid records\")\n",
    "\n",
    "# Check 3: Date range\n",
    "date_stats = bronze_df.select(\n",
    "    F.min(\"lpep_pickup_datetime\").alias(\"min_pickup\"),\n",
    "    F.max(\"lpep_pickup_datetime\").alias(\"max_pickup\"),\n",
    "    F.count(\"*\").alias(\"total_records\")\n",
    ").first()\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Date Range:\")\n",
    "print(f\"   Earliest pickup: {date_stats['min_pickup']}\")\n",
    "print(f\"   Latest pickup: {date_stats['max_pickup']}\")\n",
    "print(f\"   Total records: {date_stats['total_records']:,}\")\n",
    "\n",
    "# Check 4: Vendor distribution\n",
    "print(\"\\n4Ô∏è‚É£ Vendor Distribution:\")\n",
    "vendor_dist = bronze_df.groupBy(\"VendorID\").count().orderBy(\"VendorID\")\n",
    "display(vendor_dist)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Data quality checks complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Stream Management\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check if stream is active\n",
    "print(\"üìä Stream Status:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if bronze_stream_query.isActive:\n",
    "    print(f\"‚úÖ Stream is ACTIVE\")\n",
    "    print(f\"   Stream ID: {bronze_stream_query.id}\")\n",
    "    print(f\"   Name: {bronze_stream_query.name or 'Unnamed'}\")\n",
    "    \n",
    "    # Get recent progress\n",
    "    recent_progress = bronze_stream_query.recentProgress\n",
    "    if recent_progress:\n",
    "        print(f\"\\nüìà Recent Activity:\")\n",
    "        print(f\"   Total Batches: {len(recent_progress)}\")\n",
    "        total_rows = sum([p.get('numInputRows', 0) for p in recent_progress])\n",
    "        print(f\"   Total Rows Processed: {total_rows:,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Stream is NOT ACTIVE\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Stop Stream (Optional)\n",
    "# MAGIC \n",
    "# MAGIC Uncomment to stop the stream. For continuous processing, leave it running.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop the stream\n",
    "# bronze_stream_query.stop()\n",
    "# print(\"üõë Stream stopped successfully\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 10. View Stream Metrics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display stream metrics\n",
    "print(\"üìä Stream Metrics Dashboard\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all active streams\n",
    "active_streams = spark.streams.active\n",
    "\n",
    "print(f\"Active Streams: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    "    print(f\"\\nüîÑ Stream: {stream.id}\")\n",
    "    print(f\"   Name: {stream.name or 'Unnamed'}\")\n",
    "    print(f\"   Status: {'Active' if stream.isActive else 'Inactive'}\")\n",
    "    \n",
    "    if stream.lastProgress:\n",
    "        progress = stream.lastProgress\n",
    "        print(f\"   Last Batch: {progress.get('batchId', 'N/A')}\")\n",
    "        print(f\"   Input Rows: {progress.get('numInputRows', 0):,}\")\n",
    "        print(f\"   Processing Time: {progress.get('durationMs', {}).get('triggerExecution', 0):,} ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 11. Query Bronze Table Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get table statistics\n",
    "print(\"üìä Bronze Table Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stats_query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT VendorID) as unique_vendors,\n",
    "    COUNT(DISTINCT DATE(lpep_pickup_datetime)) as unique_days,\n",
    "    MIN(lpep_pickup_datetime) as earliest_trip,\n",
    "    MAX(lpep_pickup_datetime) as latest_trip,\n",
    "    SUM(trip_distance) as total_distance,\n",
    "    SUM(total_amount) as total_revenue,\n",
    "    AVG(trip_distance) as avg_distance,\n",
    "    AVG(total_amount) as avg_fare\n",
    "FROM {CATALOG}.{SCHEMA}.{BRONZE_TABLE}\n",
    "\"\"\"\n",
    "\n",
    "stats_df = spark.sql(stats_query)\n",
    "display(stats_df)\n",
    "\n",
    "print(\"‚úÖ Statistics query complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 12. Checkpoint Information\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display checkpoint information\n",
    "print(\"üìÇ Checkpoint Information\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # List checkpoint files\n",
    "    checkpoint_files = dbutils.fs.ls(bronze_checkpoint)\n",
    "    print(f\"Checkpoint Location: {bronze_checkpoint}\")\n",
    "    print(f\"Total Files: {len(checkpoint_files)}\")\n",
    "    \n",
    "    # Show commit files\n",
    "    commits = [f for f in checkpoint_files if f.name.startswith('commits/')]\n",
    "    print(f\"\\n‚úÖ Commit Logs: {len(commits)}\")\n",
    "    \n",
    "    # Show offset files\n",
    "    offsets = [f for f in checkpoint_files if 'offsets' in f.name]\n",
    "    print(f\"‚úÖ Offset Files: {len(offsets)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not read checkpoint: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 13. Next Steps\n",
    "# MAGIC \n",
    "# MAGIC 1. ‚úÖ **Bronze Layer Complete** - Data is being incrementally ingested\n",
    "# MAGIC 2. üìä **Data Quality Verified** - Basic checks passed\n",
    "# MAGIC 3. üîÑ **Stream Running** - Autoloader is monitoring for new files\n",
    "# MAGIC 4. ‚≠ê **Ready for Silver** - Proceed to `02_silver_star_schema.py`\n",
    "# MAGIC \n",
    "# MAGIC **Stream Management:**\n",
    "# MAGIC - Stream will continue running until manually stopped\n",
    "# MAGIC - New files added to source location will be automatically processed\n",
    "# MAGIC - Checkpoint ensures exactly-once processing\n",
    "# MAGIC \n",
    "# MAGIC **Monitoring:**\n",
    "# MAGIC - Check stream status: `spark.streams.active`\n",
    "# MAGIC - View progress: `bronze_stream_query.lastProgress`\n",
    "# MAGIC - Stop stream: `bronze_stream_query.stop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a6904a-bb87-4885-9036-df7b1e815e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from `na-dbxtraining`.biju_bronze."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8897429830020923,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01bronzeautoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
