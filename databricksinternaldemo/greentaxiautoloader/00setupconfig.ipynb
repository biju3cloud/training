{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fa85de-9b08-43b6-b996-50212d1a1f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # 00 - Setup and Configuration (Using Real Data)\n",
    "# MAGIC \n",
    "# MAGIC This notebook sets up the environment for the NYC Green Cabs Star Schema pipeline and loads your real dataset.\n",
    "# MAGIC \n",
    "# MAGIC **Features:**\n",
    "# MAGIC - Unity Catalog configuration\n",
    "# MAGIC - Database and volume creation\n",
    "# MAGIC - Load and validate real Green Taxi data\n",
    "# MAGIC - Performance optimization settings\n",
    "# MAGIC \n",
    "# MAGIC **Author:** Data Engineering Team  \n",
    "# MAGIC **Last Updated:** December 2024\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Configuration Parameters\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "CATALOG = \"na-dbxtraining\"  # Change to your catalog name\n",
    "SCHEMA = \"biju_raw\"\n",
    "VOLUME = \"raw_data\"\n",
    "\n",
    "# Table Names\n",
    "BRONZE_TABLE = \"bronze_green_trips\"\n",
    "FACT_TABLE = \"fact_green_trips\"\n",
    "DIM_VENDOR = \"dim_vendor\"\n",
    "DIM_RATECODE = \"dim_ratecode\"\n",
    "DIM_PAYMENT_TYPE = \"dim_payment_type\"\n",
    "DIM_TRIP_TYPE = \"dim_trip_type\"\n",
    "DIM_DATE = \"dim_date\"\n",
    "DIM_LOCATION = \"dim_location\"\n",
    "\n",
    "# Storage Paths\n",
    "CHECKPOINT_LOCATION = f\"/Volumes/na-dbxtraining/biju_raw/biju_vol/{CATALOG}/{SCHEMA}/{VOLUME}/checkpoints/\"\n",
    "SOURCE_DATA_PATH = f\"/Volumes/na-dbxtraining/biju_raw/biju_vol/greencabs/raw_data\"\n",
    "\n",
    "# Liquid Clustering Configuration\n",
    "CLUSTER_COLUMNS = [\"pickup_date\", \"vendor_id\", \"payment_type_id\"]\n",
    "\n",
    "# Broadcast Join Threshold (10MB default, increase if needed)\n",
    "BROADCAST_THRESHOLD = 10 * 1024 * 1024  # 10MB\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(f\"üìç Catalog: {CATALOG}\")\n",
    "print(f\"üìç Schema: {SCHEMA}\")\n",
    "print(f\"üìç Source Path: {SOURCE_DATA_PATH}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Create Database and Volume\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create catalog if it doesn't exist (requires appropriate permissions)\n",
    "# spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "\n",
    "# Create schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.{SCHEMA}\")\n",
    "\n",
    "# Create volume for raw data storage\n",
    "#spark.sql(f\"\"\"\n",
    "#CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\n",
    "#COMMENT 'Volume for NYC Green Taxi raw data and checkpoints'\n",
    "##\"\"\")\n",
    "\n",
    "#print(f\"‚úÖ Schema created: {CATALOG}.{SCHEMA}\")\n",
    "#print(f\"‚úÖ Volume created: {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Set Spark Configurations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Broadcast join threshold (for dimension tables)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", BROADCAST_THRESHOLD)\n",
    "\n",
    "# Adaptive Query Execution (improves join strategy)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# Liquid Clustering support\n",
    "spark.conf.set(\"spark.databricks.delta.clusteredTable.enableClusteringTablePreview\", \"true\")\n",
    "\n",
    "# Auto Optimize for Delta tables\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "# Display settings\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "\n",
    "print(\"‚úÖ Spark configurations set successfully!\")\n",
    "print(f\"   - Broadcast threshold: {BROADCAST_THRESHOLD / (1024*1024)} MB\")\n",
    "print(f\"   - Adaptive Query Execution: Enabled\")\n",
    "print(f\"   - Liquid Clustering: Enabled\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Create Source Data Directory\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create directory structure\n",
    "try:\n",
    "    dbutils.fs.mkdirs(SOURCE_DATA_PATH)\n",
    "    print(f\"‚úÖ Source data directory created: {SOURCE_DATA_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Directory may already exist: {e}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "try:\n",
    "    dbutils.fs.mkdirs(CHECKPOINT_LOCATION)\n",
    "    print(f\"‚úÖ Checkpoint directory created: {CHECKPOINT_LOCATION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Directory may already exist: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b267a9eb-d96a-4d47-8cb7-4e16c69af821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Load Your Real Green Taxi Data\n",
    "# MAGIC \n",
    "# MAGIC This cell loads the uploaded CSV file and prepares it for the pipeline.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Path to uploaded file (adjust if needed)\n",
    "UPLOADED_FILE = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/greencabs/raw_data/2023_Green_Taxi_Trip_Data-small.csv\"\n",
    "\n",
    "# Read the CSV file with proper parsing\n",
    "print(\"üì• Loading real Green Taxi data...\")\n",
    "\n",
    "# Define schema to handle the data properly\n",
    "raw_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(UPLOADED_FILE)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {raw_df.count()} records from uploaded file\")\n",
    "print(\"\\nüìä Sample of raw data:\")\n",
    "display(raw_df.limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Clean and Transform Data\n",
    "# MAGIC \n",
    "# MAGIC Parse dates correctly and handle data quality issues.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Clean the data - handle date parsing and nulls\n",
    "cleaned_df = (raw_df\n",
    "    # Parse dates from M/d/yy H:mm format to proper timestamp\n",
    "    .withColumn(\"lpep_pickup_datetime\", \n",
    "                F.to_timestamp(F.col(\"lpep_pickup_datetime\"), \"M/d/yy H:mm\"))\n",
    "    .withColumn(\"lpep_dropoff_datetime\", \n",
    "                F.to_timestamp(F.col(\"lpep_dropoff_datetime\"), \"M/d/yy H:mm\"))\n",
    "    \n",
    "    # Handle empty string values - convert to null\n",
    "    .withColumn(\"ehail_fee\", \n",
    "                F.when(F.col(\"ehail_fee\").isNull() | (F.col(\"ehail_fee\") == \"\"), None)\n",
    "                .otherwise(F.col(\"ehail_fee\").cast(\"double\")))\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    .withColumn(\"VendorID\", F.col(\"VendorID\").cast(\"int\"))\n",
    "    .withColumn(\"RatecodeID\", F.col(\"RatecodeID\").cast(\"int\"))\n",
    "    .withColumn(\"PULocationID\", F.col(\"PULocationID\").cast(\"int\"))\n",
    "    .withColumn(\"DOLocationID\", F.col(\"DOLocationID\").cast(\"int\"))\n",
    "    .withColumn(\"passenger_count\", F.col(\"passenger_count\").cast(\"int\"))\n",
    "    .withColumn(\"trip_distance\", F.col(\"trip_distance\").cast(\"double\"))\n",
    "    .withColumn(\"fare_amount\", F.col(\"fare_amount\").cast(\"double\"))\n",
    "    .withColumn(\"extra\", F.col(\"extra\").cast(\"double\"))\n",
    "    .withColumn(\"mta_tax\", F.col(\"mta_tax\").cast(\"double\"))\n",
    "    .withColumn(\"tip_amount\", F.col(\"tip_amount\").cast(\"double\"))\n",
    "    .withColumn(\"tolls_amount\", F.col(\"tolls_amount\").cast(\"double\"))\n",
    "    .withColumn(\"improvement_surcharge\", F.col(\"improvement_surcharge\").cast(\"double\"))\n",
    "    .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"double\"))\n",
    "    .withColumn(\"payment_type\", F.col(\"payment_type\").cast(\"int\"))\n",
    "    .withColumn(\"trip_type\", F.col(\"trip_type\").cast(\"int\"))\n",
    "    .withColumn(\"congestion_surcharge\", F.col(\"congestion_surcharge\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data cleaned and transformed\")\n",
    "print(f\"   Records after cleaning: {cleaned_df.count()}\")\n",
    "print(\"\\nüìä Cleaned data with proper timestamps:\")\n",
    "display(cleaned_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"üîç Running Data Quality Checks on Real Data...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: Date validity\n",
    "date_check = cleaned_df.select(\n",
    "    F.min(\"lpep_pickup_datetime\").alias(\"earliest_pickup\"),\n",
    "    F.max(\"lpep_pickup_datetime\").alias(\"latest_pickup\"),\n",
    "    F.count(\"*\").alias(\"total_records\")\n",
    ").first()\n",
    "\n",
    "print(\"1Ô∏è‚É£ Date Range Check:\")\n",
    "print(f\"   Earliest pickup: {date_check['earliest_pickup']}\")\n",
    "print(f\"   Latest pickup: {date_check['latest_pickup']}\")\n",
    "print(f\"   Total records: {date_check['total_records']}\")\n",
    "\n",
    "# Check 2: Null values in critical columns\n",
    "null_check = cleaned_df.select(\n",
    "    F.sum(F.when(F.col(\"VendorID\").isNull(), 1).otherwise(0)).alias(\"null_vendor\"),\n",
    "    F.sum(F.when(F.col(\"lpep_pickup_datetime\").isNull(), 1).otherwise(0)).alias(\"null_pickup\"),\n",
    "    F.sum(F.when(F.col(\"total_amount\").isNull(), 1).otherwise(0)).alias(\"null_amount\")\n",
    ").first()\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Null Value Check:\")\n",
    "print(f\"   Null VendorID: {null_check['null_vendor']}\")\n",
    "print(f\"   Null pickup_datetime: {null_check['null_pickup']}\")\n",
    "print(f\"   Null total_amount: {null_check['null_amount']}\")\n",
    "\n",
    "# Check 3: Value distributions\n",
    "print(\"\\n3Ô∏è‚É£ Vendor Distribution:\")\n",
    "vendor_dist = cleaned_df.groupBy(\"VendorID\").count().orderBy(\"VendorID\")\n",
    "display(vendor_dist)\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Payment Type Distribution:\")\n",
    "payment_dist = cleaned_df.groupBy(\"payment_type\").count().orderBy(\"payment_type\")\n",
    "display(payment_dist)\n",
    "\n",
    "# Check 4: Basic statistics\n",
    "print(\"\\n5Ô∏è‚É£ Trip Statistics:\")\n",
    "stats = cleaned_df.select(\n",
    "    F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "    F.avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "    F.avg(\"total_amount\").alias(\"avg_total\"),\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").first()\n",
    "\n",
    "print(f\"   Average distance: {stats['avg_distance']:.2f} miles\")\n",
    "print(f\"   Average fare: ${stats['avg_fare']:.2f}\")\n",
    "print(f\"   Average total: ${stats['avg_total']:.2f}\")\n",
    "print(f\"   Total revenue: ${stats['total_revenue']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Data quality checks complete!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Save Data to Source Location\n",
    "# MAGIC \n",
    "# MAGIC Save the cleaned data to the volume location where Autoloader will pick it up.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save cleaned data to source location for Autoloader\n",
    "output_file = f\"{SOURCE_DATA_PATH}green_tripdata_2023_01.csv\"\n",
    "\n",
    "print(f\"üíæ Saving cleaned data to: {output_file}\")\n",
    "\n",
    "# Write as CSV (single file for this small dataset)\n",
    "(cleaned_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(output_file)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data saved successfully!\")\n",
    "\n",
    "# Verify the file was created\n",
    "try:\n",
    "    files = dbutils.fs.ls(SOURCE_DATA_PATH)\n",
    "    print(f\"\\nüìÅ Files in source directory ({len(files)}):\")\n",
    "    for file in files:\n",
    "        print(f\"   - {file.name} ({file.size:,} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not list files: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Verify Data is Ready for Autoloader\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read back the saved CSV to verify it's correct\n",
    "print(\"üîç Verifying saved data...\")\n",
    "\n",
    "test_read = spark.read.option(\"header\", \"true\").csv(SOURCE_DATA_PATH)\n",
    "test_count = test_read.count()\n",
    "\n",
    "print(f\"‚úÖ Successfully verified {test_count:,} records in source location\")\n",
    "print(\"\\nüìä Sample of saved data:\")\n",
    "display(test_read.limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 10. Create Configuration Widget\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Store configuration in notebook widgets for easy access in other notebooks\n",
    "dbutils.widgets.text(\"catalog\", CATALOG, \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", SCHEMA, \"Schema\")\n",
    "dbutils.widgets.text(\"volume\", VOLUME, \"Volume\")\n",
    "\n",
    "# Create a configuration dictionary for easy import\n",
    "config = {\n",
    "    \"catalog\": CATALOG,\n",
    "    \"schema\": SCHEMA,\n",
    "    \"volume\": VOLUME,\n",
    "    \"bronze_table\": BRONZE_TABLE,\n",
    "    \"fact_table\": FACT_TABLE,\n",
    "    \"dim_vendor\": DIM_VENDOR,\n",
    "    \"dim_ratecode\": DIM_RATECODE,\n",
    "    \"dim_payment_type\": DIM_PAYMENT_TYPE,\n",
    "    \"dim_trip_type\": DIM_TRIP_TYPE,\n",
    "    \"dim_date\": DIM_DATE,\n",
    "    \"dim_location\": DIM_LOCATION,\n",
    "    \"checkpoint_location\": CHECKPOINT_LOCATION,\n",
    "    \"source_data_path\": SOURCE_DATA_PATH,\n",
    "    \"cluster_columns\": CLUSTER_COLUMNS,\n",
    "    \"broadcast_threshold\": BROADCAST_THRESHOLD\n",
    "}\n",
    "\n",
    "# Save config to temp view for access in other notebooks\n",
    "spark.createDataFrame([config]).createOrReplaceTempView(\"pipeline_config\")\n",
    "\n",
    "print(\"‚úÖ Configuration saved to pipeline_config temp view\")\n",
    "print(\"\\nüìã Configuration Summary:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 11. Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"üéâ Setup Complete with Real Data!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ Environment Configuration:\")\n",
    "print(f\"   Catalog: {CATALOG}\")\n",
    "print(f\"   Schema: {SCHEMA}\")\n",
    "print(f\"   Volume: {VOLUME}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data Loaded:\")\n",
    "print(f\"   Records: {test_count:,}\")\n",
    "print(f\"   Source: Real NYC Green Taxi data (2023)\")\n",
    "print(f\"   Location: {SOURCE_DATA_PATH}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for Pipeline:\")\n",
    "print(\"   ‚úì Data cleaned and validated\")\n",
    "print(\"   ‚úì Dates properly parsed\")\n",
    "print(\"   ‚úì Data quality checks passed\")\n",
    "print(\"   ‚úì Files ready for Autoloader\")\n",
    "\n",
    "print(\"\\nüìä Data Summary:\")\n",
    "print(f\"   Date range: January 2023\")\n",
    "print(f\"   Vendors: {cleaned_df.select('VendorID').distinct().count()}\")\n",
    "print(f\"   Payment types: {cleaned_df.select('payment_type').distinct().count()}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. ‚úÖ Setup complete\")\n",
    "print(\"   2. üì• Run 01_bronze_autoloader.py\")\n",
    "print(\"   3. ‚≠ê Run 02_silver_star_schema.py\")\n",
    "print(\"   4. üìä Run 03_gold_analytics.py\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00setupconfig",
   "widgets": {
    "catalog": {
     "currentValue": "na-dbxtraining",
     "nuid": "96dd6822-ed8c-44b7-bb23-baa1f11db3b6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "na-dbxtraining",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "na-dbxtraining",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "biju_raw",
     "nuid": "638179e7-79ee-4e69-b461-551bbf24fae3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "biju_raw",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "biju_raw",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "volume": {
     "currentValue": "raw_data",
     "nuid": "2537d34b-ae86-4dbf-9ff1-b59af451e288",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "raw_data",
      "label": "Volume",
      "name": "volume",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "raw_data",
      "label": "Volume",
      "name": "volume",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
