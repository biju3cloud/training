{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ecd4b39-3976-4710-aecd-55317426fb97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %run ./00setupconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2246b321-a15b-49f9-b40f-e34309d6a09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # 02 - Silver Layer: Star Schema with Liquid Clustering\n",
    "# MAGIC \n",
    "# MAGIC This notebook creates the star schema with dimension and fact tables, implementing liquid clustering for optimal query performance.\n",
    "# MAGIC \n",
    "# MAGIC **Features:**\n",
    "# MAGIC - Star schema design (fact + dimension tables)\n",
    "# MAGIC - Liquid clustering on fact table\n",
    "# MAGIC - Broadcast-optimized dimension tables\n",
    "# MAGIC - SCD Type 1 for dimension updates\n",
    "# MAGIC \n",
    "# MAGIC **Author:** Data Engineering Team  \n",
    "# MAGIC **Last Updated:** December 2024\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Load Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Configuration variables are already available from %run command\n",
    "# No need to read from temp view - they're in the namespace\n",
    "\n",
    "print(\"‚úÖ Configuration loaded for star schema creation\")\n",
    "print(f\"   Catalog: {CATALOG}\")\n",
    "print(f\"   Bronze Schema: {BRONZE_SCHEMA}\")\n",
    "print(f\"   Silver Schema: {SILVER_SCHEMA}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Create Dimension Tables\n",
    "# MAGIC \n",
    "# MAGIC Create small, broadcast-friendly dimension tables for lookups.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.1 Dimension: Vendor\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create vendor dimension\n",
    "vendor_data = [\n",
    "    (1, \"Creative Mobile Technologies, LLC\", \"CMT\"),\n",
    "    (2, \"VeriFone Inc.\", \"VTS\")\n",
    "]\n",
    "\n",
    "dim_vendor = spark.createDataFrame(vendor_data, [\"vendor_id\", \"vendor_name\", \"vendor_code\"])\n",
    "\n",
    "# Add audit columns\n",
    "dim_vendor = dim_vendor.withColumn(\"created_at\", F.current_timestamp())\n",
    "\n",
    "# Write to Delta table\n",
    "dim_vendor.write.mode(\"overwrite\").saveAsTable(f\"`{CATALOG}`.{SILVER_SCHEMA}.{DIM_VENDOR}\")\n",
    "\n",
    "print(f\"‚úÖ Created: {CATALOG}.{SILVER_SCHEMA}.{config['dim_vendor']}\")\n",
    "display(dim_vendor)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.2 Dimension: Rate Code\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create rate code dimension\n",
    "ratecode_data = [\n",
    "    (1, \"Standard rate\", \"Standard\"),\n",
    "    (2, \"JFK\", \"JFK Airport\"),\n",
    "    (3, \"Newark\", \"Newark Airport\"),\n",
    "    (4, \"Nassau or Westchester\", \"Nassau/Westchester\"),\n",
    "    (5, \"Negotiated fare\", \"Negotiated\"),\n",
    "    (6, \"Group ride\", \"Group\")\n",
    "]\n",
    "\n",
    "dim_ratecode = spark.createDataFrame(ratecode_data, [\"ratecode_id\", \"ratecode_name\", \"ratecode_description\"])\n",
    "dim_ratecode = dim_ratecode.withColumn(\"created_at\", F.current_timestamp())\n",
    "\n",
    "dim_ratecode.write.mode(\"overwrite\").saveAsTable(f\"`{CATALOG}`.{SILVER_SCHEMA}.{DIM_RATECODE}\")\n",
    "\n",
    "print(f\"‚úÖ Created: `{CATALOG}`.{SILVER_SCHEMA}.{config['dim_ratecode']}\")\n",
    "display(dim_ratecode)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.3 Dimension: Payment Type\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create payment type dimension\n",
    "payment_data = [\n",
    "    (1, \"Credit card\", \"CC\"),\n",
    "    (2, \"Cash\", \"CASH\"),\n",
    "    (3, \"No charge\", \"NC\"),\n",
    "    (4, \"Dispute\", \"DIS\"),\n",
    "    (5, \"Unknown\", \"UNK\"),\n",
    "    (6, \"Voided trip\", \"VOID\")\n",
    "]\n",
    "\n",
    "dim_payment = spark.createDataFrame(payment_data, [\"payment_type_id\", \"payment_type_name\", \"payment_type_code\"])\n",
    "dim_payment = dim_payment.withColumn(\"created_at\", F.current_timestamp())\n",
    "\n",
    "dim_payment.write.mode(\"overwrite\").saveAsTable(f\"`{CATALOG}`.{SILVER_SCHEMA}.{DIM_PAYMENT_TYPE}\")\n",
    "\n",
    "print(f\"‚úÖ Created: {CATALOG}.{SILVER_SCHEMA}.{config['dim_payment_type']}\")\n",
    "display(dim_payment)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.4 Dimension: Trip Type\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create trip type dimension\n",
    "trip_type_data = [\n",
    "    (1, \"Street-hail\", \"Street\"),\n",
    "    (2, \"Dispatch\", \"Dispatch\")\n",
    "]\n",
    "\n",
    "dim_trip_type = spark.createDataFrame(trip_type_data, [\"trip_type_id\", \"trip_type_name\", \"trip_type_description\"])\n",
    "dim_trip_type = dim_trip_type.withColumn(\"created_at\", F.current_timestamp())\n",
    "\n",
    "dim_trip_type.write.mode(\"overwrite\").saveAsTable(f\"`{CATALOG}`.{SILVER_SCHEMA}.{DIM_TRIP_TYPE}\")\n",
    "\n",
    "print(f\"‚úÖ Created: `{CATALOG}`.{SILVER_SCHEMA}.{config['dim_trip_type']}\")\n",
    "display(dim_trip_type)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.5 Dimension: Date\n",
    "# MAGIC \n",
    "# MAGIC Create a date dimension for time-based analysis.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_date_dimension(start_date, end_date):\n",
    "    \"\"\"Generate date dimension with common attributes\"\"\"\n",
    "    dates = []\n",
    "    current_date = start_date\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        dates.append({\n",
    "            \"date\": current_date,\n",
    "            \"year\": current_date.year,\n",
    "            \"quarter\": (current_date.month - 1) // 3 + 1,\n",
    "            \"month\": current_date.month,\n",
    "            \"month_name\": current_date.strftime(\"%B\"),\n",
    "            \"day\": current_date.day,\n",
    "            \"day_of_week\": current_date.weekday() + 1,  # Monday = 1\n",
    "            \"day_name\": current_date.strftime(\"%A\"),\n",
    "            \"week_of_year\": current_date.isocalendar()[1],\n",
    "            \"is_weekend\": 1 if current_date.weekday() >= 5 else 0,\n",
    "            \"is_holiday\": 0  # Can be enhanced with holiday logic\n",
    "        })\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return dates\n",
    "\n",
    "# Generate date dimension for 2023-2025\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "\n",
    "date_data = create_date_dimension(start_date, end_date)\n",
    "dim_date = spark.createDataFrame(date_data)\n",
    "dim_date = dim_date.withColumn(\"created_at\", F.current_timestamp())\n",
    "\n",
    "dim_date.write.mode(\"overwrite\").saveAsTable(f\"`{CATALOG}`.{SILVER_SCHEMA}.{DIM_DATE}\")\n",
    "\n",
    "print(f\"‚úÖ Created: `{CATALOG}`.{SILVER_SCHEMA}.{config['dim_date']}\")\n",
    "print(f\"   Date range: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"   Total days: {dim_date.count():,}\")\n",
    "display(dim_date.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.6 Dimension: Location\n",
    "# MAGIC \n",
    "# MAGIC Create location dimension for pickup/dropoff zones.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# For now, create a simple location dimension with IDs\n",
    "# In production, you would load from NYC TLC taxi zone lookup\n",
    "location_ids = list(range(1, 266))  # 265 taxi zones\n",
    "\n",
    "dim_location = spark.createDataFrame(\n",
    "    [(loc_id, f\"Zone {loc_id}\", \"Unknown\") for loc_id in location_ids],\n",
    "    [\"location_id\", \"zone_name\", \"borough\"]\n",
    ")\n",
    "dim_location = dim_location.withColumn(\"created_at\", F.current_timestamp())\n",
    "\n",
    "dim_location.write.mode(\"overwrite\").saveAsTable(f\"`{CATALOG}`.{SILVER_SCHEMA}.{DIM_LOCATION}\")\n",
    "\n",
    "print(f\"‚úÖ Created: `{CATALOG}`.{SILVER_SCHEMA}.{config['dim_location']}\")\n",
    "print(f\"   Total locations: {dim_location.count():,}\")\n",
    "display(dim_location.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Verify Dimension Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check all dimension tables\n",
    "dimension_tables = [\n",
    "    config['dim_vendor'],\n",
    "    config['dim_ratecode'],\n",
    "    config['dim_payment_type'],\n",
    "    config['dim_trip_type'],\n",
    "    config['dim_date'],\n",
    "    config['dim_location']\n",
    "]\n",
    "\n",
    "print(\"üìä Dimension Table Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for table_name in dimension_tables:\n",
    "    full_table_name = f\"`{CATALOG}`.{SILVER_SCHEMA}.{table_name}\"\n",
    "    count = spark.table(full_table_name).count()\n",
    "    size_mb = spark.sql(f\"DESCRIBE DETAIL {full_table_name}\").select(\"sizeInBytes\").first()[0] / (1024 * 1024)\n",
    "    \n",
    "    broadcast_eligible = \"‚úÖ Yes\" if size_mb < 10 else \"‚ö†Ô∏è  No\"\n",
    "    \n",
    "    print(f\"{table_name}:\")\n",
    "    print(f\"   Records: {count:,}\")\n",
    "    print(f\"   Size: {size_mb:.2f} MB\")\n",
    "    print(f\"   Broadcast eligible: {broadcast_eligible}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Create Fact Table with Liquid Clustering\n",
    "# MAGIC \n",
    "# MAGIC Create the fact table with liquid clustering for optimal query performance.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read bronze data\n",
    "bronze_df = spark.table(f\"`{CATALOG}`.{BRONZE_SCHEMA}.{BRONZE_TABLE}\")\n",
    "\n",
    "# Transform to fact table structure\n",
    "# Transform to fact table structure\n",
    "fact_df = (\n",
    "    bronze_df\n",
    "    # Extract pickup and dropoff dates for clustering\n",
    "    .withColumn(\"pickup_date\", F.to_timestamp(F.col(\"lpep_pickup_datetime\"), \"M/d/yy H:mm\"))\n",
    "    .withColumn(\"dropoff_date\", F.to_timestamp(F.col(\"lpep_dropoff_datetime\"), \"M/d/yy H:mm\"))\n",
    "    # Rename columns to match dimension keys\n",
    "    .withColumnRenamed(\"VendorID\", \"vendor_id\")\n",
    "    .withColumnRenamed(\"RatecodeID\", \"ratecode_id\")\n",
    "    .withColumnRenamed(\"payment_type\", \"payment_type_id\")\n",
    "    .withColumnRenamed(\"trip_type\", \"trip_type_id\")\n",
    "    .withColumnRenamed(\"PULocationID\", \"pickup_location_id\")\n",
    "    .withColumnRenamed(\"DOLocationID\", \"dropoff_location_id\")\n",
    "    # Calculate derived metrics\n",
    "    .withColumn(\n",
    "        \"trip_duration_minutes\",\n",
    "        (F.unix_timestamp(F.col(\"lpep_dropoff_datetime\")) - F.unix_timestamp(F.col(\"lpep_pickup_datetime\"))) / 60\n",
    "    )\n",
    "    # Add fact table audit columns\n",
    "    .withColumn(\"fact_created_at\", F.current_timestamp())\n",
    "    .withColumn(\"fact_updated_at\", F.current_timestamp())\n",
    "    # Select relevant columns\n",
    "    .select(\n",
    "        \"vendor_id\",\n",
    "        \"pickup_date\",\n",
    "        \"dropoff_date\",\n",
    "        \"lpep_pickup_datetime\",\n",
    "        \"lpep_dropoff_datetime\",\n",
    "        \"ratecode_id\",\n",
    "        \"pickup_location_id\",\n",
    "        \"dropoff_location_id\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"trip_duration_minutes\",\n",
    "        \"fare_amount\",\n",
    "        \"extra\",\n",
    "        \"mta_tax\",\n",
    "        \"tip_amount\",\n",
    "        \"tolls_amount\",\n",
    "        \"ehail_fee\",\n",
    "        \"improvement_surcharge\",\n",
    "        \"total_amount\",\n",
    "        \"payment_type_id\",\n",
    "        \"trip_type_id\",\n",
    "        \"congestion_surcharge\",\n",
    "        \"store_and_fwd_flag\",\n",
    "        \"fact_created_at\",\n",
    "        \"fact_updated_at\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fact table transformation complete\")\n",
    "print(f\"   Total records: {fact_df.count():,}\")\n",
    "display(fact_df.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.1 Create Fact Table with Liquid Clustering\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create fact table with liquid clustering\n",
    "# Note: Liquid clustering is specified during table creation\n",
    "\n",
    "fact_table_name = f\"`{CATALOG}`.{SILVER_SCHEMA}.{FACT_TABLE}\"\n",
    "\n",
    "# Drop table if exists (for fresh creation)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {fact_table_name}\")\n",
    "\n",
    "# Create table with liquid clustering\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE {fact_table_name}\n",
    "USING DELTA\n",
    "CLUSTER BY (pickup_date, vendor_id, payment_type_id)\n",
    "COMMENT 'Fact table for Green Taxi trips with liquid clustering'\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableLiquidClustering' = 'true',\n",
    "  'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "  'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM {fact_df.createOrReplaceTempView('temp_fact')} OR (SELECT * FROM temp_fact LIMIT 0)\n",
    "\"\"\"\n",
    "\n",
    "# Write data to the fact table\n",
    "fact_df.write.mode(\"overwrite\").saveAsTable(fact_table_name)\n",
    "\n",
    "\n",
    "\n",
    "# Set clustering columns\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {fact_table_name}\n",
    "CLUSTER BY (pickup_date, vendor_id, payment_type_id)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Fact table created with liquid clustering: {fact_table_name}\")\n",
    "print(f\"   Clustering columns: {', '.join(CLUSTER_COLUMNS)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Verify Fact Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check fact table properties\n",
    "fact_details = spark.sql(f\"DESCRIBE DETAIL {fact_table_name}\").first()\n",
    "\n",
    "print(\"üìä Fact Table Details\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Table: {fact_table_name}\")\n",
    "print(f\"Format: {fact_details['format']}\")\n",
    "print(f\"Records: {fact_details['numFiles']:,} files\")\n",
    "print(f\"Size: {fact_details['sizeInBytes'] / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Partitions: {fact_details['numPartitions'] if 'numPartitions' in fact_details else 'N/A'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check table properties\n",
    "properties = spark.sql(f\"SHOW TBLPROPERTIES {fact_table_name}\").collect()\n",
    "\n",
    "print(\"\\nüìã Table Properties:\")\n",
    "for prop in properties:\n",
    "    if 'cluster' in prop['key'].lower() or 'liquid' in prop['key'].lower():\n",
    "        print(f\"   {prop['key']}: {prop['value']}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get clustering information\n",
    "try:\n",
    "    clustering_info = spark.sql(f\"DESCRIBE EXTENDED {fact_table_name}\").filter(\n",
    "        F.col(\"col_name\").contains(\"Clustering\")\n",
    "    ).collect()\n",
    "    \n",
    "    if clustering_info:\n",
    "        print(\"\\nüéØ Clustering Information:\")\n",
    "        for info in clustering_info:\n",
    "            print(f\"   {info['col_name']}: {info['data_type']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Clustering info: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Verify Star Schema Relationships\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Test joins between fact and dimension tables\n",
    "test_query = f\"\"\"\n",
    "SELECT \n",
    "    f.pickup_date,\n",
    "    v.vendor_name,\n",
    "    pt.payment_type_name,\n",
    "    tt.trip_type_name,\n",
    "    COUNT(*) as trip_count,\n",
    "    SUM(f.total_amount) as total_revenue,\n",
    "    AVG(f.trip_distance) as avg_distance\n",
    "FROM {fact_table_name} f\n",
    "JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_VENDOR} v ON f.vendor_id = v.vendor_id\n",
    "JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_PAYMENT_TYPE} pt ON f.payment_type_id = pt.payment_type_id\n",
    "JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_TRIP_TYPE} tt ON f.trip_type_id = tt.trip_type_id\n",
    "WHERE f.pickup_date >= '2024-01-01'\n",
    "GROUP BY f.pickup_date, v.vendor_name, pt.payment_type_name, tt.trip_type_name\n",
    "ORDER BY f.pickup_date, total_revenue DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Testing star schema joins...\")\n",
    "result_df = spark.sql(test_query)\n",
    "display(result_df)\n",
    "\n",
    "print(\"\\n‚úÖ Star schema relationships verified successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Star Schema Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate comprehensive statistics\n",
    "stats_query = f\"\"\"\n",
    "SELECT \n",
    "    'üìä Overall Statistics' as category,\n",
    "    CAST(COUNT(*) AS STRING) as total_trips,\n",
    "    CAST(COUNT(DISTINCT vendor_id) AS STRING) as unique_vendors,\n",
    "    CAST(COUNT(DISTINCT pickup_date) AS STRING) as unique_days,\n",
    "    CAST(SUM(trip_distance) AS STRING) as total_distance,\n",
    "    CAST(SUM(total_amount) AS STRING) as total_revenue,\n",
    "    CAST(AVG(trip_distance) AS STRING) as avg_distance,\n",
    "    CAST(AVG(total_amount) AS STRING) as avg_fare,\n",
    "    CAST(AVG(trip_duration_minutes) AS STRING) as avg_duration_min,\n",
    "    NULL as date_range\n",
    "FROM {fact_table_name}\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'üìÖ Date Range' as category,\n",
    "    NULL as total_trips,\n",
    "    NULL as unique_vendors,\n",
    "    NULL as unique_days,\n",
    "    NULL as total_distance,\n",
    "    NULL as total_revenue,\n",
    "    NULL as avg_distance,\n",
    "    NULL as avg_fare,\n",
    "    NULL as avg_duration_min,\n",
    "    CONCAT(CAST(MIN(pickup_date) AS STRING), ' to ', CAST(MAX(pickup_date) AS STRING)) as date_range\n",
    "FROM {fact_table_name}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìà Star Schema Statistics\")\n",
    "print(\"=\" * 80)\n",
    "stats_result = spark.sql(stats_query)\n",
    "display(stats_result)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Create Views for Analytics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create a convenient view joining all dimensions\n",
    "view_name = f\"`{CATALOG}`.{SILVER_SCHEMA}.vw_green_trips_enriched\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {view_name} AS\n",
    "SELECT \n",
    "    f.*,\n",
    "    v.vendor_name,\n",
    "    v.vendor_code,\n",
    "    rc.ratecode_name,\n",
    "    pt.payment_type_name,\n",
    "    tt.trip_type_name,\n",
    "    d.year,\n",
    "    d.month,\n",
    "    d.month_name,\n",
    "    d.day_of_week,\n",
    "    d.day_name,\n",
    "    d.is_weekend,\n",
    "    pu.zone_name as pickup_zone,\n",
    "    pu.borough as pickup_borough,\n",
    "    do.zone_name as dropoff_zone,\n",
    "    do.borough as dropoff_borough\n",
    "FROM {fact_table_name} f\n",
    "LEFT JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_VENDOR} v \n",
    "    ON f.vendor_id = v.vendor_id\n",
    "LEFT JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_RATECODE} rc \n",
    "    ON f.ratecode_id = rc.ratecode_id\n",
    "LEFT JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_PAYMENT_TYPE} pt \n",
    "    ON f.payment_type_id = pt.payment_type_id\n",
    "LEFT JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_TRIP_TYPE} tt \n",
    "    ON f.trip_type_id = tt.trip_type_id\n",
    "LEFT JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_DATE} d \n",
    "    ON f.pickup_date = d.date\n",
    "LEFT JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_LOCATION} pu \n",
    "    ON f.pickup_location_id = pu.location_id\n",
    "LEFT JOIN `{CATALOG}`.{SILVER_SCHEMA}.{DIM_LOCATION} do \n",
    "    ON f.dropoff_location_id = do.location_id\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Created view: {view_name}\")\n",
    "\n",
    "# Test the view\n",
    "print(\"\\nüìä Sample from enriched view:\")\n",
    "display(spark.table(view_name).limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"üéâ Silver Layer Star Schema - Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ Created Dimension Tables:\")\n",
    "for table_name in dimension_tables:\n",
    "    full_name = f\"`{CATALOG}`.{SILVER_SCHEMA}.{table_name}\"\n",
    "    count = spark.table(full_name).count()\n",
    "    print(f\"   {table_name}: {count:,} records\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created Fact Table:\")\n",
    "fact_count = spark.table(fact_table_name).count()\n",
    "print(f\"   {FACT_TABLE}: {fact_count:,} records\")\n",
    "print(f\"   Liquid clustering: pickup_date, vendor_id, payment_type_id\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created View:\")\n",
    "print(f\"   vw_green_trips_enriched: Ready for analytics\")\n",
    "\n",
    "print(\"\\nüìä Star Schema Ready for:\")\n",
    "print(\"   1. Broadcast joins with dimension tables\")\n",
    "print(\"   2. Optimized queries using liquid clustering\")\n",
    "print(\"   3. Time-based analysis with date dimension\")\n",
    "print(\"   4. Location-based analysis with zone lookups\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   - Run gold layer analytics: 03_gold_analytics.py\")\n",
    "print(\"   - Query with broadcast hints for best performance\")\n",
    "print(\"   - Monitor liquid clustering optimization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
