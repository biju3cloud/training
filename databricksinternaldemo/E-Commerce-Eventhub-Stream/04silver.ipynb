{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333fb534-f840-4c33-bacb-113135a68042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20e3a90-8562-4228-8ff4-9aa16e851f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Silver Layer - Streaming Join Orders with Products\n",
    "# MAGIC Creates enriched order_details table by streaming join of Bronze tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "catalog = \"na-dbxtraining\"\n",
    "schema_bronze = \"biju_bronze\"\n",
    "schema_silver = \"biju_silver\"\n",
    "schema_gold = \"biju_gold\"\n",
    "\n",
    "# Table Names (with backticks for catalog)\n",
    "bronze_orders_table = f\"`{catalog}`.{schema_bronze}.orders\"\n",
    "bronze_products_table = f\"`{catalog}`.{schema_bronze}.products\"\n",
    "silver_table = f\"`{catalog}`.{schema_silver}.order_details\"\n",
    "\n",
    "# Checkpoint location\n",
    "checkpoint_base = f\"/Volumes/na-dbxtraining/biju_raw/biju_vol/checkpoints/{catalog.replace('-', '_')}\"\n",
    "silver_checkpoint = f\"{checkpoint_base}/silver_order_details\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SILVER LAYER STREAMING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Source Tables:\")\n",
    "print(f\"  Orders: {bronze_orders_table}\")\n",
    "print(f\"  Products: {bronze_products_table}\")\n",
    "print(f\"Target Table:\")\n",
    "print(f\"  Silver: {silver_table}\")\n",
    "print(f\"Checkpoint: {silver_checkpoint}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Read Streaming Orders from Bronze\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READING BRONZE STREAMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read streaming orders (with watermark for late data handling)\n",
    "orders_stream = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .table(bronze_orders_table)\n",
    "    .withWatermark(\"bronze_timestamp\", \"1 minute\")  # Handle late arrivals within 1 minute\n",
    ")\n",
    "\n",
    "print(f\"‚úì Orders stream configured\")\n",
    "print(f\"  Source: {bronze_orders_table}\")\n",
    "print(f\"  Watermark: 1 minute on bronze_timestamp\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Read Products (Static or Streaming)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Option 1: Read products as STATIC table (recommended for small, slowly changing dimension)\n",
    "# This is more efficient as products don't change frequently\n",
    "products_df = spark.read.format(\"delta\").table(bronze_products_table)\n",
    "\n",
    "print(f\"\\n‚úì Products loaded as static dimension\")\n",
    "print(f\"  Source: {bronze_products_table}\")\n",
    "print(f\"  Records: {products_df.count()}\")\n",
    "\n",
    "# Option 2: If you want streaming products (uncomment if needed)\n",
    "\"\"\"\n",
    "products_stream = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .table(bronze_products_table)\n",
    "    .withWatermark(\"bronze_timestamp\", \"1 minute\")\n",
    ")\n",
    "print(f\"\\n‚úì Products stream configured\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Streaming Join and Enrichment\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURING STREAMING JOIN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Perform streaming join (stream-to-static)\n",
    "# For stream-to-stream join, use watermarks on both sides\n",
    "order_details_stream = (orders_stream\n",
    "    .join(\n",
    "        products_df,  # Use products_stream for stream-to-stream join\n",
    "        on=\"product_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        # Order fields\n",
    "        orders_stream.order_id,\n",
    "        orders_stream.customer_id,\n",
    "        orders_stream.customer_name,\n",
    "        orders_stream.location,\n",
    "        orders_stream.order_status,\n",
    "        orders_stream.payment_method,\n",
    "        orders_stream.quantity,\n",
    "        orders_stream.discount_pct,\n",
    "        orders_stream.total_amount,\n",
    "        \n",
    "        # Parse order timestamp if it's a string\n",
    "        when(col(\"order_timestamp_parsed\").isNotNull(), \n",
    "             col(\"order_timestamp_parsed\"))\n",
    "        .otherwise(to_timestamp(orders_stream.order_timestamp))\n",
    "        .alias(\"order_timestamp\"),\n",
    "        \n",
    "        # Product fields (from join)\n",
    "        products_df.product_id.alias(\"product_id_joined\"),\n",
    "        products_df.product_name,\n",
    "        products_df.category,\n",
    "        products_df.brand,\n",
    "        products_df.base_price,\n",
    "        products_df.unit_price,\n",
    "        \n",
    "        # Metadata\n",
    "        orders_stream.event_time,\n",
    "        orders_stream.kafka_offset,\n",
    "        orders_stream.partition_id,\n",
    "        orders_stream.bronze_timestamp\n",
    "    )\n",
    "    # Add calculated fields\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_timestamp\")))\n",
    "    .withColumn(\"order_hour\", hour(col(\"order_timestamp\")))\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"order_timestamp\")))\n",
    "    .withColumn(\"line_total\", col(\"quantity\") * col(\"unit_price\"))\n",
    "    .withColumn(\"discount_amount\", \n",
    "                col(\"total_amount\") - (col(\"quantity\") * col(\"unit_price\")))\n",
    "    .withColumn(\"silver_timestamp\", current_timestamp())\n",
    "    # Clean up duplicate product_id column\n",
    "    .drop(\"product_id_joined\")\n",
    ")\n",
    "\n",
    "print(\"‚úì Streaming join configured\")\n",
    "print(\"  Join Type: LEFT JOIN\")\n",
    "print(\"  Join Key: product_id\")\n",
    "print(\"  Mode: Stream-to-Static\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Add Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Add data quality flags\n",
    "order_details_stream_with_quality = (order_details_stream\n",
    "    .withColumn(\"has_product_info\", col(\"product_name\").isNotNull())\n",
    "    .withColumn(\"is_valid_quantity\", col(\"quantity\") > 0)\n",
    "    .withColumn(\"is_valid_amount\", col(\"total_amount\") > 0)\n",
    "    .withColumn(\"data_quality_score\", \n",
    "                (col(\"has_product_info\").cast(\"int\") + \n",
    "                 col(\"is_valid_quantity\").cast(\"int\") + \n",
    "                 col(\"is_valid_amount\").cast(\"int\")))\n",
    ")\n",
    "\n",
    "print(\"‚úì Data quality checks added\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: Write to Silver Table (Streaming)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING SILVER LAYER STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Write enriched data to Silver table\n",
    "silver_query = (order_details_stream_with_quality\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", silver_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "   # .trigger(processingTime=\"15 seconds\")  # Process every 15 seconds\n",
    "     .trigger(once=True) # for testing\n",
    "    .toTable(silver_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Silver stream started\")\n",
    "print(f\"  Query ID: {silver_query.id}\")\n",
    "print(f\"  Target: {silver_table}\")\n",
    "print(f\"  Checkpoint: {silver_checkpoint}\")\n",
    "print(f\"  Trigger: 15 seconds\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Streaming Query\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVE STREAMING QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nQuery ID: {stream.id}\")\n",
    "    print(f\"  Name: {stream.name if stream.name else 'unnamed'}\")\n",
    "    print(f\"  Status: {stream.status['message']}\")\n",
    "    print(f\"  Is Active: {stream.isActive}\")\n",
    "    \n",
    "    if stream.recentProgress:\n",
    "        latest = stream.recentProgress[-1]\n",
    "        print(f\"  Recent Progress:\")\n",
    "        print(f\"    - Batch: {latest.get('batchId', 'N/A')}\")\n",
    "        print(f\"    - Input Rows: {latest.get('numInputRows', 0)}\")\n",
    "        print(f\"    - Processing Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Wait and Monitor (Run for 30 seconds)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONITORING SILVER STREAM FOR 30 SECONDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        silver_count = spark.table(silver_table).count()\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Silver records: {silver_count}\")\n",
    "        \n",
    "        # Show data quality metrics\n",
    "        quality_stats = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                SUM(CASE WHEN has_product_info THEN 1 ELSE 0 END) as with_product_info,\n",
    "                SUM(CASE WHEN NOT has_product_info THEN 1 ELSE 0 END) as missing_product_info,\n",
    "                AVG(data_quality_score) as avg_quality_score\n",
    "            FROM {silver_table}\n",
    "        \"\"\").first()\n",
    "        \n",
    "        print(f\"    - With product info: {quality_stats['with_product_info']}\")\n",
    "        print(f\"    - Missing product info: {quality_stats['missing_product_info']}\")\n",
    "        print(f\"    - Avg quality score: {quality_stats['avg_quality_score']:.2f}/3.0\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚è±Ô∏è  {(i+1)*5}s - Silver table not created yet or error: {e}\")\n",
    "\n",
    "print(\"\\n‚úì Monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Silver Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SILVER LAYER VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    silver_df = spark.table(silver_table)\n",
    "    total_records = silver_df.count()\n",
    "    \n",
    "    print(f\"\\nüìä Silver Table: {silver_table}\")\n",
    "    print(f\"   Total Records: {total_records}\")\n",
    "    \n",
    "    if total_records > 0:\n",
    "        # Data quality summary\n",
    "        print(\"\\n   Data Quality Summary:\")\n",
    "        quality_df = silver_df.groupBy(\"has_product_info\").count().collect()\n",
    "        for row in quality_df:\n",
    "            status = \"‚úì Complete\" if row['has_product_info'] else \"‚ö†Ô∏è Missing Product\"\n",
    "            print(f\"     {status}: {row['count']} records\")\n",
    "        \n",
    "        print(\"\\n   Latest 10 enriched orders:\")\n",
    "        display(\n",
    "            silver_df\n",
    "            .select(\"order_id\", \"customer_name\", \"location\", \"product_name\", \"brand\", \n",
    "                    \"category\", \"quantity\", \"unit_price\", \"total_amount\", \"order_timestamp\",\n",
    "                    \"data_quality_score\")\n",
    "            .orderBy(desc(\"silver_timestamp\"))\n",
    "            .limit(10)\n",
    "        )\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Silver table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Real-Time Analysis Queries\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REAL-TIME SILVER LAYER ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    silver_df = spark.table(silver_table)\n",
    "    \n",
    "    # 1. Orders by Brand (Real-time)\n",
    "    print(\"\\n1. Orders by Brand:\")\n",
    "    display(\n",
    "        silver_df\n",
    "        .groupBy(\"brand\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Analysis not available yet: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 2. Orders by Category\n",
    "try:\n",
    "    print(\"\\n2. Orders by Category:\")\n",
    "    display(\n",
    "        silver_df\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            sum(\"quantity\").alias(\"total_quantity\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 3. Top Customers (Real-time)\n",
    "try:\n",
    "    print(\"\\n3. Top Customers by Revenue:\")\n",
    "    display(\n",
    "        silver_df\n",
    "        .groupBy(\"customer_id\", \"customer_name\", \"location\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            sum(\"total_amount\").alias(\"total_spent\"),\n",
    "            avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_spent\"))\n",
    "        .limit(20)\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 4. Hourly Order Trends\n",
    "try:\n",
    "    print(\"\\n4. Hourly Order Trends:\")\n",
    "    display(\n",
    "        silver_df\n",
    "        .groupBy(\"order_hour\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            sum(\"total_amount\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .orderBy(\"order_hour\")\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Create Streaming Dashboard View\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create a view for real-time dashboard\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW silver_dashboard AS\n",
    "SELECT \n",
    "    order_date,\n",
    "    order_hour,\n",
    "    category,\n",
    "    brand,\n",
    "    location,\n",
    "    COUNT(*) as order_count,\n",
    "    SUM(total_amount) as revenue,\n",
    "    AVG(total_amount) as avg_order_value,\n",
    "    SUM(quantity) as total_items_sold,\n",
    "    MAX(silver_timestamp) as last_updated\n",
    "FROM {silver_table}\n",
    "GROUP BY order_date, order_hour, category, brand, location\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Created real-time dashboard view: silver_dashboard\")\n",
    "\n",
    "# Sample query\n",
    "print(\"\\nSample Dashboard Query - Today's Performance:\")\n",
    "display(spark.sql(\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    brand,\n",
    "    SUM(order_count) as total_orders,\n",
    "    SUM(revenue) as total_revenue,\n",
    "    MAX(last_updated) as last_update\n",
    "FROM silver_dashboard\n",
    "WHERE order_date = CURRENT_DATE()\n",
    "GROUP BY category, brand\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Alternative: Stream-to-Stream Join (If Products Also Streaming)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment this section if you want to use stream-to-stream join\n",
    "\"\"\"\n",
    "print(\"=\"*70)\n",
    "print(\"ALTERNATIVE: STREAM-TO-STREAM JOIN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read products as stream\n",
    "products_stream = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .table(bronze_products_table)\n",
    "    .withWatermark(\"bronze_timestamp\", \"1 minute\")\n",
    ")\n",
    "\n",
    "# Stream-to-Stream join with watermarks\n",
    "order_details_stream_v2 = (orders_stream\n",
    "    .join(\n",
    "        products_stream,\n",
    "        on=orders_stream.product_id == products_stream.product_id,\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        orders_stream.order_id,\n",
    "        orders_stream.customer_id,\n",
    "        # ... other fields\n",
    "    )\n",
    "    # ... rest of transformation\n",
    ")\n",
    "\n",
    "# Write stream\n",
    "silver_query_v2 = (order_details_stream_v2\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{silver_checkpoint}_v2\")\n",
    "    .trigger(processingTime=\"15 seconds\")\n",
    "    .toTable(f\"{silver_table}_v2\")\n",
    ")\n",
    "\n",
    "print(\"‚úì Stream-to-stream join configured\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop Streams (Run when done)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop all streams\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"STOPPING SILVER LAYER STREAMS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    if \"silver\" in stream.id.lower() or \"order_details\" in str(stream.name).lower():\n",
    "        print(f\"\\nStopping: {stream.id}\")\n",
    "        stream.stop()\n",
    "        print(f\"  ‚úì Stopped\")\n",
    "\n",
    "print(\"\\n‚úì Silver streams stopped\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì SILVER LAYER STREAMING SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Get current state\n",
    "    orders_count = spark.table(bronze_orders_table).count()\n",
    "    products_count = spark.table(bronze_products_table).count()\n",
    "    silver_count = spark.table(silver_table).count()\n",
    "    \n",
    "    # Data quality check\n",
    "    quality_check = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            SUM(CASE WHEN has_product_info THEN 1 ELSE 0 END) as complete,\n",
    "            SUM(CASE WHEN NOT has_product_info THEN 1 ELSE 0 END) as incomplete\n",
    "        FROM {silver_table}\n",
    "    \"\"\").first()\n",
    "    \n",
    "    print(f\"\\nCurrent State:\")\n",
    "    print(f\"  Bronze Orders: {orders_count} records\")\n",
    "    print(f\"  Bronze Products: {products_count} records\")\n",
    "    print(f\"  Silver Order Details: {silver_count} records\")\n",
    "    \n",
    "    print(f\"\\nData Quality:\")\n",
    "    if quality_check:\n",
    "        success_rate = (quality_check['complete'] / quality_check['total'] * 100) if quality_check['total'] > 0 else 0\n",
    "        print(f\"  Complete records: {quality_check['complete']}\")\n",
    "        print(f\"  Incomplete records: {quality_check['incomplete']}\")\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nStreaming Status:\")\n",
    "    print(f\"  Active Queries: {len(spark.streams.active)}\")\n",
    "    \n",
    "    for stream in spark.streams.active:\n",
    "        print(f\"    - {stream.id}: {stream.status['message']}\")\n",
    "    \n",
    "    print(f\"\\nData Flow:\")\n",
    "    print(f\"  Bronze Orders (streaming)\")\n",
    "    print(f\"    + Bronze Products (static)\")\n",
    "    print(f\"    ‚Üì LEFT JOIN on product_id\")\n",
    "    print(f\"  Silver Order Details (enriched)\")\n",
    "    print(f\"    ‚Ä¢ Full order + product info\")\n",
    "    print(f\"    ‚Ä¢ Calculated fields\")\n",
    "    print(f\"    ‚Ä¢ Data quality flags\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Next Steps:\")\n",
    "    print(\"  1. Monitor streams using 'Monitor Streaming Query' cell\")\n",
    "    print(\"  2. Run Gold Layer notebook for business aggregations\")\n",
    "    print(\"  3. Use dashboard view for real-time analytics\")\n",
    "    print(\"  4. Stop streams when done using 'Stop Streams' cell\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Tables not fully populated yet. Wait for streams to process data.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
