{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f87a72f-e55e-4542-9b83-259b53465cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./02config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeebcc6-d4f9-4df7-99ce-5f34119177a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Define Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event payload schema\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"âœ“ Event schema defined\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 1: Read from Event Hub and Parse Events\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING EVENT HUB STREAMING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read streaming data from Event Hub\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON payload and add metadata\n",
    "parsed_stream = (raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"timestamp\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "    .withColumn(\"order_timestamp_parsed\", to_timestamp(col(\"order_timestamp\")))\n",
    ")\n",
    "\n",
    "print(\"âœ“ Stream parsing configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 2: Write Orders to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ORDERS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select orders columns\n",
    "orders_stream = parsed_stream.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"location\"),\n",
    "    col(\"product_id\"),  # FK to products\n",
    "    col(\"order_status\"),\n",
    "    col(\"payment_method\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"discount_pct\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"order_timestamp\"),\n",
    "    col(\"order_timestamp_parsed\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"kafka_offset\"),\n",
    "    col(\"partition_id\"),\n",
    "    col(\"bronze_timestamp\")\n",
    ")\n",
    "\n",
    "# Write orders stream to Delta table\n",
    "orders_query = (orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "   # .trigger(processingTime=\"10 seconds\")  # Micro-batch every 10 seconds\n",
    "     .trigger(once=True) # for testing\n",
    "    .toTable(bronze_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Orders stream started\")\n",
    "print(f\"  Query ID: {orders_query.id}\")\n",
    "print(f\"  Target: {bronze_orders_table}\")\n",
    "print(f\"  Checkpoint: {orders_checkpoint}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream 3: Deduplicate and Write Products to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PRODUCTS STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select and deduplicate products\n",
    "products_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"base_price\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"bronze_timestamp\")\n",
    "    )\n",
    "    .dropDuplicates([\"product_id\"])  # Keep only unique products per batch\n",
    ")\n",
    "\n",
    "# Write products stream to Delta table with merge logic\n",
    "products_query = (products_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", products_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    #.trigger(processingTime=\"10 seconds\")\n",
    "    .trigger(once=True) # for testing\n",
    "    .foreachBatch(lambda batch_df, batch_id: upsert_products(batch_df, batch_id))\n",
    "    .start()\n",
    ")\n",
    "\n",
    "def upsert_products(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Upsert products to avoid duplicates across batches\n",
    "    \"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    # Create temp view\n",
    "    batch_df.createOrReplaceTempView(\"products_batch\")\n",
    "    \n",
    "    # Merge logic\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {bronze_products_table} target\n",
    "    USING products_batch source\n",
    "    ON target.product_id = source.product_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.product_name = source.product_name,\n",
    "            target.category = source.category,\n",
    "            target.brand = source.brand,\n",
    "            target.base_price = source.base_price,\n",
    "            target.unit_price = source.unit_price,\n",
    "            target.bronze_timestamp = source.bronze_timestamp\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (product_id, product_name, category, brand, base_price, unit_price, bronze_timestamp)\n",
    "        VALUES (source.product_id, source.product_name, source.category, source.brand, \n",
    "                source.base_price, source.unit_price, source.bronze_timestamp)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute merge\n",
    "    try:\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"  Batch {batch_id}: Upserted {batch_df.count()} products\")\n",
    "    except Exception as e:\n",
    "        # If table doesn't exist, create it\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "            batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_products_table)\n",
    "            print(f\"  Batch {batch_id}: Created table and inserted {batch_df.count()} products\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"âœ“ Products stream started\")\n",
    "print(f\"  Query ID: {products_query.id}\")\n",
    "print(f\"  Target: {bronze_products_table}\")\n",
    "print(f\"  Checkpoint: {products_checkpoint}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Streaming Queries\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVE STREAMING QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nQuery ID: {stream.id}\")\n",
    "    print(f\"  Name: {stream.name if stream.name else 'unnamed'}\")\n",
    "    print(f\"  Status: {stream.status['message']}\")\n",
    "    print(f\"  Is Active: {stream.isActive}\")\n",
    "    \n",
    "    if stream.recentProgress:\n",
    "        latest = stream.recentProgress[-1]\n",
    "        print(f\"  Recent Progress:\")\n",
    "        print(f\"    - Batch: {latest.get('batchId', 'N/A')}\")\n",
    "        print(f\"    - Input Rows: {latest.get('numInputRows', 0)}\")\n",
    "        print(f\"    - Processing Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Wait and Monitor (Run for 30 seconds)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONITORING STREAMS FOR 30 SECONDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Check orders table\n",
    "    try:\n",
    "        orders_count = spark.table(bronze_orders_table).count()\n",
    "        print(f\"\\nâ±ï¸  {(i+1)*5}s - Orders: {orders_count} records\")\n",
    "    except:\n",
    "        print(f\"\\nâ±ï¸  {(i+1)*5}s - Orders table not created yet\")\n",
    "    \n",
    "    # Check products table\n",
    "    try:\n",
    "        products_count = spark.table(bronze_products_table).count()\n",
    "        print(f\"    Products: {products_count} records\")\n",
    "    except:\n",
    "        print(f\"    Products table not created yet\")\n",
    "\n",
    "print(\"\\nâœ“ Monitoring complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Bronze Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRONZE LAYER VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Orders table\n",
    "try:\n",
    "    orders_df = spark.table(bronze_orders_table)\n",
    "    orders_count = orders_df.count()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Orders Table: {bronze_orders_table}\")\n",
    "    print(f\"   Total Records: {orders_count}\")\n",
    "    \n",
    "    if orders_count > 0:\n",
    "        print(\"\\n   Latest 10 orders:\")\n",
    "        display(orders_df.orderBy(desc(\"bronze_timestamp\")).limit(10))\n",
    "        \n",
    "        print(\"\\n   Orders by Location:\")\n",
    "        display(\n",
    "            orders_df.groupBy(\"location\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"order_count\"),\n",
    "                sum(\"total_amount\").alias(\"total_revenue\")\n",
    "            )\n",
    "            .orderBy(desc(\"order_count\"))\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  Orders table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Products table\n",
    "try:\n",
    "    products_df = spark.table(bronze_products_table)\n",
    "    products_count = products_df.count()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Products Table: {bronze_products_table}\")\n",
    "    print(f\"   Total Records: {products_count}\")\n",
    "    \n",
    "    if products_count > 0:\n",
    "        print(\"\\n   All products:\")\n",
    "        display(products_df.orderBy(\"product_id\"))\n",
    "        \n",
    "        print(\"\\n   Products by Category:\")\n",
    "        display(\n",
    "            products_df.groupBy(\"category\", \"brand\")\n",
    "            .count()\n",
    "            .orderBy(\"category\", \"brand\")\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  Products table not available: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stream Management\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Option 1: Keep streams running\n",
    "print(\"\\nðŸ’¡ Streams are running continuously\")\n",
    "print(\"   They will process new events as they arrive in Event Hub\")\n",
    "print(\"\\n   To stop streams, run the next cell\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop All Streams (Run when done)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop all streams\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"STOPPING ALL STREAMING QUERIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nStopping: {stream.id}\")\n",
    "    stream.stop()\n",
    "    print(f\"  âœ“ Stopped\")\n",
    "\n",
    "print(\"\\nâœ“ All streams stopped\")\n",
    "print(\"\\nNote: You can restart streams by re-running the stream cells above\")\n",
    "\"\"\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ BRONZE LAYER STREAMING SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    orders_count = spark.table(bronze_orders_table).count()\n",
    "    products_count = spark.table(bronze_products_table).count()\n",
    "    \n",
    "    print(f\"\\nCurrent State:\")\n",
    "    print(f\"  Orders: {orders_count} records\")\n",
    "    print(f\"  Products: {products_count} records\")\n",
    "    print(f\"\\nStreaming Status:\")\n",
    "    print(f\"  Active Queries: {len(spark.streams.active)}\")\n",
    "    \n",
    "    for stream in spark.streams.active:\n",
    "        print(f\"    - {stream.id}: {stream.status['message']}\")\n",
    "    \n",
    "    print(f\"\\nData Flow:\")\n",
    "    print(f\"  Event Hub ({eh_name})\")\n",
    "    print(f\"    â†“ Kafka Streaming\")\n",
    "    print(f\"  Parsed Events\")\n",
    "    print(f\"    â†“ Split & Transform\")\n",
    "    print(f\"  Bronze Tables\")\n",
    "    print(f\"    â€¢ Orders (with product_id FK)\")\n",
    "    print(f\"    â€¢ Products (deduplicated)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Next Steps:\")\n",
    "    print(\"  1. Monitor streams using 'Monitor Streaming Queries' cell\")\n",
    "    print(\"  2. Run Silver Layer notebook to join Orders with Products\")\n",
    "    print(\"  3. Stop streams when done using 'Stop All Streams' cell\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  Tables not created yet. Wait for streams to process data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768234df-a2ff-43e5-96d0-6a57107cd12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer - Orders Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Event Hub Configuration\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STREAMING CONFIGURATION - ORDERS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Event Hub: {eh_name}\")\n",
    "print(f\"Orders Table: {bronze_orders_table}\")\n",
    "print(f\"Checkpoint: {orders_checkpoint}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Schema Definition\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read from Event Hub\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read streaming data\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON payload\n",
    "parsed_stream = (raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"timestamp\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    "    .withColumn(\"order_timestamp_parsed\", to_timestamp(col(\"order_timestamp\")))\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write Orders Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Select orders columns\n",
    "orders_stream = parsed_stream.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"location\"),\n",
    "    col(\"product_id\"),\n",
    "    col(\"order_status\"),\n",
    "    col(\"payment_method\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"discount_pct\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"order_timestamp\"),\n",
    "    col(\"order_timestamp_parsed\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"kafka_offset\"),\n",
    "    col(\"partition_id\"),\n",
    "    col(\"bronze_timestamp\")\n",
    ")\n",
    "\n",
    "# Write to Delta table\n",
    "orders_query = (orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    # .trigger(once=True)  # For testing\n",
    "    .toTable(bronze_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Orders stream started: {orders_query.id}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Query: {stream.id}\")\n",
    "    print(f\"Status: {stream.status['message']}\")\n",
    "    if stream.recentProgress:\n",
    "        latest = stream.recentProgress[-1]\n",
    "        print(f\"Batch: {latest.get('batchId', 'N/A')}\")\n",
    "        print(f\"Rows: {latest.get('numInputRows', 0)}\")\n",
    "        print(f\"Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Orders Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "orders_df = spark.table(bronze_orders_table)\n",
    "print(f\"Total Orders: {orders_df.count()}\")\n",
    "\n",
    "display(orders_df.orderBy(desc(\"bronze_timestamp\")).limit(10))\n",
    "\n",
    "# Orders by location\n",
    "display(\n",
    "    orders_df.groupBy(\"location\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\")\n",
    "    )\n",
    "    .orderBy(desc(\"order_count\"))\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop\n",
    "# for stream in spark.streams.active:\n",
    "#     stream.stop()\n",
    "#     print(f\"Stopped: {stream.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fed30c31-8aa9-46f1-9101-2de3c1079eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Schema Definition\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"discount_pct\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"base_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read from Event Hub\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read streaming data\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse JSON payload\n",
    "parsed_stream = (raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"json_value\"), event_schema))\n",
    "    .select(\n",
    "        col(\"parsed_data.*\"),\n",
    "        col(\"timestamp\").alias(\"event_time\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"partition\").alias(\"partition_id\")\n",
    "    )\n",
    "    .withColumn(\"bronze_timestamp\", current_timestamp())\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Upsert Function\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def upsert_products(batch_df, batch_id):\n",
    "    \"\"\"Upsert products to avoid duplicates (SCD Type 1)\"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    batch_df.createOrReplaceTempView(\"products_batch\")\n",
    "    \n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {bronze_products_table} target\n",
    "    USING products_batch source\n",
    "    ON target.product_id = source.product_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.product_name = source.product_name,\n",
    "            target.category = source.category,\n",
    "            target.brand = source.brand,\n",
    "            target.base_price = source.base_price,\n",
    "            target.unit_price = source.unit_price,\n",
    "            target.bronze_timestamp = source.bronze_timestamp\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (product_id, product_name, category, brand, base_price, unit_price, bronze_timestamp)\n",
    "        VALUES (source.product_id, source.product_name, source.category, source.brand, \n",
    "                source.base_price, source.unit_price, source.bronze_timestamp)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"Batch {batch_id}: Upserted {batch_df.count()} products\")\n",
    "    except Exception as e:\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "            batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_products_table)\n",
    "            print(f\"Batch {batch_id}: Created table, inserted {batch_df.count()} products\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write Products Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Select and deduplicate products\n",
    "products_stream = (parsed_stream\n",
    "    .select(\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"brand\"),\n",
    "        col(\"base_price\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"bronze_timestamp\")\n",
    "    )\n",
    "    .dropDuplicates([\"product_id\"])\n",
    ")\n",
    "\n",
    "# Write with upsert logic\n",
    "products_query = (products_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", products_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    # .trigger(once=True)  # For testing\n",
    "    .foreachBatch(upsert_products)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Products stream started: {products_query.id}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitor Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Query: {stream.id}\")\n",
    "    print(f\"Status: {stream.status['message']}\")\n",
    "    if stream.recentProgress:\n",
    "        latest = stream.recentProgress[-1]\n",
    "        print(f\"Batch: {latest.get('batchId', 'N/A')}\")\n",
    "        print(f\"Rows: {latest.get('numInputRows', 0)}\")\n",
    "        print(f\"Rate: {latest.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Products Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "products_df = spark.table(bronze_products_table)\n",
    "print(f\"Total Unique Products: {products_df.count()}\")\n",
    "\n",
    "display(products_df.orderBy(\"product_id\"))\n",
    "\n",
    "# Products by category\n",
    "display(\n",
    "    products_df.groupBy(\"category\", \"brand\")\n",
    "    .count()\n",
    "    .orderBy(\"category\", \"brand\")\n",
    ")\n",
    "\n",
    "# Verify no duplicates\n",
    "duplicate_check = (products_df\n",
    "    .groupBy(\"product_id\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    ")\n",
    "print(f\"Duplicate product_ids: {duplicate_check.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Stop Stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Uncomment to stop\n",
    "# for stream in spark.streams.active:\n",
    "#     stream.stop()\n",
    "#     print(f\"Stopped: {stream.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d27408-3ebb-4895-b57e-a077af9a835b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Quick Event Hub Test\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Event Hub Configuration\n",
    "eh_namespace = \"evhns-natraining.servicebus.windows.net\"\n",
    "eh_name = \"evh-natraining-biju\"\n",
    "keyvault_scope = \"dbx-ss-kv-natraining-2\"\n",
    "secret_name = \"evh-natraining-read-write\"\n",
    "shared_access_key_name = \"SharedAccessKeyToSendAndListen\"\n",
    "\n",
    "sas_key = dbutils.secrets.get(scope=keyvault_scope, key=secret_name)\n",
    "\n",
    "# Kafka options\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}:9093\",\n",
    "    \"subscribe\": eh_name,\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://{eh_namespace}/;SharedAccessKeyName={shared_access_key_name};SharedAccessKey={sas_key};\";',\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "print(f\"Connecting to Event Hub: {eh_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read events\n",
    "events = spark.read.format(\"kafka\").options(**KAFKA_OPTIONS).load()\n",
    "\n",
    "print(f\"âœ“ Connected! Total events: {events.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# View raw events\n",
    "display(events.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Parse JSON payload\n",
    "parsed = (events\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp\", \"offset\", \"partition\")\n",
    ")\n",
    "\n",
    "display(parsed.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Extract specific fields (adjust based on your JSON structure)\n",
    "details = (parsed\n",
    "    .select(\n",
    "        col(\"timestamp\"),\n",
    "        col(\"offset\"),\n",
    "        get_json_object(col(\"json_value\"), \"$.order_id\").alias(\"order_id\"),\n",
    "        get_json_object(col(\"json_value\"), \"$.customer_name\").alias(\"customer_name\"),\n",
    "        get_json_object(col(\"json_value\"), \"$.product_name\").alias(\"product_name\"),\n",
    "        get_json_object(col(\"json_value\"), \"$.location\").alias(\"location\"),\n",
    "        get_json_object(col(\"json_value\"), \"$.total_amount\").alias(\"total_amount\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(details.limit(20))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Test streaming\n",
    "stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "query = (stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\")\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"Streaming query started: {query.id}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Stop stream\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "print(\"âœ“ Streams stopped\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5534691258377251,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
