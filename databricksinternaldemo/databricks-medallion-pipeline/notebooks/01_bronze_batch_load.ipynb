{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05b4fef-f74e-4594-b150-e27be9873050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b48a8d6-73d6-49fb-9686-1e01c449518d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer - Batch Load\n",
    "# MAGIC Extract data from Azure SQL and load to Bronze using Auto Loader\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, input_file_name\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Create Bronze Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "create_schema_if_not_exists(CATALOG, SCHEMA_BRONZE, \"Bronze layer - raw data from Azure SQL\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Extract from Azure SQL to Staging\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_table_to_staging(table_name: str, table_config: dict):\n",
    "    \"\"\"Extract table from Azure SQL and save to staging\"\"\"\n",
    "    \n",
    "    source_schema = table_config['source_schema']\n",
    "    staging_path = table_config['staging_path']\n",
    "    watermark_column = table_config['watermark_column']\n",
    "    jdbc_table = f\"{source_schema}.{table_name}\"\n",
    "    \n",
    "    logger.info(f\"Extracting {jdbc_table}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read from Azure SQL with partitioning\n",
    "        df = (\n",
    "            spark.read\n",
    "            .jdbc(\n",
    "                url=JDBC_URL,\n",
    "                table=jdbc_table,\n",
    "                properties=CONNECTION_PROPS,\n",
    "                column=watermark_column,\n",
    "                lowerBound=JDBC_CONFIG['lower_bound'],\n",
    "                upperBound=JDBC_CONFIG['upper_bound'],\n",
    "                numPartitions=JDBC_CONFIG['num_partitions']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add extraction metadata\n",
    "        df = (# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer - Batch Load\n",
    "# MAGIC Extract data from Azure SQL and load to Bronze using Auto Loader\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, input_file_name\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Create Bronze Schema\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "create_schema_if_not_exists(CATALOG, SCHEMA_BRONZE, \"Bronze layer - raw data from Azure SQL\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Extract from Azure SQL to Staging\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_table_to_staging(table_name: str, table_config: dict):\n",
    "    \"\"\"Extract table from Azure SQL and save to staging\"\"\"\n",
    "    \n",
    "    source_schema = table_config['source_schema']\n",
    "    staging_path = table_config['staging_path']\n",
    "    watermark_column = table_config['watermark_column']\n",
    "    jdbc_table = f\"{source_schema}.{table_name}\"\n",
    "    \n",
    "    logger.info(f\"Extracting {jdbc_table}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read from Azure SQL\n",
    "        # Method 1: Simple read without partitioning (good for smaller tables)\n",
    "        df = (\n",
    "            spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"url\", JDBC_URL)\n",
    "            .option(\"dbtable\", jdbc_table)\n",
    "            .option(\"user\", CONNECTION_PROPS[\"user\"])\n",
    "            .option(\"password\", CONNECTION_PROPS[\"password\"])\n",
    "            .option(\"driver\", CONNECTION_PROPS[\"driver\"])\n",
    "            .option(\"fetchsize\", JDBC_CONFIG['fetch_size'])\n",
    "            .load()\n",
    "        )\n",
    "        \n",
    "        # Add extraction metadata\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"_extracted_at\", current_timestamp())\n",
    "            .withColumn(\"_source_system\", lit(\"azure_sql\"))\n",
    "            .withColumn(\"_source_table\", lit(jdbc_table))\n",
    "        )\n",
    "        \n",
    "        # Get count before writing\n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Write to staging as Parquet\n",
    "        df.write.mode(\"overwrite\").parquet(staging_path)\n",
    "        \n",
    "        logger.info(f\"✓ Extracted {record_count:,} records from {jdbc_table}\")\n",
    "        \n",
    "        return {\"status\": \"SUCCESS\", \"records\": record_count}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Failed to extract {jdbc_table}: {str(e)}\")\n",
    "        return {\"status\": \"FAILED\", \"error\": str(e)}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Alternative: Extract with partitioning for large tables\n",
    "def extract_table_to_staging_partitioned(table_name: str, table_config: dict):\n",
    "    \"\"\"Extract large table from Azure SQL with partitioning\"\"\"\n",
    "    \n",
    "    source_schema = table_config['source_schema']\n",
    "    staging_path = table_config['staging_path']\n",
    "    primary_key = table_config['primary_key']\n",
    "    jdbc_table = f\"{source_schema}.{table_name}\"\n",
    "    \n",
    "    # Use primary key for partitioning if it's a single numeric column\n",
    "    if isinstance(primary_key, str):\n",
    "        partition_column = primary_key\n",
    "    else:\n",
    "        # If composite key, use first column or don't partition\n",
    "        logger.warning(f\"Composite primary key for {table_name}, reading without partitioning\")\n",
    "        return extract_table_to_staging(table_name, table_config)\n",
    "    \n",
    "    logger.info(f\"Extracting {jdbc_table} with partitioning on {partition_column}...\")\n",
    "    \n",
    "    try:\n",
    "        # First, get min and max values for the partition column\n",
    "        bounds_query = f\"(SELECT MIN({partition_column}) as min_val, MAX({partition_column}) as max_val FROM {jdbc_table}) as bounds\"\n",
    "        \n",
    "        bounds_df = (\n",
    "            spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"url\", JDBC_URL)\n",
    "            .option(\"dbtable\", bounds_query)\n",
    "            .option(\"user\", CONNECTION_PROPS[\"user\"])\n",
    "            .option(\"password\", CONNECTION_PROPS[\"password\"])\n",
    "            .option(\"driver\", CONNECTION_PROPS[\"driver\"])\n",
    "            .load()\n",
    "        )\n",
    "        \n",
    "        bounds = bounds_df.collect()[0]\n",
    "        min_val = bounds['min_val']\n",
    "        max_val = bounds['max_val']\n",
    "        \n",
    "        logger.info(f\"Partition bounds: {min_val} to {max_val}\")\n",
    "        \n",
    "        # Read with partitioning\n",
    "        df = (\n",
    "            spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"url\", JDBC_URL)\n",
    "            .option(\"dbtable\", jdbc_table)\n",
    "            .option(\"user\", CONNECTION_PROPS[\"user\"])\n",
    "            .option(\"password\", CONNECTION_PROPS[\"password\"])\n",
    "            .option(\"driver\", CONNECTION_PROPS[\"driver\"])\n",
    "            .option(\"fetchsize\", JDBC_CONFIG['fetch_size'])\n",
    "            .option(\"partitionColumn\", partition_column)\n",
    "            .option(\"lowerBound\", str(min_val))\n",
    "            .option(\"upperBound\", str(max_val))\n",
    "            .option(\"numPartitions\", str(JDBC_CONFIG['num_partitions']))\n",
    "            .load()\n",
    "        )\n",
    "        \n",
    "        # Add extraction metadata\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"_extracted_at\", current_timestamp())\n",
    "            .withColumn(\"_source_system\", lit(\"azure_sql\"))\n",
    "            .withColumn(\"_source_table\", lit(jdbc_table))\n",
    "        )\n",
    "        \n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Write to staging as Parquet\n",
    "        df.write.mode(\"overwrite\").parquet(staging_path)\n",
    "        \n",
    "        logger.info(f\"✓ Extracted {record_count:,} records from {jdbc_table}\")\n",
    "        \n",
    "        return {\"status\": \"SUCCESS\", \"records\": record_count}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Failed to extract {jdbc_table}: {str(e)}\")\n",
    "        # Fallback to non-partitioned read\n",
    "        logger.info(f\"Retrying without partitioning...\")\n",
    "        return extract_table_to_staging(table_name, table_config)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Extract all tables\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTING DATA FROM AZURE SQL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "extraction_results = {}\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Determine which tables to partition\n",
    "large_tables = ['SalesOrderHeader', 'SalesOrderDetail']  # Tables that benefit from partitioning\n",
    "\n",
    "for table_name, table_config in BRONZE_TABLES.items():\n",
    "    # Use partitioned extraction for large tables\n",
    "    if table_name in large_tables:\n",
    "        result = extract_table_to_staging_partitioned(table_name, table_config)\n",
    "    else:\n",
    "        result = extract_table_to_staging(table_name, table_config)\n",
    "    \n",
    "    extraction_results[table_name] = result\n",
    "\n",
    "extraction_duration = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "successful = 0\n",
    "failed = 0\n",
    "total_records = 0\n",
    "\n",
    "for table_name, result in extraction_results.items():\n",
    "    status_symbol = \"✓\" if result['status'] == 'SUCCESS' else \"✗\"\n",
    "    if result['status'] == 'SUCCESS':\n",
    "        print(f\"{status_symbol} {table_name}: {result['records']:,} records\")\n",
    "        successful += 1\n",
    "        total_records += result['records']\n",
    "    else:\n",
    "        print(f\"{status_symbol} {table_name}: {result['error']}\")\n",
    "        failed += 1\n",
    "\n",
    "print(f\"\\nSuccessful: {successful}/{len(BRONZE_TABLES)}\")\n",
    "print(f\"Failed: {failed}/{len(BRONZE_TABLES)}\")\n",
    "print(f\"Total Records Extracted: {total_records:,}\")\n",
    "print(f\"Total Time: {extraction_duration:.2f} seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Load to Bronze using Auto Loader\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_table_to_bronze(table_name: str, table_config: dict):\n",
    "    \"\"\"Load table from staging to Bronze using Auto Loader\"\"\"\n",
    "    \n",
    "    staging_path = table_config['staging_path']\n",
    "    checkpoint_path = get_checkpoint_path('bronze', table_name)\n",
    "    schema_location = get_schema_path('bronze', table_name)\n",
    "    \n",
    "    bronze_table = get_table_path(SCHEMA_BRONZE, f\"bronze_{table_name.lower()}\")\n",
    "    \n",
    "    logger.info(f\"Loading {table_name} to {bronze_table}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read using Auto Loader\n",
    "        df = (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", AUTOLOADER_CONFIG['format'])\n",
    "            .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", AUTOLOADER_CONFIG['schema_evolution'])\n",
    "            .load(staging_path)\n",
    "        )\n",
    "        \n",
    "        # Add Bronze metadata\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"_bronze_load_timestamp\", current_timestamp())\n",
    "            .withColumn(\"_source_file\", input_file_name())\n",
    "        )\n",
    "        \n",
    "        # Write to Bronze using trigger(availableNow=True) for batch processing\n",
    "        query = (\n",
    "            df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", checkpoint_path)\n",
    "            .trigger(availableNow=True)\n",
    "            .toTable(bronze_table)\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        # Get record count\n",
    "        record_count = spark.table(bronze_table).count()\n",
    "        logger.info(f\"✓ Loaded {record_count:,} records to {bronze_table}\")\n",
    "        \n",
    "        return {\"status\": \"SUCCESS\", \"records\": record_count}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Failed to load {table_name}: {str(e)}\")\n",
    "        return {\"status\": \"FAILED\", \"error\": str(e)}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Load all tables to Bronze\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING DATA TO BRONZE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "load_results = {}\n",
    "start_time = datetime.now()\n",
    "\n",
    "for table_name, table_config in BRONZE_TABLES.items():\n",
    "    result = load_table_to_bronze(table_name, table_config)\n",
    "    load_results[table_name] = result\n",
    "\n",
    "load_duration = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "successful = 0\n",
    "failed = 0\n",
    "total_records = 0\n",
    "\n",
    "for table_name, result in load_results.items():\n",
    "    status_symbol = \"✓\" if result['status'] == 'SUCCESS' else \"✗\"\n",
    "    if result['status'] == 'SUCCESS':\n",
    "        print(f\"{status_symbol} {table_name}: {result['records']:,} records\")\n",
    "        successful += 1\n",
    "        total_records += result['records']\n",
    "    else:\n",
    "        print(f\"{status_symbol} {table_name}: {result['error']}\")\n",
    "        failed += 1\n",
    "\n",
    "print(f\"\\nSuccessful: {successful}/{len(BRONZE_TABLES)}\")\n",
    "print(f\"Failed: {failed}/{len(BRONZE_TABLES)}\")\n",
    "print(f\"Total Records Loaded: {total_records:,}\")\n",
    "print(f\"Total Time: {load_duration:.2f} seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Bronze Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC SHOW TABLES IN na-dbxtraining.biju_bronze;\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display sample data from a bronze table\n",
    "print(\"Sample data from bronze_salesorderheader:\")\n",
    "display(spark.table(get_table_path(SCHEMA_BRONZE, \"bronze_salesorderheader\")).limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Print all table counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BRONZE TABLE COUNTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_records = 0\n",
    "for table_name in BRONZE_TABLES.keys():\n",
    "    bronze_table = f\"bronze_{table_name.lower()}\"\n",
    "    try:\n",
    "        count = spark.table(get_table_path(SCHEMA_BRONZE, bronze_table)).count()\n",
    "        print(f\"{bronze_table}: {count:,} rows\")\n",
    "        total_records += count\n",
    "    except Exception as e:\n",
    "        print(f\"{bronze_table}: ERROR - {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal Records: {total_records:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Show table schemas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BRONZE TABLE SCHEMAS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for table_name in BRONZE_TABLES.keys():\n",
    "    bronze_table = f\"bronze_{table_name.lower()}\"\n",
    "    print(f\"\\n{bronze_table}:\")\n",
    "    try:\n",
    "        spark.table(get_table_path(SCHEMA_BRONZE, bronze_table)).printSchema()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "            df\n",
    "            .withColumn(\"_extracted_at\", current_timestamp())\n",
    "            .withColumn(\"_source_system\", lit(\"azure_sql\"))\n",
    "            .withColumn(\"_source_table\", lit(jdbc_table))\n",
    "        )\n",
    "        \n",
    "        # Write to staging as Parquet\n",
    "        df.write.mode(\"overwrite\").parquet(staging_path)\n",
    "        \n",
    "        record_count = df.count()\n",
    "        logger.info(f\"✓ Extracted {record_count:,} records from {jdbc_table}\")\n",
    "        \n",
    "        return {\"status\": \"SUCCESS\", \"records\": record_count}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Failed to extract {jdbc_table}: {str(e)}\")\n",
    "        return {\"status\": \"FAILED\", \"error\": str(e)}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Extract all tables\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTING DATA FROM AZURE SQL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "extraction_results = {}\n",
    "start_time = datetime.now()\n",
    "\n",
    "for table_name, table_config in BRONZE_TABLES.items():\n",
    "    result = extract_table_to_staging(table_name, table_config)\n",
    "    extraction_results[table_name] = result\n",
    "\n",
    "extraction_duration = (datetime.now() - start_time).total_seconds()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b38186e4-0a71-4575-8576-8808921b2917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for table_name, result in extraction_results.items():\n",
    "    status_symbol = \"✓\" if result['status'] == 'SUCCESS' else \"✗\"\n",
    "    if result['status'] == 'SUCCESS':\n",
    "        print(f\"{status_symbol} {table_name}: {result['records']:,} records\")\n",
    "    else:\n",
    "        print(f\"{status_symbol} {table_name}: {result['error']}\")\n",
    "print(f\"\\nTotal Time: {extraction_duration:.2f} seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Load to Bronze using Auto Loader\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_table_to_bronze(table_name: str, table_config: dict):\n",
    "    \"\"\"Load table from staging to Bronze using Auto Loader\"\"\"\n",
    "    \n",
    "    staging_path = table_config['staging_path']\n",
    "    checkpoint_path = get_checkpoint_path('bronze', table_name)\n",
    "    schema_location = get_schema_path('bronze', table_name)\n",
    "    \n",
    "    bronze_table = get_table_path(SCHEMA_BRONZE, f\"bronze_{table_name.lower()}\")\n",
    "    \n",
    "    logger.info(f\"Loading {table_name} to {bronze_table}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read using Auto Loader\n",
    "        df = (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", AUTOLOADER_CONFIG['format'])\n",
    "            .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", AUTOLOADER_CONFIG['schema_evolution'])\n",
    "            .load(staging_path)\n",
    "        )\n",
    "        \n",
    "        # Add Bronze metadata\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"_bronze_load_timestamp\", current_timestamp())\n",
    "            .withColumn(\"_source_file\", input_file_name())\n",
    "        )\n",
    "        \n",
    "        # Write to Bronze using trigger(availableNow=True) for batch processing\n",
    "        query = (\n",
    "            df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", checkpoint_path)\n",
    "            .trigger(availableNow=True)\n",
    "            .toTable(bronze_table)\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        # Get record count\n",
    "        record_count = spark.table(bronze_table).count()\n",
    "        logger.info(f\"✓ Loaded {record_count:,} records to {bronze_table}\")\n",
    "        \n",
    "        return {\"status\": \"SUCCESS\", \"records\": record_count}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Failed to load {table_name}: {str(e)}\")\n",
    "        return {\"status\": \"FAILED\", \"error\": str(e)}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Load all tables to Bronze\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING DATA TO BRONZE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "load_results = {}\n",
    "start_time = datetime.now()\n",
    "\n",
    "for table_name, table_config in BRONZE_TABLES.items():\n",
    "    result = load_table_to_bronze(table_name, table_config)\n",
    "    load_results[table_name] = result\n",
    "\n",
    "load_duration = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for table_name, result in load_results.items():\n",
    "    status_symbol = \"✓\" if result['status'] == 'SUCCESS' else \"✗\"\n",
    "    if result['status'] == 'SUCCESS':\n",
    "        print(f\"{status_symbol} {table_name}: {result['records']:,} records\")\n",
    "    else:\n",
    "        print(f\"{status_symbol} {table_name}: {result['error']}\")\n",
    "print(f\"\\nTotal Time: {load_duration:.2f} seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Verify Bronze Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC SHOW TABLES IN na-dbxtraining.biju_bronze;\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display sample data\n",
    "display(\n",
    "    spark.table(\"`na-dbxtraining.biju_bronze.salesorderheader`\").limit(10)\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Print all table counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BRONZE TABLE COUNTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_records = 0\n",
    "for table_name in BRONZE_TABLES.keys():\n",
    "    bronze_table = f\"bronze_{table_name.lower()}\"\n",
    "    try:\n",
    "        count = spark.table(get_table_path(SCHEMA_BRONZE, bronze_table)).count()\n",
    "        print(f\"{bronze_table}: {count:,} rows\")\n",
    "        total_records += count\n",
    "    except Exception as e:\n",
    "        print(f\"{bronze_table}: ERROR - {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal Records: {total_records:,}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_batch_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
