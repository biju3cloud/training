{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f2841c-2a9e-4f3e-8d98-cca9d184399a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Configuration Notebook\n",
    "# MAGIC Central configuration for the entire pipeline. This notebook is included in all other notebooks using %run.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Catalog & Schema Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "CATALOG = \"`na-dbxtraining`\"\n",
    "SCHEMA_BRONZE = \"biju_bronze\"\n",
    "SCHEMA_SILVER = \"biju_silver\"\n",
    "SCHEMA_GOLD = \"biju_gold\"\n",
    "\n",
    "# Full table paths\n",
    "def get_table_path(schema, table):\n",
    "    \"\"\"Helper function to get full table path\"\"\"\n",
    "    return f\"{CATALOG}.{schema}.{table}\"\n",
    "\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Bronze Schema: {SCHEMA_BRONZE}\")\n",
    "print(f\"Silver Schema: {SCHEMA_SILVER}\")\n",
    "print(f\"Gold Schema: {SCHEMA_GOLD}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Azure SQL Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Azure SQL Connection Details\n",
    "JDBC_URL = (\n",
    "    \"jdbc:sqlserver://sqldbdbxtraining.database.windows.net:1433;\"\n",
    "    \"database=sqldb-nadbxtraining;\"\n",
    "    \"encrypt=true;trustServerCertificate=false;\"\n",
    "    \"hostNameInCertificate=*.database.windows.net;\"\n",
    ")\n",
    "\n",
    "# Secret configuration\n",
    "SECRET_SCOPE = \"dbx-ss-kv-natraining-2\"\n",
    "SECRET_KEY = \"sqladministrator-password\"\n",
    "\n",
    "# Get password from secrets\n",
    "DB_PASSWORD = dbutils.secrets.get(SECRET_SCOPE, SECRET_KEY)\n",
    "\n",
    "# Connection properties\n",
    "CONNECTION_PROPS = {\n",
    "    \"user\": \"sqladministrator@sqldbdbxtraining\",\n",
    "    \"password\": DB_PASSWORD,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "print(\"Azure SQL connection configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Table Configurations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Table configurations with source schema and staging paths\n",
    "BRONZE_TABLES = {\n",
    "    \"SalesOrderHeader\": {\n",
    "        \"source_schema\": \"bijuadventureworks\",\n",
    "        \"primary_key\": \"SalesOrderID\",\n",
    "        \"watermark_column\": \"ModifiedDate\",\n",
    "        \"staging_path\": \"/Volumes/na-dbxtraining/biju_raw/biju_vol/staging/adventureworks/SalesOrderHeader\"\n",
    "    },\n",
    "    \"SalesOrderDetail\": {\n",
    "        \"source_schema\": \"bijuadventureworks\",\n",
    "        \"primary_key\": \"SalesOrderDetailID\",\n",
    "        \"watermark_column\": \"ModifiedDate\",\n",
    "        \"staging_path\": \"/Volumes/na-dbxtraining/biju_raw/biju_vol/staging/adventureworks/SalesOrderDetail\"\n",
    "    },\n",
    "    \"Customer\": {\n",
    "        \"source_schema\": \"bijuadventureworks\",\n",
    "        \"primary_key\": \"CustomerID\",\n",
    "        \"watermark_column\": \"ModifiedDate\",\n",
    "        \"staging_path\": \"/Volumes/na-dbxtraining/biju_raw/biju_vol/staging/adventureworks/Customer\"\n",
    "    },\n",
    "    \"Person\": {\n",
    "        \"source_schema\": \"bijuadventureworks\",\n",
    "        \"primary_key\": \"BusinessEntityID\",\n",
    "        \"watermark_column\": \"ModifiedDate\",\n",
    "        \"staging_path\": \"/Volumes/na-dbxtraining/biju_raw/biju_vol/staging/adventureworks/Person\"\n",
    "    },\n",
    "    \"Address\": {\n",
    "        \"source_schema\": \"bijuadventureworks\",\n",
    "        \"primary_key\": \"AddressID\",\n",
    "        \"watermark_column\": \"ModifiedDate\",\n",
    "        \"staging_path\": \"/Volumes/na-dbxtraining/biju_raw/biju_vol/staging/adventureworks/Address\"\n",
    "    },\n",
    "    \"BusinessEntityAddress\": {\n",
    "        \"source_schema\": \"bijuadventureworks\",\n",
    "        \"primary_key\": [\"BusinessEntityID\", \"AddressID\", \"AddressTypeID\"],\n",
    "        \"watermark_column\": \"ModifiedDate\",\n",
    "        \"staging_path\": \"/Volumes/na-dbxtraining/biju_raw/biju_vol/staging/adventureworks/BusinessEntityAddress\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(BRONZE_TABLES)} bronze tables\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Checkpoint & Schema Locations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Checkpoint and schema locations for Auto Loader\n",
    "CHECKPOINT_BASE_PATH = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/checkpoints/adventureworks\"\n",
    "SCHEMA_BASE_PATH = \"/Volumes/na-dbxtraining/biju_raw/biju_vol/schema/adventureworks\"\n",
    "\n",
    "def get_checkpoint_path(layer, table_name):\n",
    "    \"\"\"Get checkpoint path for a table\"\"\"\n",
    "    return f\"{CHECKPOINT_BASE_PATH}/{layer}/{table_name}\"\n",
    "\n",
    "def get_schema_path(layer, table_name):\n",
    "    \"\"\"Get schema location path for a table\"\"\"\n",
    "    return f\"{SCHEMA_BASE_PATH}/{layer}/{table_name}\"\n",
    "\n",
    "print(\"Checkpoint and schema paths configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Auto Loader Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Auto Loader settings\n",
    "AUTOLOADER_CONFIG = {\n",
    "    \"format\": \"parquet\",\n",
    "    \"max_files_per_trigger\": 1000,\n",
    "    \"schema_inference\": True,\n",
    "    \"schema_evolution\": \"addNewColumns\"\n",
    "}\n",
    "\n",
    "print(\"Auto Loader configured\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## JDBC Read Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# JDBC read settings\n",
    "JDBC_CONFIG = {\n",
    "    \"fetch_size\": 10000,\n",
    "    \"num_partitions\": 8,\n",
    "    \"lower_bound\": \"2011-01-01\",\n",
    "    \"upper_bound\": \"2015-01-01\"\n",
    "}\n",
    "\n",
    "print(\"JDBC configuration set\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Utility Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def table_exists(catalog: str, schema: str, table: str) -> bool:\n",
    "    \"\"\"Check if a table exists\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {catalog}.{schema}.{table}\")\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_schema_if_not_exists(catalog: str, schema: str, comment: str = \"\"):\n",
    "    \"\"\"Create schema if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema} COMMENT '{comment}'\")\n",
    "        logger.info(f\"Schema {catalog}.{schema} ready\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating schema {catalog}.{schema}: {str(e)}\")\n",
    "\n",
    "def upsert_delta_table(\n",
    "    source_df: DataFrame,\n",
    "    target_table: str,\n",
    "    merge_keys: List[str],\n",
    "    update_columns: List[str] = None\n",
    "):\n",
    "    \"\"\"Perform upsert operation on Delta table\"\"\"\n",
    "    \n",
    "    if not table_exists(*target_table.split('.')):\n",
    "        # First load - just write\n",
    "        source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "        logger.info(f\"Created table {target_table} with {source_df.count()} records\")\n",
    "        return\n",
    "    \n",
    "    # Build merge condition\n",
    "    merge_condition = \" AND \".join([f\"target.{key} = source.{key}\" for key in merge_keys])\n",
    "    \n",
    "    # Determine columns to update\n",
    "    if update_columns is None:\n",
    "        update_columns = [col for col in source_df.columns if col not in merge_keys]\n",
    "    \n",
    "    # Perform merge\n",
    "    target = DeltaTable.forName(spark, target_table)\n",
    "    \n",
    "    target.alias(\"target\").merge(\n",
    "        source_df.alias(\"source\"),\n",
    "        merge_condition\n",
    "    ).whenMatchedUpdate(\n",
    "        set={col: f\"source.{col}\" for col in update_columns}\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={col: f\"source.{col}\" for col in source_df.columns}\n",
    "    ).execute()\n",
    "    \n",
    "    logger.info(f\"Upserted data into {target_table}\")\n",
    "\n",
    "def merge_scd_type2(\n",
    "    source_df: DataFrame,\n",
    "    target_table: str,\n",
    "    merge_keys: List[str],\n",
    "    compare_columns: List[str]\n",
    "):\n",
    "    \"\"\"Perform SCD Type 2 merge operation\"\"\"\n",
    "    \n",
    "    if not table_exists(*target_table.split('.')):\n",
    "        # First load\n",
    "        source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "        logger.info(f\"Created SCD Type 2 table {target_table}\")\n",
    "        return\n",
    "    \n",
    "    target = DeltaTable.forName(spark, target_table)\n",
    "    \n",
    "    # Build merge condition\n",
    "    merge_condition = \" AND \".join([f\"target.{key} = source.{key}\" for key in merge_keys])\n",
    "    merge_condition += \" AND target.is_current = true\"\n",
    "    \n",
    "    # Build comparison condition\n",
    "    compare_condition = \" OR \".join([\n",
    "        f\"target.{col} <> source.{col} OR (target.{col} IS NULL AND source.{col} IS NOT NULL) OR (target.{col} IS NOT NULL AND source.{col} IS NULL)\"\n",
    "        for col in compare_columns\n",
    "    ])\n",
    "    \n",
    "    # Close out changed records\n",
    "    target.alias(\"target\").merge(\n",
    "        source_df.alias(\"source\"),\n",
    "        merge_condition\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=compare_condition,\n",
    "        set={\n",
    "            \"is_current\": \"false\",\n",
    "            \"effective_end_date\": \"current_timestamp()\"\n",
    "        }\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={col: f\"source.{col}\" for col in source_df.columns}\n",
    "    ).execute()\n",
    "    \n",
    "    logger.info(f\"Applied SCD Type 2 merge to {target_table}\")\n",
    "\n",
    "print(\"Utility functions loaded\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Print Configuration Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Bronze Schema: {SCHEMA_BRONZE}\")\n",
    "print(f\"Silver Schema: {SCHEMA_SILVER}\")\n",
    "print(f\"Gold Schema: {SCHEMA_GOLD}\")\n",
    "print(f\"Bronze Tables: {len(BRONZE_TABLES)}\")\n",
    "print(f\"JDBC URL: {JDBC_URL.split(';')[0]}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_config",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
